<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Introduction | Machine learning, statistics, and optimization: A collection of intuitions</title>
  <meta name="description" content="." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Introduction | Machine learning, statistics, and optimization: A collection of intuitions" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="." />
  <meta name="github-repo" content="rezabontadi/machine-learning-hyper-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Introduction | Machine learning, statistics, and optimization: A collection of intuitions" />
  
  <meta name="twitter:description" content="." />
  

<meta name="author" content="Reza Bonyadi, Ph.D." />


<meta name="date" content="2019-08-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html">
<link rel="next" href="classification.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Machine learning</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#raw-data-vs-characterized-data-features"><i class="fa fa-check"></i><b>1.1</b> Raw data vs characterized data (features)</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#supervised-and-unsupervised-learning"><i class="fa fa-check"></i><b>1.2</b> Supervised and unsupervised learning</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#supervised-models"><i class="fa fa-check"></i><b>1.3</b> Supervised models</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#sec:biasVariance"><i class="fa fa-check"></i><b>1.4</b> The bias-variance debate</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>2</b> Classification</a><ul>
<li class="chapter" data-level="2.1" data-path="classification.html"><a href="classification.html#generative-vs.discriminative-classifiers"><i class="fa fa-check"></i><b>2.1</b> Generative vs. discriminative classifiers</a></li>
<li class="chapter" data-level="2.2" data-path="classification.html"><a href="classification.html#loss-function-for-classification"><i class="fa fa-check"></i><b>2.2</b> 0/1 loss function for classification</a><ul>
<li class="chapter" data-level="2.2.1" data-path="classification.html"><a href="classification.html#improvements"><i class="fa fa-check"></i><b>2.2.1</b> Improvements</a></li>
<li class="chapter" data-level="2.2.2" data-path="classification.html"><a href="classification.html#sec:01lossMath"><i class="fa fa-check"></i><b>2.2.2</b> More details</a></li>
<li class="chapter" data-level="2.2.3" data-path="classification.html"><a href="classification.html#pros-and-cons"><i class="fa fa-check"></i><b>2.2.3</b> Pros and cons</a></li>
<li class="chapter" data-level="2.2.4" data-path="classification.html"><a href="classification.html#implementation"><i class="fa fa-check"></i><b>2.2.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>2.3</b> Logistic regression</a><ul>
<li class="chapter" data-level="2.3.1" data-path="classification.html"><a href="classification.html#variable-importance"><i class="fa fa-check"></i><b>2.3.1</b> Variable importance</a></li>
<li class="chapter" data-level="2.3.2" data-path="classification.html"><a href="classification.html#pros-and-cons-1"><i class="fa fa-check"></i><b>2.3.2</b> Pros and cons</a></li>
<li class="chapter" data-level="2.3.3" data-path="classification.html"><a href="classification.html#more-details"><i class="fa fa-check"></i><b>2.3.3</b> More details</a></li>
<li class="chapter" data-level="2.3.4" data-path="classification.html"><a href="classification.html#implementation-1"><i class="fa fa-check"></i><b>2.3.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="classification.html"><a href="classification.html#bayes-classifier"><i class="fa fa-check"></i><b>2.4</b> Bayes classifier</a><ul>
<li class="chapter" data-level="2.4.1" data-path="classification.html"><a href="classification.html#more-details-1"><i class="fa fa-check"></i><b>2.4.1</b> More details</a></li>
<li class="chapter" data-level="2.4.2" data-path="classification.html"><a href="classification.html#continuous-variables"><i class="fa fa-check"></i><b>2.4.2</b> Continuous variables</a></li>
<li class="chapter" data-level="2.4.3" data-path="classification.html"><a href="classification.html#pros-and-cons-2"><i class="fa fa-check"></i><b>2.4.3</b> Pros and cons</a></li>
<li class="chapter" data-level="2.4.4" data-path="classification.html"><a href="classification.html#implementation-2"><i class="fa fa-check"></i><b>2.4.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="classification.html"><a href="classification.html#support-vector-machines"><i class="fa fa-check"></i><b>2.5</b> Support vector machines</a><ul>
<li class="chapter" data-level="2.5.1" data-path="classification.html"><a href="classification.html#kernel-trick"><i class="fa fa-check"></i><b>2.5.1</b> Kernel trick</a></li>
<li class="chapter" data-level="2.5.2" data-path="classification.html"><a href="classification.html#some-improvements"><i class="fa fa-check"></i><b>2.5.2</b> Some improvements</a></li>
<li class="chapter" data-level="2.5.3" data-path="classification.html"><a href="classification.html#more-details-2"><i class="fa fa-check"></i><b>2.5.3</b> More details</a></li>
<li class="chapter" data-level="2.5.4" data-path="classification.html"><a href="classification.html#implementation-3"><i class="fa fa-check"></i><b>2.5.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="classification.html"><a href="classification.html#sec:descisiontree"><i class="fa fa-check"></i><b>2.6</b> Decision tree</a></li>
<li class="chapter" data-level="2.7" data-path="classification.html"><a href="classification.html#sec:KNN"><i class="fa fa-check"></i><b>2.7</b> K-nearest neighbor</a><ul>
<li class="chapter" data-level="2.7.1" data-path="classification.html"><a href="classification.html#improvements-1"><i class="fa fa-check"></i><b>2.7.1</b> Improvements</a></li>
<li class="chapter" data-level="2.7.2" data-path="classification.html"><a href="classification.html#pros-and-cons-3"><i class="fa fa-check"></i><b>2.7.2</b> Pros and cons</a></li>
<li class="chapter" data-level="2.7.3" data-path="classification.html"><a href="classification.html#more-details-3"><i class="fa fa-check"></i><b>2.7.3</b> More details</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="classification.html"><a href="classification.html#gaussianprocessclassifier"><i class="fa fa-check"></i><b>2.8</b> Gaussian process classifier</a></li>
<li class="chapter" data-level="2.9" data-path="classification.html"><a href="classification.html#general-additive-model"><i class="fa fa-check"></i><b>2.9</b> General additive model</a></li>
<li class="chapter" data-level="2.10" data-path="classification.html"><a href="classification.html#regularization"><i class="fa fa-check"></i><b>2.10</b> Regularization</a><ul>
<li class="chapter" data-level="2.10.1" data-path="classification.html"><a href="classification.html#famous-types"><i class="fa fa-check"></i><b>2.10.1</b> Famous types</a></li>
<li class="chapter" data-level="2.10.2" data-path="classification.html"><a href="classification.html#more-details-4"><i class="fa fa-check"></i><b>2.10.2</b> More details</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="classification.html"><a href="classification.html#turning-binary-classifiers-to-multi-class"><i class="fa fa-check"></i><b>2.11</b> Turning binary classifiers to multi-class</a></li>
<li class="chapter" data-level="2.12" data-path="classification.html"><a href="classification.html#performance-measures-and-evaluation"><i class="fa fa-check"></i><b>2.12</b> Performance measures and evaluation</a><ul>
<li class="chapter" data-level="2.12.1" data-path="classification.html"><a href="classification.html#signal-detection"><i class="fa fa-check"></i><b>2.12.1</b> Signal detection</a></li>
<li class="chapter" data-level="2.12.2" data-path="classification.html"><a href="classification.html#receiver-operating-characteristic-roc-and-area-under-the-curve-auc"><i class="fa fa-check"></i><b>2.12.2</b> Receiver operating characteristic (ROC) and Area under the curve (AUC)</a></li>
<li class="chapter" data-level="2.12.3" data-path="classification.html"><a href="classification.html#confusion-matrix"><i class="fa fa-check"></i><b>2.12.3</b> Confusion matrix</a></li>
<li class="chapter" data-level="2.12.4" data-path="classification.html"><a href="classification.html#benchmarking"><i class="fa fa-check"></i><b>2.12.4</b> Benchmarking</a></li>
<li class="chapter" data-level="2.12.5" data-path="classification.html"><a href="classification.html#stratified-sampling"><i class="fa fa-check"></i><b>2.12.5</b> Stratified sampling</a></li>
<li class="chapter" data-level="2.12.6" data-path="classification.html"><a href="classification.html#cross-validation-and-random-permutation"><i class="fa fa-check"></i><b>2.12.6</b> Cross validation and random permutation</a></li>
<li class="chapter" data-level="2.12.7" data-path="classification.html"><a href="classification.html#imbalance-data-sets"><i class="fa fa-check"></i><b>2.12.7</b> Imbalance data sets</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>3</b> Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="regression.html"><a href="regression.html#linear-regression"><i class="fa fa-check"></i><b>3.1</b> Linear regression</a></li>
<li class="chapter" data-level="3.2" data-path="regression.html"><a href="regression.html#decision-tree-for-regression"><i class="fa fa-check"></i><b>3.2</b> Decision tree for regression</a></li>
<li class="chapter" data-level="3.3" data-path="regression.html"><a href="regression.html#performance-measures"><i class="fa fa-check"></i><b>3.3</b> Performance measures</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>4</b> Dimensionality reduction</a><ul>
<li class="chapter" data-level="4.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#matrix-decomposition"><i class="fa fa-check"></i><b>4.1</b> Matrix decomposition</a><ul>
<li class="chapter" data-level="4.1.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#eigen-decomposition"><i class="fa fa-check"></i><b>4.1.1</b> Eigen decomposition</a></li>
<li class="chapter" data-level="4.1.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#singular-value-decomposition"><i class="fa fa-check"></i><b>4.1.2</b> Singular value decomposition</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#principle-component-analysis"><i class="fa fa-check"></i><b>4.2</b> Principle component analysis</a></li>
<li class="chapter" data-level="4.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#t-sne"><i class="fa fa-check"></i><b>4.3</b> T-SNE</a></li>
<li class="chapter" data-level="4.4" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#independent-component-analysis"><i class="fa fa-check"></i><b>4.4</b> Independent component analysis</a></li>
<li class="chapter" data-level="4.5" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#partial-least-square"><i class="fa fa-check"></i><b>4.5</b> Partial least square</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html"><i class="fa fa-check"></i><b>5</b> Metrics learning</a><ul>
<li class="chapter" data-level="5.1" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html#large-margin-nearest-neighbor"><i class="fa fa-check"></i><b>5.1</b> Large margin nearest neighbor</a></li>
<li class="chapter" data-level="5.2" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html#metric-learning-for-kernel-regression"><i class="fa fa-check"></i><b>5.2</b> Metric learning for kernel regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>6</b> Neural networks</a><ul>
<li class="chapter" data-level="6.1" data-path="neural-networks.html"><a href="neural-networks.html#multi-layer-perceptron"><i class="fa fa-check"></i><b>6.1</b> Multi-layer perceptron</a></li>
<li class="chapter" data-level="6.2" data-path="neural-networks.html"><a href="neural-networks.html#mixed-density-networks"><i class="fa fa-check"></i><b>6.2</b> Mixed density networks</a></li>
<li class="chapter" data-level="6.3" data-path="neural-networks.html"><a href="neural-networks.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>6.3</b> Convolutional neural networks</a></li>
<li class="chapter" data-level="6.4" data-path="neural-networks.html"><a href="neural-networks.html#autoencoders"><i class="fa fa-check"></i><b>6.4</b> Autoencoders</a></li>
<li class="chapter" data-level="6.5" data-path="neural-networks.html"><a href="neural-networks.html#generative-adversial-neural-network"><i class="fa fa-check"></i><b>6.5</b> Generative adversial neural network</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>7</b> Bayesian inference</a></li>
<li class="chapter" data-level="8" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html"><i class="fa fa-check"></i><b>8</b> Ensemble techniques</a><ul>
<li class="chapter" data-level="8.1" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html#random-forest"><i class="fa fa-check"></i><b>8.1</b> Random forest</a></li>
<li class="chapter" data-level="8.2" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html#gradient-boosting"><i class="fa fa-check"></i><b>8.2</b> Gradient boosting</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html"><i class="fa fa-check"></i><b>9</b> Reinforcement learning</a><ul>
<li class="chapter" data-level="9.1" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#q-learning"><i class="fa fa-check"></i><b>9.1</b> Q-learning</a></li>
<li class="chapter" data-level="9.2" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#curiosity"><i class="fa fa-check"></i><b>9.2</b> Curiosity</a><ul>
<li class="chapter" data-level="9.2.1" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#implementation-idea"><i class="fa fa-check"></i><b>9.2.1</b> Implementation idea</a></li>
<li class="chapter" data-level="9.2.2" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#what-does-it-solve"><i class="fa fa-check"></i><b>9.2.2</b> What does it solve?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html"><i class="fa fa-check"></i><b>10</b> Bagging and boosting</a><ul>
<li class="chapter" data-level="10.1" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#extreme-boosted-tree"><i class="fa fa-check"></i><b>10.1</b> Extreme boosted tree</a></li>
<li class="chapter" data-level="10.2" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#anomaly-detection"><i class="fa fa-check"></i><b>10.2</b> Anomaly detection</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="sec-preprocessing.html"><a href="sec-preprocessing.html"><i class="fa fa-check"></i><b>11</b> Preprocessing</a></li>
<li class="part"><span><b>II Optimziation</b></span></li>
<li class="chapter" data-level="12" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>12</b> Introduction</a><ul>
<li class="chapter" data-level="12.1" data-path="introduction-1.html"><a href="introduction-1.html#derivative-free-vs-with-derivative-optimization-methods"><i class="fa fa-check"></i><b>12.1</b> Derivative-free vs with derivative optimization methods</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="optimization-problems.html"><a href="optimization-problems.html"><i class="fa fa-check"></i><b>13</b> Optimization problems</a><ul>
<li class="chapter" data-level="13.1" data-path="optimization-problems.html"><a href="optimization-problems.html#single-and-multi-objective"><i class="fa fa-check"></i><b>13.1</b> Single and Multi objective</a></li>
<li class="chapter" data-level="13.2" data-path="optimization-problems.html"><a href="optimization-problems.html#constrains-in-problems"><i class="fa fa-check"></i><b>13.2</b> Constrains in problems</a></li>
<li class="chapter" data-level="13.3" data-path="optimization-problems.html"><a href="optimization-problems.html#dynamic-optimization-problems"><i class="fa fa-check"></i><b>13.3</b> Dynamic optimization problems</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="use-of-derivative-in-optimization.html"><a href="use-of-derivative-in-optimization.html"><i class="fa fa-check"></i><b>14</b> Use of derivative in optimization</a></li>
<li class="chapter" data-level="15" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html"><i class="fa fa-check"></i><b>15</b> Derivative-free algorithms</a><ul>
<li class="chapter" data-level="15.1" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#finite-difference"><i class="fa fa-check"></i><b>15.1</b> Finite difference</a></li>
<li class="chapter" data-level="15.2" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#population-based-optimization"><i class="fa fa-check"></i><b>15.2</b> Population-based optimization</a><ul>
<li class="chapter" data-level="15.2.1" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#genetic-algorithm"><i class="fa fa-check"></i><b>15.2.1</b> Genetic algorithm</a></li>
<li class="chapter" data-level="15.2.2" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#evolutionary-strategy"><i class="fa fa-check"></i><b>15.2.2</b> Evolutionary strategy</a></li>
<li class="chapter" data-level="15.2.3" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#covariance-matrix-adaptation"><i class="fa fa-check"></i><b>15.2.3</b> Covariance matrix adaptation</a></li>
<li class="chapter" data-level="15.2.4" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#particle-swarm-optimization"><i class="fa fa-check"></i><b>15.2.4</b> Particle swarm optimization</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Statistics</b></span></li>
<li class="chapter" data-level="16" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>16</b> Introduction</a></li>
<li class="chapter" data-level="17" data-path="basics-statistics.html"><a href="basics-statistics.html"><i class="fa fa-check"></i><b>17</b> Basics statistics</a><ul>
<li class="chapter" data-level="17.1" data-path="basics-statistics.html"><a href="basics-statistics.html#correlation"><i class="fa fa-check"></i><b>17.1</b> Correlation</a></li>
<li class="chapter" data-level="17.2" data-path="basics-statistics.html"><a href="basics-statistics.html#moments"><i class="fa fa-check"></i><b>17.2</b> Moments</a></li>
<li class="chapter" data-level="17.3" data-path="basics-statistics.html"><a href="basics-statistics.html#covariance-matrix"><i class="fa fa-check"></i><b>17.3</b> Covariance matrix</a></li>
<li class="chapter" data-level="17.4" data-path="basics-statistics.html"><a href="basics-statistics.html#distributions"><i class="fa fa-check"></i><b>17.4</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="statistical-analysis.html"><a href="statistical-analysis.html"><i class="fa fa-check"></i><b>18</b> Statistical analysis</a><ul>
<li class="chapter" data-level="18.1" data-path="statistical-analysis.html"><a href="statistical-analysis.html#statistical-tests"><i class="fa fa-check"></i><b>18.1</b> Statistical tests</a></li>
<li class="chapter" data-level="18.2" data-path="statistical-analysis.html"><a href="statistical-analysis.html#causality"><i class="fa fa-check"></i><b>18.2</b> Causality</a></li>
<li class="chapter" data-level="18.3" data-path="statistical-analysis.html"><a href="statistical-analysis.html#anova"><i class="fa fa-check"></i><b>18.3</b> Anova</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="terms-and-notations.html"><a href="terms-and-notations.html"><i class="fa fa-check"></i><b>19</b> Terms and notations</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine learning, statistics, and optimization: A collection of intuitions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Introduction</h1>
<p>Machine learning refers to a set of algorithms which make a machine (with a computer as its brain) to apparently learn. While learning in animals and human is not very well understood, in machines it is simply to find a generic rule, given a set of examples, which</p>
<p>Not fully observed, hence, there need to be assumptions as the solutions is not unique</p>
<div id="raw-data-vs-characterized-data-features" class="section level2">
<h2><span class="header-section-number">1.1</span> Raw data vs characterized data (features)</h2>
<p>Feature space</p>
</div>
<div id="supervised-and-unsupervised-learning" class="section level2">
<h2><span class="header-section-number">1.2</span> Supervised and unsupervised learning</h2>
<p>Supervised learning asks for</p>
<p>Unsupervised learning, on the other hand, …</p>
</div>
<div id="supervised-models" class="section level2">
<h2><span class="header-section-number">1.3</span> Supervised models</h2>
<p>In a more general framework, given a set of instances, <span class="math inline">\(X\)</span>, with <span class="math inline">\(m\)</span> rows and <span class="math inline">\(n\)</span> columns (each row is an instance and each column is an attribute) and their associated responses, <span class="math inline">\(Y\)</span>, with <span class="math inline">\(m\)</span> rows, a supervised model <span class="math inline">\(M(.,\theta)\)</span>, <span class="math inline">\(\theta\)</span> being model parameters (e.g., linear model), provides a generic rule by which the response, <span class="math inline">\(y\)</span>, is correctly estimated for any given instance, <span class="math inline">\(x\)</span> (<span class="math inline">\(1\)</span> row and <span class="math inline">\(n\)</span> columns). The parameters <span class="math inline">\(\theta\)</span> are optimized by an optimizer for the model <span class="math inline">\(M\)</span> to best satisfy an <em>evaluation</em> metric (in some algorithms, this is called the <em>loss function</em>) which evaluates how similar <span class="math inline">\(M(.,\theta)\)</span> is to actual <span class="math inline">\(y\)</span> for each <span class="math inline">\(x\)</span> and any given <span class="math inline">\(\theta\)</span>.</p>
<p>It is very important to design the model carefully as it is responsible to represent the “pattern” in the data in the most generic and accurate way. You may ask why not picking the most flexible mathematical equation in existence (e.g., Taylor series) and optimize its parameters to fit the given data? Flexible models (e.g., deep networks) may be able to fit the data well (simply because they are flexible and can fit anything), however, they might not be able to <em>generalize</em> well, i.e., work well for unseen data. Sometimes the models come from specific knowledge about the problem, e.g., physics models.</p>
</div>
<div id="sec:biasVariance" class="section level2">
<h2><span class="header-section-number">1.4</span> The bias-variance debate</h2>
<p>Assume there is a set of given instances, <span class="math inline">\(X\)</span> (m rows and n columns), and we are asked to optimize a model <span class="math inline">\(M(X, \theta)\)</span> to estimate <span class="math inline">\(Y\)</span> (m rows) given some examples, i.e., supervised learning. Let us call each instance <span class="math inline">\(x\)</span> and its corresponding desired output as <span class="math inline">\(y\)</span>. In reality, <span class="math inline">\(y\)</span> is the outcome of a system working with inputs <span class="math inline">\(x\)</span>, and some noise, <span class="math inline">\(y=f(x)+\epsilon\)</span>, where <span class="math inline">\(f(.)\)</span> is not known and <span class="math inline">\(\epsilon\)</span> is noise which is also unknown. For example, if <span class="math inline">\(x\)</span> represents characteristics of people (smoking habit, weight, genetics, etc.) and <span class="math inline">\(y\)</span> is whether or not they would have a heart-attack after their 50s, then <span class="math inline">\(f(x)\)</span> is how body would respond to those characteristics and leads to a heart-attack or not, which is not really known. With modelling, we try to estimate this function <span class="math inline">\(f(x)\)</span>, given some examples and some assumptions on the shape of <span class="math inline">\(f\)</span>.</p>
<p>After optimization, what we would get is <span class="math inline">\(Y=M(X, \hat{\theta})+\epsilon\)</span>, where <span class="math inline">\(\epsilon\)</span> has <span class="math inline">\(m\)</span> rows and indicates the error from <span class="math inline">\(Y\)</span>. If <span class="math inline">\(\epsilon\)</span> is very small then that means <span class="math inline">\(M(X, \hat{\theta})\)</span> is an accurate estimation of <span class="math inline">\(Y\)</span>. If the model M is a complex, flexible, equation then it will be able to fit any complex behavior of <span class="math inline">\(Y\)</span> as function of <span class="math inline">\(X\)</span>, including all fluctuations. In that case, the outputs of the model will be very very similar to the values of <span class="math inline">\(Y\)</span> (non-biased), however, it will have lots of fluctuations, which leads to a high variance (large fluctuations usually leads to large variance). If the model is not complex though, the gap between the output of the model and <span class="math inline">\(Y\)</span> might be large (bias), however, it will not fit all fluctuations in the <span class="math inline">\(Y\)</span> which leads to smaller variance. That is why it is said a complex model has a large variance and small bias, and vice versa.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
