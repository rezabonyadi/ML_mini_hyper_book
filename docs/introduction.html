<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Introduction | Machine learning, statistics, and optimization: A collection of intuitions</title>
  <meta name="description" content="." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Introduction | Machine learning, statistics, and optimization: A collection of intuitions" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="." />
  <meta name="github-repo" content="rezabontadi/machine-learning-hyper-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Introduction | Machine learning, statistics, and optimization: A collection of intuitions" />
  
  <meta name="twitter:description" content="." />
  

<meta name="author" content="Reza Bonyadi, Ph.D." />


<meta name="date" content="2019-08-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html">
<link rel="next" href="classification.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Machine learning</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#raw-data-vs-characterized-data-features"><i class="fa fa-check"></i><b>1.1</b> Raw data vs characterized data (features)</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#sec:supervisedvsunsupervised"><i class="fa fa-check"></i><b>1.2</b> Supervised and unsupervised learning</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#sec:supervisedmodels"><i class="fa fa-check"></i><b>1.3</b> Supervised models</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#sec:biasVariance"><i class="fa fa-check"></i><b>1.4</b> The bias-variance debate</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>2</b> Classification</a><ul>
<li class="chapter" data-level="2.1" data-path="classification.html"><a href="classification.html#generative-vs.discriminative-classifiers"><i class="fa fa-check"></i><b>2.1</b> Generative vs.Â discriminative classifiers</a></li>
<li class="chapter" data-level="2.2" data-path="classification.html"><a href="classification.html#loss-function-for-classification"><i class="fa fa-check"></i><b>2.2</b> 0/1 loss function for classification</a><ul>
<li class="chapter" data-level="2.2.1" data-path="classification.html"><a href="classification.html#improvements"><i class="fa fa-check"></i><b>2.2.1</b> Improvements</a></li>
<li class="chapter" data-level="2.2.2" data-path="classification.html"><a href="classification.html#sec:01lossMath"><i class="fa fa-check"></i><b>2.2.2</b> More details</a></li>
<li class="chapter" data-level="2.2.3" data-path="classification.html"><a href="classification.html#pros-and-cons"><i class="fa fa-check"></i><b>2.2.3</b> Pros and cons</a></li>
<li class="chapter" data-level="2.2.4" data-path="classification.html"><a href="classification.html#implementation"><i class="fa fa-check"></i><b>2.2.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>2.3</b> Logistic regression</a><ul>
<li class="chapter" data-level="2.3.1" data-path="classification.html"><a href="classification.html#variable-importance"><i class="fa fa-check"></i><b>2.3.1</b> Variable importance</a></li>
<li class="chapter" data-level="2.3.2" data-path="classification.html"><a href="classification.html#pros-and-cons-1"><i class="fa fa-check"></i><b>2.3.2</b> Pros and cons</a></li>
<li class="chapter" data-level="2.3.3" data-path="classification.html"><a href="classification.html#more-details"><i class="fa fa-check"></i><b>2.3.3</b> More details</a></li>
<li class="chapter" data-level="2.3.4" data-path="classification.html"><a href="classification.html#implementation-1"><i class="fa fa-check"></i><b>2.3.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="classification.html"><a href="classification.html#bayes-classifier"><i class="fa fa-check"></i><b>2.4</b> Bayes classifier</a><ul>
<li class="chapter" data-level="2.4.1" data-path="classification.html"><a href="classification.html#more-details-1"><i class="fa fa-check"></i><b>2.4.1</b> More details</a></li>
<li class="chapter" data-level="2.4.2" data-path="classification.html"><a href="classification.html#continuous-variables"><i class="fa fa-check"></i><b>2.4.2</b> Continuous variables</a></li>
<li class="chapter" data-level="2.4.3" data-path="classification.html"><a href="classification.html#pros-and-cons-2"><i class="fa fa-check"></i><b>2.4.3</b> Pros and cons</a></li>
<li class="chapter" data-level="2.4.4" data-path="classification.html"><a href="classification.html#implementation-2"><i class="fa fa-check"></i><b>2.4.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="classification.html"><a href="classification.html#support-vector-machines"><i class="fa fa-check"></i><b>2.5</b> Support vector machines</a><ul>
<li class="chapter" data-level="2.5.1" data-path="classification.html"><a href="classification.html#more-details-2"><i class="fa fa-check"></i><b>2.5.1</b> More details</a></li>
<li class="chapter" data-level="2.5.2" data-path="classification.html"><a href="classification.html#kernel-trick"><i class="fa fa-check"></i><b>2.5.2</b> Kernel trick</a></li>
<li class="chapter" data-level="2.5.3" data-path="classification.html"><a href="classification.html#some-improvements"><i class="fa fa-check"></i><b>2.5.3</b> Some improvements</a></li>
<li class="chapter" data-level="2.5.4" data-path="classification.html"><a href="classification.html#implementation-3"><i class="fa fa-check"></i><b>2.5.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="classification.html"><a href="classification.html#sec:descisiontree"><i class="fa fa-check"></i><b>2.6</b> Decision tree</a></li>
<li class="chapter" data-level="2.7" data-path="classification.html"><a href="classification.html#sec:KNN"><i class="fa fa-check"></i><b>2.7</b> K-nearest neighbor</a><ul>
<li class="chapter" data-level="2.7.1" data-path="classification.html"><a href="classification.html#improvements-1"><i class="fa fa-check"></i><b>2.7.1</b> Improvements</a></li>
<li class="chapter" data-level="2.7.2" data-path="classification.html"><a href="classification.html#pros-and-cons-3"><i class="fa fa-check"></i><b>2.7.2</b> Pros and cons</a></li>
<li class="chapter" data-level="2.7.3" data-path="classification.html"><a href="classification.html#more-details-3"><i class="fa fa-check"></i><b>2.7.3</b> More details</a></li>
<li class="chapter" data-level="2.7.4" data-path="classification.html"><a href="classification.html#implementation-4"><i class="fa fa-check"></i><b>2.7.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="classification.html"><a href="classification.html#gaussianprocessclassifier"><i class="fa fa-check"></i><b>2.8</b> Gaussian process classifier</a></li>
<li class="chapter" data-level="2.9" data-path="classification.html"><a href="classification.html#general-additive-model"><i class="fa fa-check"></i><b>2.9</b> General additive model</a></li>
<li class="chapter" data-level="2.10" data-path="classification.html"><a href="classification.html#regularization"><i class="fa fa-check"></i><b>2.10</b> Regularization</a><ul>
<li class="chapter" data-level="2.10.1" data-path="classification.html"><a href="classification.html#famous-types"><i class="fa fa-check"></i><b>2.10.1</b> Famous types</a></li>
<li class="chapter" data-level="2.10.2" data-path="classification.html"><a href="classification.html#more-details-4"><i class="fa fa-check"></i><b>2.10.2</b> More details</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="classification.html"><a href="classification.html#turning-binary-classifiers-to-multi-class"><i class="fa fa-check"></i><b>2.11</b> Turning binary classifiers to multi-class</a></li>
<li class="chapter" data-level="2.12" data-path="classification.html"><a href="classification.html#performance-measures-and-evaluation"><i class="fa fa-check"></i><b>2.12</b> Performance measures and evaluation</a><ul>
<li class="chapter" data-level="2.12.1" data-path="classification.html"><a href="classification.html#signal-detection"><i class="fa fa-check"></i><b>2.12.1</b> Signal detection</a></li>
<li class="chapter" data-level="2.12.2" data-path="classification.html"><a href="classification.html#receiver-operating-characteristic-roc-and-area-under-the-curve-auc"><i class="fa fa-check"></i><b>2.12.2</b> Receiver operating characteristic (ROC) and Area under the curve (AUC)</a></li>
<li class="chapter" data-level="2.12.3" data-path="classification.html"><a href="classification.html#confusion-matrix"><i class="fa fa-check"></i><b>2.12.3</b> Confusion matrix</a></li>
<li class="chapter" data-level="2.12.4" data-path="classification.html"><a href="classification.html#benchmarking"><i class="fa fa-check"></i><b>2.12.4</b> Benchmarking</a></li>
<li class="chapter" data-level="2.12.5" data-path="classification.html"><a href="classification.html#stratified-sampling"><i class="fa fa-check"></i><b>2.12.5</b> Stratified sampling</a></li>
<li class="chapter" data-level="2.12.6" data-path="classification.html"><a href="classification.html#cross-validation-and-random-permutation"><i class="fa fa-check"></i><b>2.12.6</b> Cross validation and random permutation</a></li>
<li class="chapter" data-level="2.12.7" data-path="classification.html"><a href="classification.html#imbalance-data-sets"><i class="fa fa-check"></i><b>2.12.7</b> Imbalance data sets</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>3</b> Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="regression.html"><a href="regression.html#linear-regression"><i class="fa fa-check"></i><b>3.1</b> Linear regression</a></li>
<li class="chapter" data-level="3.2" data-path="regression.html"><a href="regression.html#decision-tree-for-regression"><i class="fa fa-check"></i><b>3.2</b> Decision tree for regression</a></li>
<li class="chapter" data-level="3.3" data-path="regression.html"><a href="regression.html#performance-measures"><i class="fa fa-check"></i><b>3.3</b> Performance measures</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>4</b> Dimensionality reduction</a><ul>
<li class="chapter" data-level="4.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#more-details-5"><i class="fa fa-check"></i><b>4.1</b> More details</a></li>
<li class="chapter" data-level="4.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#principal-component-analysis"><i class="fa fa-check"></i><b>4.2</b> Principal component analysis</a></li>
<li class="chapter" data-level="4.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#t-sne"><i class="fa fa-check"></i><b>4.3</b> T-SNE</a></li>
<li class="chapter" data-level="4.4" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#independent-component-analysis"><i class="fa fa-check"></i><b>4.4</b> Independent component analysis</a></li>
<li class="chapter" data-level="4.5" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#partial-least-square"><i class="fa fa-check"></i><b>4.5</b> Partial least square</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html"><i class="fa fa-check"></i><b>5</b> Metrics learning</a><ul>
<li class="chapter" data-level="5.1" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html#large-margin-nearest-neighbor"><i class="fa fa-check"></i><b>5.1</b> Large margin nearest neighbor</a></li>
<li class="chapter" data-level="5.2" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html#metric-learning-for-kernel-regression"><i class="fa fa-check"></i><b>5.2</b> Metric learning for kernel regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>6</b> Neural networks</a><ul>
<li class="chapter" data-level="6.1" data-path="neural-networks.html"><a href="neural-networks.html#multi-layer-perceptron"><i class="fa fa-check"></i><b>6.1</b> Multi-layer perceptron</a></li>
<li class="chapter" data-level="6.2" data-path="neural-networks.html"><a href="neural-networks.html#mixed-density-networks"><i class="fa fa-check"></i><b>6.2</b> Mixed density networks</a></li>
<li class="chapter" data-level="6.3" data-path="neural-networks.html"><a href="neural-networks.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>6.3</b> Convolutional neural networks</a></li>
<li class="chapter" data-level="6.4" data-path="neural-networks.html"><a href="neural-networks.html#autoencoders"><i class="fa fa-check"></i><b>6.4</b> Autoencoders</a></li>
<li class="chapter" data-level="6.5" data-path="neural-networks.html"><a href="neural-networks.html#generative-adversial-neural-network"><i class="fa fa-check"></i><b>6.5</b> Generative adversial neural network</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>7</b> Bayesian inference</a></li>
<li class="chapter" data-level="8" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html"><i class="fa fa-check"></i><b>8</b> Ensemble techniques</a><ul>
<li class="chapter" data-level="8.1" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html#random-forest"><i class="fa fa-check"></i><b>8.1</b> Random forest</a></li>
<li class="chapter" data-level="8.2" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html#gradient-boosting"><i class="fa fa-check"></i><b>8.2</b> Gradient boosting</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html"><i class="fa fa-check"></i><b>9</b> Reinforcement learning</a><ul>
<li class="chapter" data-level="9.1" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#q-learning"><i class="fa fa-check"></i><b>9.1</b> Q-learning</a></li>
<li class="chapter" data-level="9.2" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#curiosity"><i class="fa fa-check"></i><b>9.2</b> Curiosity</a><ul>
<li class="chapter" data-level="9.2.1" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#implementation-idea"><i class="fa fa-check"></i><b>9.2.1</b> Implementation idea</a></li>
<li class="chapter" data-level="9.2.2" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#what-does-it-solve"><i class="fa fa-check"></i><b>9.2.2</b> What does it solve?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html"><i class="fa fa-check"></i><b>10</b> Bagging and boosting</a><ul>
<li class="chapter" data-level="10.1" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#extreme-boosted-tree"><i class="fa fa-check"></i><b>10.1</b> Extreme boosted tree</a></li>
<li class="chapter" data-level="10.2" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#anomaly-detection"><i class="fa fa-check"></i><b>10.2</b> Anomaly detection</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="sec-preprocessing.html"><a href="sec-preprocessing.html"><i class="fa fa-check"></i><b>11</b> Preprocessing</a><ul>
<li class="chapter" data-level="11.1" data-path="sec-preprocessing.html"><a href="sec-preprocessing.html#normalization-and-standardization"><i class="fa fa-check"></i><b>11.1</b> Normalization and standardization</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="sec-signalProcessing.html"><a href="sec-signalProcessing.html"><i class="fa fa-check"></i><b>12</b> Signal processing</a></li>
<li class="part"><span><b>II Optimziation</b></span></li>
<li class="chapter" data-level="13" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>13</b> Introduction</a><ul>
<li class="chapter" data-level="13.1" data-path="introduction-1.html"><a href="introduction-1.html#derivative-free-vs-with-derivative-optimization-methods"><i class="fa fa-check"></i><b>13.1</b> Derivative-free vs with derivative optimization methods</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="optimization-problems.html"><a href="optimization-problems.html"><i class="fa fa-check"></i><b>14</b> Optimization problems</a><ul>
<li class="chapter" data-level="14.1" data-path="optimization-problems.html"><a href="optimization-problems.html#single-and-multi-objective"><i class="fa fa-check"></i><b>14.1</b> Single and Multi objective</a></li>
<li class="chapter" data-level="14.2" data-path="optimization-problems.html"><a href="optimization-problems.html#constrains-in-problems"><i class="fa fa-check"></i><b>14.2</b> Constrains in problems</a></li>
<li class="chapter" data-level="14.3" data-path="optimization-problems.html"><a href="optimization-problems.html#dynamic-optimization-problems"><i class="fa fa-check"></i><b>14.3</b> Dynamic optimization problems</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="use-of-derivative-in-optimization.html"><a href="use-of-derivative-in-optimization.html"><i class="fa fa-check"></i><b>15</b> Use of derivative in optimization</a></li>
<li class="chapter" data-level="16" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html"><i class="fa fa-check"></i><b>16</b> Derivative-free algorithms</a><ul>
<li class="chapter" data-level="16.1" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#finite-difference"><i class="fa fa-check"></i><b>16.1</b> Finite difference</a></li>
<li class="chapter" data-level="16.2" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#population-based-optimization"><i class="fa fa-check"></i><b>16.2</b> Population-based optimization</a><ul>
<li class="chapter" data-level="16.2.1" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#genetic-algorithm"><i class="fa fa-check"></i><b>16.2.1</b> Genetic algorithm</a></li>
<li class="chapter" data-level="16.2.2" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#evolutionary-strategy"><i class="fa fa-check"></i><b>16.2.2</b> Evolutionary strategy</a></li>
<li class="chapter" data-level="16.2.3" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#covariance-matrix-adaptation"><i class="fa fa-check"></i><b>16.2.3</b> Covariance matrix adaptation</a></li>
<li class="chapter" data-level="16.2.4" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#particle-swarm-optimization"><i class="fa fa-check"></i><b>16.2.4</b> Particle swarm optimization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="what-are-constraints.html"><a href="what-are-constraints.html"><i class="fa fa-check"></i><b>17</b> What are Constraints</a><ul>
<li class="chapter" data-level="17.1" data-path="what-are-constraints.html"><a href="what-are-constraints.html#how-to-deal-with-constraints"><i class="fa fa-check"></i><b>17.1</b> How to deal with constraints</a><ul>
<li class="chapter" data-level="17.1.1" data-path="what-are-constraints.html"><a href="what-are-constraints.html#sec:lagrangian"><i class="fa fa-check"></i><b>17.1.1</b> Lagrangian</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="famous-forms-of-optimization-problems.html"><a href="famous-forms-of-optimization-problems.html"><i class="fa fa-check"></i><b>18</b> Famous forms of optimization problems</a></li>
<li class="part"><span><b>III Statistics</b></span></li>
<li class="chapter" data-level="19" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>19</b> Introduction</a></li>
<li class="chapter" data-level="20" data-path="basics-statistics.html"><a href="basics-statistics.html"><i class="fa fa-check"></i><b>20</b> Basics statistics</a><ul>
<li class="chapter" data-level="20.1" data-path="basics-statistics.html"><a href="basics-statistics.html#correlation"><i class="fa fa-check"></i><b>20.1</b> Correlation</a></li>
<li class="chapter" data-level="20.2" data-path="basics-statistics.html"><a href="basics-statistics.html#moments"><i class="fa fa-check"></i><b>20.2</b> Moments</a></li>
<li class="chapter" data-level="20.3" data-path="basics-statistics.html"><a href="basics-statistics.html#covariance-matrix"><i class="fa fa-check"></i><b>20.3</b> Covariance matrix</a></li>
<li class="chapter" data-level="20.4" data-path="basics-statistics.html"><a href="basics-statistics.html#matrix-decomposition"><i class="fa fa-check"></i><b>20.4</b> Matrix decomposition</a><ul>
<li class="chapter" data-level="20.4.1" data-path="basics-statistics.html"><a href="basics-statistics.html#eigen-decomposition"><i class="fa fa-check"></i><b>20.4.1</b> Eigen decomposition</a></li>
<li class="chapter" data-level="20.4.2" data-path="basics-statistics.html"><a href="basics-statistics.html#singular-value-decomposition"><i class="fa fa-check"></i><b>20.4.2</b> Singular value decomposition</a></li>
</ul></li>
<li class="chapter" data-level="20.5" data-path="basics-statistics.html"><a href="basics-statistics.html#distributions"><i class="fa fa-check"></i><b>20.5</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="statistical-analysis.html"><a href="statistical-analysis.html"><i class="fa fa-check"></i><b>21</b> Statistical analysis</a><ul>
<li class="chapter" data-level="21.1" data-path="statistical-analysis.html"><a href="statistical-analysis.html#statistical-tests"><i class="fa fa-check"></i><b>21.1</b> Statistical tests</a></li>
<li class="chapter" data-level="21.2" data-path="statistical-analysis.html"><a href="statistical-analysis.html#causality"><i class="fa fa-check"></i><b>21.2</b> Causality</a></li>
<li class="chapter" data-level="21.3" data-path="statistical-analysis.html"><a href="statistical-analysis.html#anova"><i class="fa fa-check"></i><b>21.3</b> Anova</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="terms-and-notations.html"><a href="terms-and-notations.html"><i class="fa fa-check"></i><b>22</b> Terms and notations</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine learning, statistics, and optimization: A collection of intuitions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Introduction</h1>
<p>Machine learning refers to a set of algorithms which make a machine (with a computer as its brain) to apparently learn. While learning in animals and human is not very well understood, in machines it is simply to find a generic rule, given a set of examples, which</p>
<p>Machine learning refers to a set of methodologies that make machines apparently âlearningâ to perform tasks. Although it is hard to define learning, it is related to progressive improvement in performing a task successfully. Intuitively there are two types of learning: self-learning (unsupervised) and learning from others (supervised).</p>
<p>Not fully observed, hence, there need to be assumptions as the solutions is not unique</p>
<div id="raw-data-vs-characterized-data-features" class="section level2">
<h2><span class="header-section-number">1.1</span> Raw data vs characterized data (features)</h2>
<p>The data in its original form, raw form, represents what is measured. The measurements, however, are not always useful to learn from. For example, in a stock market signal, the actual close daily prices are not as important as the trend of those values. Hence, depending on the problem, some characteristics of the data might be more informative than the raw data itself. These characteristics are usually called features. Features basically represent important (relevant to a given problem which uses that data for solution) attributes of the problem. They are either designed by an expert (i.e., feature engineering) or optimized by an algorithm for a given model (i.e., feature formation, popular in deep learning). For example, assume that we want to characterize fruits. Some obvious features of fruits are their shapes (e.g., being round, or egg-shaped, or long), their tastes, and their smell. Features characterize attributes that distinguish different objects</p>
<p>For many problems, the features are already known. For example, we already know that a feed in a newspaper is considered interesting for a particular reader if the reader spends more time on it (the spent time is a feature). However, recognizing someoneâs face form an image is not easy to be defined by apparent features. This encourages methods like Deep Learning to not only learn the task but also find the best presentation for the problem, just from the given data.</p>
</div>
<div id="sec:supervisedvsunsupervised" class="section level2">
<h2><span class="header-section-number">1.2</span> Supervised and unsupervised learning</h2>
<p>In supervised learning the learner learns to accomplish a task with supervision, i.e., there is a set of instances that the learner learns from, or there is a system that provides feedback to the actions of the learner (reinforcement). Imagine for example you need a system to learn how to drive. For a human, we just hire an instructor and train the person and done. This is precisely the same for a machine, we provide instances or a feedback loop so that the machine can evaluate its performance and learns. Applications of supervised learning include self-driving cars, stock market prediction, object recognition, natural language processing, maintenance prediction, recommendation engines, weather forecast, and more.</p>
<p>In the unsupervised learning, the algorithm âfigures outâ the patterns in the instances, without any supervision. This is the more sophisticated while the most exciting use of machine learning. Customer segmentation, anomaly detection, product similarities, and many other applications require unsupervised learning.</p>
<p>A human or an animal learns from the environment by <a href="https://www.cam.ac.uk/research/features/lifelong-learning-and-the-plastic-brain">re-structuring the connectivity of neurons in their brain</a>. The phenomenon has not been well understood yet but, in the abstract level, it is just like branching a graph, with billions of nodes and connections, to work better with the learning task at hand. The aim is to provide a generic (abstract) representation of the problem and the solution in the brain that is accessible when needed. This, however, is slightly different for a machine. The problem needs to first be translated into a âmathematical spaceâ (coordinate systems, or simply numbers),the instances (samples of the learning task) need to be presented by their basic components (so-called features) in a âvector spaceâ (numbers each with a particular meaning).</p>
</div>
<div id="sec:supervisedmodels" class="section level2">
<h2><span class="header-section-number">1.3</span> Supervised models</h2>
<p>Letâs assume a set of <span class="math inline">\(m\)</span> instances, <span class="math inline">\(X\)</span>, with <span class="math inline">\(m\)</span> rows and <span class="math inline">\(n\)</span> columns (each row is an instance and each column is a feature) and their associated responses, <span class="math inline">\(Y\)</span>, with <span class="math inline">\(m\)</span> rows, is given (called the training data set). A supervised model <span class="math inline">\(M(.,\theta)\)</span>, <span class="math inline">\(\theta\)</span> being model parameters (e.g., linear model), provides a generic rule by which the response, <span class="math inline">\(y\)</span>, is correctly estimated for any given instance, <span class="math inline">\(x\)</span> (<span class="math inline">\(1\)</span> row and <span class="math inline">\(n\)</span> columns). The parameters <span class="math inline">\(\theta\)</span> are optimized by an optimizer for the model <span class="math inline">\(M\)</span> to best satisfy an <em>evaluation</em> metric (in some algorithms, this metric is called the <em>loss function</em>) which evaluates how similar <span class="math inline">\(M(.,\theta)\)</span> is to actual <span class="math inline">\(y\)</span> for each <span class="math inline">\(x\)</span> and any given <span class="math inline">\(\theta\)</span>.</p>
<p>Usually, the given instances (training set) do not represent all possible instances for a given problem. Therefore, any model, in any shape and form, that transforms <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span> and optimizes the evaluation measure is acceptable. This means defining âthe best modelâ, a model that not only separates the given instances but also all other unseen instances, is limited by the knowledge encoded in the training set. This incomplete knowledge about the data leads the designer to make some <em>assumptions</em> on the model and the evaluation metric upon which different classifiers are formulated. For example, support vector machines in their original form assume that the discriminatory rule is represented by a line (an assumption on the model, a linear model) which has the maximum distance from the instances in each class (an assumption on the evaluation metric). The idea is that such a line is more empirically robust against potential uncertainties in unseen instances.</p>
<p>It is important to design the model carefully as it is responsible to represent the âpatternâ in the data in the most generic and accurate way. You may ask why not picking the most flexible mathematical equation in existence (e.g., Taylor series) and optimize its parameters to fit the given data? Flexible models (e.g., deep networks) may be able to fit the data well (simply because they are flexible and can fit anything), however, they might not be able to <em>generalize</em> well, i.e., work well for unseen data. Sometimes the models come from specific knowledge about the problem, e.g., physics models.</p>
</div>
<div id="sec:biasVariance" class="section level2">
<h2><span class="header-section-number">1.4</span> The bias-variance debate</h2>
<p>Assume there is a set of given instances, <span class="math inline">\(X\)</span> (m rows and n columns), and we are asked to optimize a model <span class="math inline">\(M(X, \theta)\)</span> to estimate <span class="math inline">\(Y\)</span> (m rows) given some examples, i.e., supervised learning. Let us call each instance <span class="math inline">\(x\)</span> and its corresponding desired output as <span class="math inline">\(y\)</span>. In reality, <span class="math inline">\(y\)</span> is the outcome of a system working with inputs <span class="math inline">\(x\)</span>, and some noise, <span class="math inline">\(y=f(x)+\epsilon\)</span>, where <span class="math inline">\(f(.)\)</span> is not known and <span class="math inline">\(\epsilon\)</span> is noise which is also unknown. For example, if <span class="math inline">\(x\)</span> represents characteristics of people (smoking habit, weight, genetics, etc.) and <span class="math inline">\(y\)</span> is whether or not they would have a heart-attack after their 50s, then <span class="math inline">\(f(x)\)</span> is how body would respond to those characteristics and leads to a heart-attack or not, which is not really known. With modelling, we estimate this function <span class="math inline">\(f(x)\)</span>, given some examples and some assumptions on the shape of <span class="math inline">\(f\)</span>.</p>
<p>A model M is responsible to estimate observed <span class="math inline">\(Y\)</span> as <span class="math inline">\(Y=f(X)+\epsilon=M(X, \hat{\theta})+e\)</span>, where <span class="math inline">\(e\)</span> has <span class="math inline">\(m\)</span> rows and indicates the error from <span class="math inline">\(Y\)</span>. If <span class="math inline">\(e\)</span> is small then <span class="math inline">\(M(X, \hat{\theta})\)</span> is an accurate estimation of <span class="math inline">\(Y\)</span>, hence the model estimates not only <span class="math inline">\(f\)</span> but large amount of the noise <span class="math inline">\(\epsilon\)</span> in it. This takes place if the model <span class="math inline">\(M\)</span> is a complex, flexible, equation which is able to fit any complex behavior of <span class="math inline">\(Y\)</span> as function of <span class="math inline">\(X\)</span>, including all fluctuations resulting from the noise. Hence, the outputs of the model will be similar to the values of <span class="math inline">\(Y\)</span> (non-biased), which includes noisy fluctuations, which leads to a high variance (fluctuations usually lead to large variance). If the model is not complex though, the gap between the output of the model and <span class="math inline">\(Y\)</span> might be large (bias), however, it would fit less to the noisy fluctuations in the <span class="math inline">\(Y\)</span> which leads to smaller variance. That is why it is said a complex model has a large variance and small bias, and vice versa. Formal calculation of this trade off can be found in the <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Derivation">bias-variance tradeoff in wikipedia</a>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
