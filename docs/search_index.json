[
["index.html", "Machine learning, statistics, and optimization: A collection of intuitions Preface", " Machine learning, statistics, and optimization: A collection of intuitions Reza Bonyadi, Ph.D. 2019-08-11 Preface I wrote my first machine learning (ML) project in the autumn of 2000, which was a characters hand-written recognition in the Pascal language, and I quite enjoyed the topic ever since. The field is growing astronomically fast these days and everyday I am learning something new. One reason behind such fast growth is that the top largest companies in the world, i.e., Microsoft, Google, Amazon, and Apple, all invest their largest R&amp;D budgets on this. This collection (or as I call it, hyper-book) is meant to document and centralize the best links, descriptions, and materials in each topic I found for my learning (so, basically my notes in an organized way with links). The intention is not to re-write what has been written hundreds of times by very skilled authors, but to summarize methods, rank best pages/books which describe them, and refer to programming codes I found the best for that topic, if relevant. For each topic I provide my opinion and intuition on what it is, what are examples of it, and how to find more details. I keep the descriptions simple and to the point, with minimum mathematical equations, if possible. The idea of this collection is not to undermine any Mathematical finding and theoretical background related to the fields reviewed here (I personally am a fan), rather, to pair those with intuitions and examples to make these topics more accessible for and larger audience. It is also important to notice the way this collection has been organized, machine learning, statistics, and optimization, is not ideal, as it is difficult to separate these topics completely. I agree that many algorithms in machine learning these days are “old news” in the field of statistics (e.g., linear regression), while some other algorithms may not have been possible to invent if statistics was going to be the main stream (e.g., deep neural networks). Topics like derivative-based optimization are so heavily involved in machine learning these days to an extent to which they owe their maturity to some extent. As it is clear, it is really difficult to claim if different parts of this collection are really different, however, the grouping has been taken place for the sake of cleanness and familiarity of the titles. "],
["introduction.html", "Chapter 1 Introduction 1.1 Raw data vs characterized data (features)", " Chapter 1 Introduction Machine learning refers to a set of algorithms which make a machine (with a computer as its brain) to apparently learn. Learning is not Not fully observed, hence, there need to be assumptions as the solutions is not unique 1.1 Raw data vs characterized data (features) Feature space "],
["classification.html", "Chapter 2 Classification 2.1 0/1 loss function for classification 2.2 Logistic regression 2.3 Bayes classifier 2.4 Support vector machines 2.5 Decision tree 2.6 K-nearest neighbor 2.7 Gaussian process classifier 2.8 General additive model 2.9 Turning binary classifiers to multi-class 2.10 Performance measures and evaluation", " Chapter 2 Classification Classification refers to learning to categorize instances by labeled examples. For example, learning if a person going to have a heart attack after their 50’s given person’s characteristics such as smoking habits, history of a heart attack in the family, number of hours of exercise per day, height, weight, among others. The task is to find a general rule which estimates, for any given person with given characteristics, if the person is going to have a heart attack after their 50s. Another example of classification is, given some attributes of a set of tumors (e.g., shape, color, genetic information, history of the patient, among others) and their categories (benign, type 1, type 2, or type 3), find a generic rule that estimates the tumor type for any tumor. The rule (also known as the model) is optimized by given examples of the problem to be solved. In a more general framework, given a set of instances, \\(X\\), with \\(m\\) rows and \\(n\\) columns (each row is an instance and each column is an attribute) and their associated classes, \\(Y\\), with \\(m\\) rows (containing a categorical value in each row), find a mathematical rule (line and hyperplane are special cases of this), also known as a model, which can generate the class label, \\(y\\), for any given instance, \\(x\\) (\\(1\\) row and \\(n\\) columns). A synthetic classification problem has been shown in Fig. 2.1. In this problem, there are two attributes per instance and two groups of instances (blue and red), and the discriminatory rule has been assumed to be represented by a line. The aim of classification (binary in this case) with these assumptions is to find the “best” discriminatory line (hyperplane in a higher number of dimensions) which discriminates between the two classes “optimally” (see figure). Ideally, this optimality needs to make sure that the rule not only works for the given data (training data) based on a performance measure but also for any unseen data point (test data), referred to as the generalization ability of the rule. Figure 2.1: Two attributes (horizontal and vertical axes), two classes (red and blue), and three lines which can separate the classes successfully. As usually the given instances do not represent all possible instances for a given problem, therefore, any rule, in any shape and form, that separates the given instances and optimizes the performance measure is acceptable (purple, green, and orange lines in the figure). Hence, defining “the best discriminatory rule”, a rule that not only separates the given instances but also all other unseen instances, is not possible. This leads to different assumptions upon which different classifiers are formulated. For example, support vector machines assume that the discriminatory rule is represented by a line which has the maximum distance from the instances in each class. The idea is that such a line is more empirically robust against potential uncertainties in unseen instances. See (Ng and Jordan 2002) for a formal definition of discriminative classification. Key points: Classification aims to find a mathematical rule, aka model, that provides the category to which an instance belongs using a set of given examples. As the training data-set is a subset of all possible data points, some assumptions need to be made to ensure that the “optimal” rule performs well not only on the training data-set but also on unseen instances (generalization ability). These assumptions include the shapes of the mathematical rule (a line, “if-then” rules, etc.) and the performance measure by which the rule is evaluated. Different classification methods usually differ by their assumptions and performance measures used. 2.1 0/1 loss function for classification Let’s assume that the discriminatory rule (model) in a two-class classification problem , aka a binary classification problem, is represented by a line (hyperplane in more than 2 dimensions). The objective is to find a hyperplane that “optimally” discriminates between the classes. To do so, we need an evaluation procedure which measures a “wrongness” score for any given hyperplane (called the loss function). Then, an optimization algorithm searches over all possible hyperplanes to find the one which minimizes the loss function (see Fig. 2.2). Figure 2.2: The objective is to find a line/hyperplane that separates the instances optimally. The search continues until a suitable hyperplane, evaluated by the loss function, is found. The final line found in this example classifies all instances correctly except one. Ideally, we expect the optimizer to find a hyperplane for which all instances from each class fall in one side of it and the instances from the other class fall in the other side of the hyperplane. Hence, one simple loss function would be to count the number of instances that are not in the “correct” side of the hyperplane, which is called the 0/1 loss function. The outcome of this minimization is a hyperplane which has a minimum 0/1 loss value, i.e., the number of instances that are in the wrong side of the hyperplane is minimized. Figure 2.3: Two candidate lines, green gives 4/30 and the orange gives 4/30 loss value according to the 0/1 loss function. One should note that there might be many hyperplanes which have the same 0/1 loss value, which means the solutions to the 0/1 loss function is not unique. In Fig. 2.3, for example, the 0/1 loss value is the same for the green line and the orange line. 2.1.1 Improvements The 0/1 loss function in its original form does not take into account the importance of the instances from different classes (all classes are assumed to be equally important). In the previous example (Fig. 2.3), the number of blue instances is smaller than the number of red instances, which means miss-classification of blue instances leads to a larger natural loss comparing to the red instances (blue instances are rare, hence more important to be classified correctly). For example, the orange line has a very high accuracy on red instances while its accuracy on the blue is not as high as the green line. To address this issue, it has been proposed in (Bonyadi and Reutens 2019) to minimize the average percentage of mis-classified instances accross all classes rather than counting the number of mis-classified instances. In Fig. 2.3, for example, this average for the green line is \\(\\frac{(2/10+2/20)}{2}\\) (2 miss-classified instances from each class) and for the orange line is \\(\\frac{3/10+1/20}{2}\\) (3 miss-classified from blue and 1 from red). Hence, the green line would be preferred. Finding a line, or a hyperplane in larger number of dimensions, that minimizes the average of the 0/1 loss function in both above-mentioned forms is not easy (see (Doerr et al. 2015) and (Bonyadi and Reutens 2019)). One issue is that this line is not unique. For example, both orange and green lines in Fig. 1 have the same accuracy in terms of discriminating between the classes. This is solved by selecting the line (hyperplane) which has the maximum distance from the instances of both classes (see this (Bonyadi and Reutens 2019)), called maximum margin hyperplane. The basic idea is, if there are multiple lines (hyperplanes) which separate instances from each class with the same accuracy, the one which has the maximum distance from each class is preferred. Otehr improvements in (Bonyadi and Reutens 2019) included addition of \\(L_1\\) and \\(L_2\\) regularization, that enables the usage of coefficients for estimation of variable importance. 2.1.2 More details Any hyperplane can be represented by its normal vector (norm), \\(\\vec w\\), and an intercept, \\(b\\), \\(&lt;\\vec w, b&gt;\\). An instance \\(\\vec x_i\\) is classified by a given hyperplane as class 0 if \\(\\vec w \\vec x_i^T+b&lt;0\\) (\\(T\\) is the transpose) and class 1 otherwise. One way to define the 0/1 loss function in its original form is then as follows: \\[\\begin{equation} \\frac{1}{N}\\sum_i y_i(1-f(\\vec x_i))+(1-y_i)f(\\vec x_i) \\tag{2.1} \\end{equation}\\] Eq. (2.1) where \\(N\\) is the number of instances. For the 0/1 loss function, \\(f(.): \\mathbb{R}^n \\to \\mathbb{R}\\) is defined by: \\[\\begin{equation} f(\\vec x)= \\begin{cases} 0 &amp; \\vec w \\vec x_i^T+b&lt;0\\\\ 1 &amp; otherwise\\\\ \\end{cases} \\tag{2.2} \\end{equation}\\] This function has been shown in Fig. 2.4. Figure 2.4: The 0/1 function which returns 0 or 1 depending the side to which a given instance belong. The loss function was revised in (Bonyadi and Reutens 2019) as follows: \\[ \\frac{1}{K} \\sum_{i=1}^K \\left( \\frac{1}{N_i}\\sum_{j=1}^{N_i} \\left[ y_j(1-f(\\vec x_j))+(1-y_j)f(\\vec x_j)\\right] \\right) \\] This formulation is less biased in the case of imbalance number of instances in different classes. The value of k was set to 2 in that paper. There are two equivalent geometrical views to imagine the discrimination by a hyperplane. One is that the hyperplane, defined by \\(&lt;\\vec w, b&gt;\\), separates the two classes, which means it is placed somewhere between the instances from two classes in a way that (ideally) all instances from one class are in one side and all instances from the other class are on the other side of that hyperplane. The other view is that \\(\\vec w\\) is a linear transformation which transforms each instance \\(\\vec x_i\\) to a one dimensional line. The scalar \\(b\\) then is the threshold to determine the class label of the transformed instances. Figure 2.5: Two geometrical views: The given hyperplane equation defines a discriminatory hyperplane to separate the classes (black line in the left panel) OR the norm of the hyperplane transforms the instances to a one dimensional space (green line, and the right panel) and the intercept separates the classes. The right panel is essentially the histogram of instances along the green line in the left panel. There are multiple ways to find a hyper-plane which minimizes the original 0/1 loss function (sum of 0/1 losses), which have been described in details in (Doerr et al. 2015). Find also more detailes in (Bonyadi and Reutens 2019). 2.1.3 Pros and cons Pros: The 0/1 loss function in both above-mentioned forms is unbiased and not sensitive to outliers as any outlier would only contribute 1 unit to the loss function if it is miss-classified (see Fig. 2.6). Figure 2.6: 0/1 loss function is not sensitive to outliers while other loss functions are (other loss functions are described in next sections). The figure was taken from (Doerr et al. 2015) Cons: The main issue with this idea (optimal 0/1 binary classification, with or without maximum margin idea) is that optimizing the 0/1 loss function in the formed mentioned before is not practical, i.e., it is NP-Complete. Hence, all algorithms which implement this idea are rather slow. The only practically fast implementation has been described in (Bonyadi and Reutens 2019), which uses evolutionary strategy for optimization. This has encouraged introduction to lots of new loss functions which approximate the 0/1 loss function while they are differentiable. 2.1.4 Implementation Methods described in (Doerr et al. 2015) are in an algorithmic, step by step, format which makes implementation easier. The Python, Java, and Matlab code for (Bonyadi and Reutens 2019) is available (Bonyadi and Reutens 2018). 2.2 Logistic regression Logistic regression seeks a hyperplane which best discriminates two classes. The hyperplane is evaluated by a loss function which is a smooth (differentiable) estimation of the function used in the 0/1 loss function (see Section 2.1.2), hence, can be optimized effectively by a gradient descent. For a given hyperplane, the estimation provides a value between 0 and 1 (rather than 0 or 1 as it was the case in the 0/1 loss function) for each instance that represents to what extent the instance has been classified as the class 0 or class 1. While there might be many different choices for such function, logistic regression uses the logarithm sigmoid function that looks like Fig. 2.7. Figure 2.7: The logistic function is an estimation of the 0/1 function. In the figure, the \\(&lt;\\vec w, b&gt;\\) defines the distinguishing hyperplane (\\(\\vec w\\) is the norm of the plane and \\(b\\) is the intercept). The value of \\(\\vec w \\vec x_i^T + b\\) is an indication of the distance between the hyperplane and the instance while ths ign of it indicates to which side of the hyperplane ths instance belong. When we put this value for an instance in the logarithm sigmoid function, we would get a value close to zero when \\(\\vec w \\vec x_i^T + b&lt;0\\) and a value close to one otherwise. If \\(\\vec w \\vec x_i^T + b\\) is close to zero the sigmoid function returns values closer to \\(0.5\\), which may be interpreted as ambiguous. Ideal hyperplane would lead to large positive and large negative values for \\(\\vec w \\vec x_i^T + b\\) to ensure smaller loss value. 2.2.1 Variable importance The coefficients of the hyperplane found by the logistic regression cannot be interpreted directly as indicator for variable importance. The reason is that the variables ranges might be inherently different, meaning that some coefficients need to be larger to compensate for larger values. For example, if the values of one variable is in the range of 1000 and the other is in the range of 0.1, it is expected that the coefficients related to the first variable to be larger. This, however, does not show that the first variable is more important than the second. If the value of variables are standardized (see Section 11), however, the coefficients can be used as indicators of importance. The idea is that, the smaller the absolute value of a coefficient is the less important that variable is. To imagine this, think of a variable that is not important at all (see Fig. 2.8). This means that, from the perspective of that variable, the instances from both groups are the same. We expect the discriminatory hyperplane to have a small or zero coefficient value for that variable. Figure 2.8: ????. 2.2.2 Pros and cons It is easy to implement Can be effectively solved by second order optimization methods (see ????) 2.2.3 More details More details on this algorithm can be found in 2.2.4 Implementation (Andrew Ng 2016) Scipy and Sikit-Learn 2.3 Bayes classifier For a classification task, Bayes classifier calculates how likely it is that a given instance belongs to each class. Intuitively, the probability that a given instance, \\(\\vec x\\), belongs to a class \\(c\\) depends on two main components: Prior: How likely it is that any given instance belongs to the class \\(c\\) in general. Likelihood: If we know an instance from class \\(c\\), how likely it is that instance looks like \\(\\vec x\\). For any given instance, we calculate these two and multiply their values. The outcome is a measure (not exact) of the probability if that instance belongs to class \\(c\\). The larger this value, the more likely it is that the instance belongs to the class \\(c\\). This approach is called the Bayes optimal classifier. The first component is easy to calculate given the history. We can simply count the number of instances in the class \\(c\\) and divide that by the total number of training instances, which represents how likely it is that an instance come from that class. The second component is very difficult, if possible, to calculate if we assume that the attributes are not independent. The reason is each instance is a mix of multiple attributes, some might be the same as what has been observed in the training set, some might not be. The comparison between these instances to calculate the likelihood is difficult as all attributes need to be considered at the same time. If we assume independence between variables, however, that probability is calculated easily. This is called the Naive Bayes classifier because the independence assumption is somewhat “naive”. For a given instance \\(\\vec x\\), for each attribute, we calculate how likely it is that an instance from class \\(c\\) has the same value as of \\(\\vec x\\) for that attribute, i.e., attributes are independent. We then multiply those probabilities which would be an estimate of the original likelihood with mixed variables. 2.3.1 More details Bayes classifier calculates the probability of the class of an instance \\(\\vec x\\) belonging to class \\(c\\) given the instance, which is written as: \\[\\begin{equation} P(Y=c|\\vec x) = \\frac{P(\\vec x|Y=c)P(Y=c)}{P(\\vec x)} \\tag{2.3} \\end{equation}\\] where \\(P(\\vec x|Y=c)\\) is the likelihood of \\(\\vec x\\) belonging to class \\(c\\) (\\(Y\\) is the class label), \\(P(Y=c)\\) is the prior knowledge about the class \\(c\\) in general, and \\(P(\\vec x)\\) is called the evidence. Calculation of the evidence and likelihood is difficult as the instance \\(\\vec x\\) is a mixture of attributes, which may be dependent (see (Wikipedia 2016) for complete calculation of these probabilities). However, considering independence between variables, \\(P(\\vec x|Y=c)=P(x_1|Y=c)P(x_2|Y=c)...P(x_n|Y=c)\\). \\(P(Y=c)\\) is easy to calculate as it is equal to the number of instances in the class \\(c\\) divided by the number of instances in the training set. While the calculation of evidence is also straightforward under the independence assumption, it is not needed as it is constant across all classes. Let’s have an example. Table 2.1: An example data-set of male/female with their characteristics. Person Height (&gt;5.5 feet) Weight (&gt;150 lbs) Foot size(&gt;10 inches) male Y Y Y male Y Y N male Y N Y male Y Y Y female N N N female N N N female N N N female Y N N Given this data, we have been asked to which class a person belong if their Height is 6 (&gt;5.5, hence ‘Y’), their Weight is 130 (&lt;150, hence ‘N’), and their Foot size is 8 (&lt;10, hence ‘N’). Prior probability of being male is 0.5 and being female is 0.5, based on the table. It is clear that \\(P(x_1=Y|Y=female)=0.25\\), \\(P(x_2=N|Y=female)=P(x_3=N|Y=female)=1.0\\), and \\(P(Y=male)=P(Y=female)=0.5\\). Hence, \\(P(Y=female|\\vec x=&lt;Y, N, N&gt;)=\\frac{0.25 \\times 1.0 \\times 1.0 \\times 0.5}{P(\\vec x)}\\) and \\(P(Y=male|\\vec x=&lt;Y, N, N&gt;)=\\frac{1.0 \\times 0.25 \\times 0.25 \\times 0.5}{P(\\vec x)}\\).As the evidence is the same for both probabilities, it seems it is more likely that the given instance is a “female”. See this for more examples and descriptions. 2.3.2 Continuous variables For discrete variables it is rather easy to calculate the probabilities based on the given instances. For continuous variables (e.g., age, weight), however, this probability need to follow a distribution. Given the distribution, one can calculate the probabilities (Wikipedia 2016) 2.3.3 Pros and cons Pros: It is easy and fast to predict class of test data set. It also performs well in multi class prediction. When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data. It performs well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption). Cons: If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent. 2.3.4 Implementation https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/ For continuous variables, it is assumed that the variables follow a given description (usually normal) to be able to calculate the probability of . 2.4 Support vector machines What is support vector machine 2.4.1 Kernel trick 2.4.2 Some improvements 2.4.3 More details Also talk about dimensionality reduction 2.4.4 Implementation 2.5 Decision tree 2.6 K-nearest neighbor The k-nearest neighbor (KNN) classifier assumes that the instances “close” to one another have the same class label. Hence, to assign a class label to a new instance, KNN finds \\(k\\) instances from the training data set to which the new instance is “closest” and use those labels to vote for the class label of the new instance (see Fig. 2.9). Figure 2.9: Example of how KNN works. The “closest” instances to a new instance (green) are used to vote for its class label. The “closeness” is defined by a distance measure, such as Euclidean distance. 2.6.1 Improvements In KNN, some attributes may lead to a biased distance. For example, if one of the attributes is in the order of 1,000 and another is in the order of 0.1, the latter would have a small impact on the calculation of the distance. This is usually resolved by standardizing the space, which ensures all attributes are in the same range. This method, however, may break some structural integrity of the instances. Another issue is that some attributes might be misleading and their impact is better to be reduced. For example, Fig. 2.10 indicates that the horizontal dimension is responsible for the green instance to be of the type “red”. This, however, may not be correct as if the horizontal dimension shrinks then the green instance becomes closer to blue instances, which makes it a blue class. Figure 2.10: Example of how KNN works. The “closest” instances to a new instance (green) are used to vote for its class label. This indeed leads to an in-accuracy in KNN; the algorithm does not take into account the importance of attributes. This can be addressed by optimizing a metric which shrinks/contracts the space along different attributes to achieve the best transformation in which KNN performs best. See Section 5 for details. 2.6.2 Pros and cons Pros: It is easy to implement. It can model non-linearity. The assumption behind KNN is not parametric and only depends on the given data. Cons: Requires storage of the training data. This can be reduced as proposed by [???] The closeness needs to be defined. Euclidean distance is an obvious choice, however, it is not always optimal [????]. Requires searching for the closest instances in the training set, which is slow. This can be resolved by using smart search methods [????] Some of the dimensions may lead to biasing the distance KNN does not provide any information about the importance of attributes. This, however, is resolved by optimizing a metric to transform the space to represent the training data optimally (see section 5) 2.6.3 More details 2.7 Gaussian process classifier TODO 2.8 General additive model 2.9 Turning binary classifiers to multi-class One vs one One vs all 2.10 Performance measures and evaluation Consider we are solving a binary (2-class) classification problem using classification algorithm. Let’s assume there are 100 items of class 1 and 200 of the class 0 in our training set. The classifier uses this data set to learn the patter of the data and provide a general rule to classify the instances. After training, it can classify 75 items of the class 0 and 190 of the class 1 correctly. Now, the question is, how well the classifier is doing its job? One simple way is to calculate the error percentage: \\(100\\frac{75+190}{100+200}=88.3\\) percent. 2.10.1 Signal detection Signal detection claims that the error percentage is not accurate measure of performance as it ignores the frequency of the items. For example, in the example above, the number of items in class 1 is twice as much as the the number of items in the class 0. This is usually the case in real-world data sets, i.e., the number of items in classes is imbalance (the number of unhealthy subjects is much smaller than the number of healthy ones). This poses lots of complications to the evaluation of classification methods. For example, assume that the number of items in one class is 100 times larger than the number of items in the other. Mis-classification of items from the smaller class leads to more “catastrophic” decisions as the more rare events are usually the most valuable ones which need to be detected correctly. Specificty and ??? 2.10.2 receiver operating characteristic (ROC) and Area under the curve (AUC) 2.10.3 Confusion matrix 2.10.4 Benchmarking https://github.com/EpistasisLab/penn-ml-benchmarks https://www.openml.org/home 2.10.5 Stratified sampling 2.10.6 Cross validation and random permutation 2.10.7 Imbalance datas ets References "],
["regression.html", "Chapter 3 Regression 3.1 Linear regression 3.2 Decision tree for regression 3.3 Performance measures", " Chapter 3 Regression This introduces regression. 3.1 Linear regression 3.2 Decision tree for regression 3.3 Performance measures R^2 and Adjusted R^2 Mean squared error Error percentile: The percentage of instances which have smaller than x% of error (absolute of difference divided by the actual value). When the actual is zero we have a problem "],
["dimensionality-reduction.html", "Chapter 4 Dimensionality reduction 4.1 Matrix decomposition 4.2 Principle component analysis 4.3 T-SNE 4.4 Independent component analysis 4.5 Partial least square", " Chapter 4 Dimensionality reduction What is it Supervised and unsupervised 4.1 Matrix decomposition 4.1.1 Eigen decomposition https://datascienceplus.com/understanding-the-covariance-matrix/ 4.1.2 Singular value decomposition 4.2 Principle component analysis 4.3 T-SNE 4.4 Independent component analysis Is supervised 4.5 Partial least square "],
["sec-metricslearning.html", "Chapter 5 Metrics learning 5.1 Large margin nearest neighbor", " Chapter 5 Metrics learning What is it 5.1 Large margin nearest neighbor "],
["neural-networks.html", "Chapter 6 Neural networks 6.1 Multi-layer perceptron 6.2 Mixed density networks 6.3 Convolutional neural networks 6.4 Autoencoders 6.5 Generative adversial neural network", " Chapter 6 Neural networks Great question. Classification and regression are done by optimizing a “model” (a parametrized mathematical equation) which is expected to describe the data. A linear model is the simplest one while a deep network represents a highly flexible model for which the “Weights” are optimized by an optimization method (e.g., gradient descent) on the given data. If the number of data points is equal to or larger than the number of variables then the linear model has a unique global optimum, which is the only local optimum (so, basically one optimum). Hence, simple gradient descent can solve it to optimality (i.e., find the best linear model). For a highly flexible model like deep neural networks, the less data you have, the more local optima would appear in that highly flexible equation, which means there is a larger chance that the found local optimum by the optimization algorithm is far from the global optima. Hence, the more data you have the better until the global optimum is the only local optimum. Note, however, that if your network does not have many weights then you would not need many training instances too (as a rule of thumb). 6.1 Multi-layer perceptron 6.2 Mixed density networks 6.3 Convolutional neural networks 6.4 Autoencoders PCA’s objective is to find an invertible transformation from the original space of the data to a lower-dimensional space. In its original form, PCA seeks a linear transformation for this, however, it can be kernalized to enable the search for non-linear transformations. Autoencoder generalizes this idea, seeking a transformation (not necessarily linear) from original transformation to a lower-dimensional space (encoder) and another transformation that inverses the encoder (called decoder). This can be formulated in general as: let f(., ): R^p -&gt; R^n, and g(., ): R^n -&gt; R^p, p&lt; n, find and in a way that ||X-f(g(X, ), )|| is minimized. For PCA, g(X, ) is XM, where M is a n by p matrix (assuming X is m by n) and g(X, ) is psodu-inverse of M (which is p by n). Autoencoders are a type of deep neural networks that map the data to itself through a process of (non-linear) dimensionality reduction, followed by dimensionality expansion. Given X as an m by n matrix (m samples and n dimensions), M_i an n_i by n_{i+1} dimensional matrix, f_i a function R^{n_{i+1}} R^{n_{i+1}}, an autoencoder maps the samples by f_p(…f_3(f_2(f_1(XM_1)M_2)M_3)…M_p) etc., where p being the size of the last matrix, and it is equal to n. Usually, some middle layers have smaller number of dimensions than n (i.e., there exists an z where \\(n_z&lt;&lt;n\\)). The set of layers before z encode the information into n_i dimensions, while the layers after that decode the information back. The aim is to find the best values in M_is so that the, for each sample, the encoded and then decoded is the same. That means each sample is mapped to itself. Once optimized, the layer z embeds information in the n dimensions. Autoencoders can be used for dimensionality reduction (detach the decoder, all n-dimensional samples are encoded into z dimensions) and anomaly detection (normal samples are mapped back to themselves with less error comparing to abnormal instances). Figure 6.1: Two candidate lines, green gives 4/30 and the orange gives 4/30 loss value according to the 0/1 loss function. 6.5 Generative adversial neural network "],
["bayesian-inference.html", "Chapter 7 Bayesian inference", " Chapter 7 Bayesian inference Frequentists: p-value depends on the number of trials Bayesian: After each trial, the probability distribution is updated (prior is modified to get posterior) https://www.quantstart.com/articles/Bayesian-Linear-Regression-Models-with-PyMC3 https://twiecki.io/blog/2015/11/10/mcmc-sampling/ "],
["ensemble-techniques.html", "Chapter 8 Ensemble techniques 8.1 Random forest 8.2 Gradient boosting", " Chapter 8 Ensemble techniques AutoML 8.1 Random forest 8.2 Gradient boosting "],
["reinforcement-learning.html", "Chapter 9 Reinforcement learning 9.1 Q-learning", " Chapter 9 Reinforcement learning 9.1 Q-learning "],
["bagging-and-boosting.html", "Chapter 10 Bagging and boosting 10.1 Extreme boosted tree 10.2 Anomaly detection", " Chapter 10 Bagging and boosting 10.1 Extreme boosted tree 10.2 Anomaly detection "],
["sec-preprocessing.html", "Chapter 11 Preprocessing", " Chapter 11 Preprocessing "],
["introduction-1.html", "Chapter 12 Introduction", " Chapter 12 Introduction What is optimziation Examples Derivative-free vs with derivative "],
["optimization-problems.html", "Chapter 13 Optimization problems 13.1 Single and Multi objective 13.2 Constrains in problems 13.3 Dynamic optimization problems", " Chapter 13 Optimization problems 13.1 Single and Multi objective 13.2 Constrains in problems 13.3 Dynamic optimization problems "],
["use-of-derivative-in-optimization.html", "Chapter 14 Use of derivative in optimization", " Chapter 14 Use of derivative in optimization https://www.datasciencecentral.com/profiles/blogs/an-overview-of-gradient-descent-optimization-algorithms "],
["derivative-free-algorithms.html", "Chapter 15 Derivative-free algorithms 15.1 Finite difference 15.2 Population-based optimization", " Chapter 15 Derivative-free algorithms 15.1 Finite difference 15.2 Population-based optimization 15.2.1 Genetic algorithm 15.2.2 Evolutionary strategy 15.2.3 Covariance matrix adaptation 15.2.4 Particle swarm optimization "],
["introduction-2.html", "Chapter 16 Introduction", " Chapter 16 Introduction "],
["basics-statistics.html", "Chapter 17 Basics statistics 17.1 Correlation 17.2 Moments 17.3 Covariance matrix 17.4 Distributions", " Chapter 17 Basics statistics http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf 17.1 Correlation 17.2 Moments 17.3 Covariance matrix 17.4 Distributions "],
["statistical-analysis.html", "Chapter 18 Statistical analysis 18.1 Statistical tests 18.2 Causality 18.3 Anova", " Chapter 18 Statistical analysis 18.1 Statistical tests 18.2 Causality 18.3 Anova "],
["terms-and-notations.html", "Chapter 19 Terms and notations", " Chapter 19 Terms and notations Attributes of instances: Instances are characterized by their attributes. Dimensions, variables, dependent variables, "]
]
