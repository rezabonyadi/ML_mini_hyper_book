[
["index.html", "A Live Handbook of Machine learning, Statistics, and Optimization Preface", " A Live Handbook of Machine learning, Statistics, and Optimization Reza Bonyadi, Ph.D. 2020-04-02 Preface I wrote my first machine learning (ML) project in the autumn of 2000, which was a characters hand-written recognition in the Pascal language. The field of ML is growing astronomically fast these days and everyday I am learning something new. One reason behind such a fast growth is that the top largest companies in the world, i.e., Microsoft, Google, Amazon, and Apple, all invest their largest R&amp;D budgets on this topic (see Medium). This collection (or as I call it, hyper-book) is meant to document and centralize the best links, descriptions, and materials in each topic I found for my learning (so, basically my notes in an organized way with links). The intention is not to re-write what has been written hundreds of times by very skilled authors as books or blogs, but to summarize methods, list best pages/books which describe them, how they have been evolving recently, and refer to programming codes I found the best for that topic, if relevant. For each topic I provide my opinion and intuition on what it is, what are examples of it, and how to find more details. I keep the descriptions simple and to the point, with minimum mathematical equations, if possible. The idea of this collection is not to undermine any Mathematical finding and theoretical background related to the fields reviewed here (I personally am a fan). It is rather to pair those with intuitions and examples to make these topics more accessible for larger audience. It is also important to notice the way this collection has been organized (machine learning, statistics, and optimization) is not ideal as it is difficult to separate these topics completely. I agree that many algorithms in machine learning these days are “old news” in the field of statistics (e.g., linear regression), while some other algorithms may not have been possible to invent if statistics was going to be the main stream (e.g., deep neural networks). Topics like derivative-based optimization are so heavily involved in machine learning these days to an extent to which they owe their maturity to some extent. It is really difficult to claim if different parts of this collection are really different, however, the grouping has been taken place for the sake of cleanness and familiarity of the titles. Each section in this book is tagged whether the discussion in that section is going to be intuitive, based on the code, or deep mathematics. This would help the audience to pick the sections which they care about most, which makes the reading more effective and enjoyable. "],
["introduction.html", "Chapter 1 Introduction 1.1 Supervised and unsupervised learning 1.2 Supervised models", " Chapter 1 Introduction Machine learning refers to a set of algorithms which make a machine (with a computer as its brain) to apparently learn. While learning in animals and human is not very well understood, in machines it is simply to find a generic rule, given a set of examples, which Machine learning refers to a set of methodologies that make machines apparently “learning” to perform tasks. Although it is hard to define learning, it is related to progressive improvement in performing a task successfully. Intuitively there are two types of learning: self-learning (unsupervised) and learning from others (supervised). Not fully observed, hence, there need to be assumptions as the solutions is not unique 1.1 Supervised and unsupervised learning In supervised learning the learner learns to accomplish a task with supervision, i.e., there is a set of instances that the learner learns from, or there is a system that provides feedback to the actions of the learner (reinforcement). Imagine for example you need a system to learn how to drive. For a human, we just hire an instructor and train the person and done. This is precisely the same for a machine, we provide instances or a feedback loop so that the machine can evaluate its performance and learns. Applications of supervised learning include self-driving cars, stock market prediction, object recognition, natural language processing, maintenance prediction, recommendation engines, weather forecast, and more. In the unsupervised learning, the algorithm “figures out” the patterns in the instances, without any supervision. This is the more sophisticated while the most exciting use of machine learning. Customer segmentation, anomaly detection, product similarities, and many other applications require unsupervised learning. A human or an animal learns from the environment by re-structuring the connectivity of neurons in their brain. The phenomenon has not been well understood yet but, in the abstract level, it is just like branching a graph, with billions of nodes and connections, to work better with the learning task at hand. The aim is to provide a generic (abstract) representation of the problem and the solution in the brain that is accessible when needed. This, however, is slightly different for a machine. The problem needs to first be translated into a “mathematical space” (coordinate systems, or simply numbers),the instances (samples of the learning task) need to be presented by their basic components (so-called features) in a “vector space” (numbers each with a particular meaning). 1.2 Supervised models Let’s assume a set of \\(m\\) instances, \\(X\\), with \\(m\\) rows and \\(n\\) columns (each row is an instance and each column is a feature) and their associated responses, \\(Y\\), with \\(m\\) rows, is given (called the training data set). A supervised model \\(M(.,\\theta)\\), \\(\\theta\\) being model parameters (e.g., linear model), provides a generic rule by which the response, \\(y\\), is correctly estimated for any given instance, \\(x\\) (\\(1\\) row and \\(n\\) columns). The parameters \\(\\theta\\) are optimized by an optimizer for the model \\(M\\) to best satisfy an evaluation metric (in some algorithms, this metric is called the loss function) which evaluates how similar \\(M(.,\\theta)\\) is to actual \\(y\\) for each \\(x\\) and any given \\(\\theta\\). Usually, the given instances (training set) do not represent all possible instances for a given problem. Therefore, any model, in any shape and form, that transforms \\(X\\) to \\(Y\\) and optimizes the evaluation measure is acceptable. This means defining “the best model”, a model that not only separates the given instances but also all other unseen instances, is limited by the knowledge encoded in the training set. This incomplete knowledge about the data leads the designer to make some assumptions on the model and the evaluation metric upon which different classifiers are formulated. For example, support vector machines in their original form assume that the discriminatory rule is represented by a line (an assumption on the model, a linear model) which has the maximum distance from the instances in each class (an assumption on the evaluation metric). The idea is that such a line is more empirically robust against potential uncertainties in unseen instances. It is important to design the model carefully as it is responsible to represent the “pattern” in the data in the most generic (low variance, see Section 15.1) and accurate (low bias) way. You may ask why not picking the most flexible mathematical equation in existence (e.g., Taylor series) and optimize its parameters to fit the given data? Flexible models (e.g., deep networks) may be able to fit the data well (simply because they are flexible and can fit anything), however, they might not be able to generalize well, i.e., work well for unseen data. Sometimes the models come from specific knowledge about the problem, e.g., physics models. "],
["sec-classification.html", "Chapter 2 Classification 2.1 Generative vs. discriminative classifiers 2.2 0/1 loss function for classification 2.3 Logistic regression 2.4 Bayes classifier 2.5 Support vector machines 2.6 Decision tree 2.7 K-nearest neighbor 2.8 Gaussian process classifier 2.9 General additive model 2.10 Turning binary classifiers to multi-class 2.11 Performance measures and evaluation", " Chapter 2 Classification Classification aims to optimize a mathematical rule, aka model, that provides the category to which an instance belongs using a set of given examples (see Section 1.1 and 1.2). An example of a classification problem is: given characteristics of a person, such as smoking habits, history of a heart attack in the family, number of hours of exercise per day, height, weight, among others, whether the person going to have a heart attack after their 50’s (yes/no). Another example of a classification problem is to find the type of a tumor (benign, type 1, type 2, or type 3) given its characteristics (e.g., shape, color, genetic information, history of the patient, among others). As the training data-set is a subset of all possible data points, any model, in any shape and form, that separates the given instances and optimizes the evaluation measure is acceptable (purple, green, and orange lines in the Fig. 2.1). Because of this incomplete knowledge, some assumptions need to be made to ensure that the “optimized” model performs well not only on the training data-set but also on unseen instances (generalization ability). This means defining “the best model”, a model that not only separates the given instances but also all other unseen instances, is limited by the knowledge encoded in the training set. These assumptions include the shapes of the mathematical rule (a line, “if-then” rules, etc.) and the evaluation metric by which the rule is evaluated. Different classification methods usually differ by their assumptions on the model, evaluation metric, and optimization method used. Figure 2.1: Two attributes (horizontal and vertical axes), two classes (red and blue), and three lines which can separate the classes successfully. To design a classifier, one would need the training data, the model (e.g., linear), an evaluation metric (e.g., maximum empirical margin in support vector machines), and an optimization algorithm to optimize the model parameters given the evaluation metric and the data. The evaluation metric is responsible to inform the optimizer on how well the current parameters perform while ensure the generalization ability is not sacrificed. 2.1 Generative vs. discriminative classifiers See (Ng and Jordan 2002) for a formal definition of discriminative classification. 2.2 0/1 loss function for classification Assuming a binary classification problem (two classes) with a linear model, the objective is to find a hyperplane that “optimally” discriminates between the classes. To do so, we need an evaluation procedure which measures a “wrongness” score for any given hyperplane (called the loss function), see 2 section. Then, an optimization algorithm searches over all possible hyperplanes to find the one which minimizes the loss function (see Fig. 2.2). Figure 2.2: The objective is to find a line/hyperplane that separates the instances optimally. The search continues until a suitable hyperplane, evaluated by the loss function, is found. The final line found in this example classifies all instances correctly except one. One simple loss function would be to count the number of instances that are not in the “correct” side of the hyperplane, called the 0/1 loss function. The outcome of this minimization is a hyperplane which has a minimum 0/1 loss value, i.e., the number of instances that are in the wrong side of the hyperplane is minimized. Figure 2.3: Two candidate lines, green gives 4/30 and the orange gives 4/30 loss value according to the 0/1 loss function. One should note that there might be many hyperplanes which have the same 0/1 loss value, which means the solutions to the 0/1 loss function is not unique. In Fig. 2.3, for example, the 0/1 loss value is the same for the green line and the orange line. 2.2.1 Improvements The 0/1 loss function in its original form does not take into account the importance of the instances from different classes (all classes are assumed to be equally important). To address this issue, it has been proposed in (Bonyadi and Reutens 2019) to minimize the average percentage of mis-classified instances accross all classes rather than counting the number of mis-classified instances. This process does not provide a unique line. Both orange and green lines in Fig. 2.1 have the same accuracy in terms of discriminating between the classes. This is solved by selecting the line (hyperplane) which has the maximum distance from the instances of both classes (see this (Bonyadi and Reutens 2019)), called maximum margin hyperplane, the one which has the maximum distance from each class is preferred. Other improvements in (Bonyadi and Reutens 2019) included addition of \\(L_1\\) and \\(L_2\\) regularization, that enables the usage of coefficients for estimation of variable importance. 2.2.2 Pros and cons Category Intuitive Pros: The 0/1 loss function is unbiased and not sensitive to outliers as any outlier would only contribute 1 unit to the loss function if it is miss-classified (see Fig. 2.4). Figure 2.4: 0/1 loss function is not sensitive to outliers while other loss functions are (other loss functions are described in next sections). The figure was taken from (Doerr et al. 2015) Cons: The main issue with this idea (optimal 0/1 binary classification, with or without maximum margin idea) is that optimizing the 0/1 loss function in the formed mentioned before is not practical, i.e., it is NP-Complete. Hence, all algorithms which implement this idea are rather slow. The only practically fast implementation has been described in (Bonyadi and Reutens 2019), which uses evolutionary strategy for optimization. This has encouraged introduction to lots of new loss functions which approximate the 0/1 loss function while they are differentiable. 2.2.3 Implementation Category Code Methods described in (Doerr et al. 2015) are in an algorithmic, step by step, format which makes implementation easier. The Python, Java, and Matlab code for (Bonyadi and Reutens 2019) is available here. 2.3 Logistic regression Category Intuitive Logistic regression seeks a hyperplane which best discriminates two classes. The hyperplane is evaluated by a loss function which is a smooth (differentiable) estimation of the function used in the 0/1 loss function, hence, can be optimized effectively by a gradient descent. While there might be many different choices for such function, logistic regression uses the logarithm sigmoid function that looks like Fig. 2.5. Figure 2.5: The logistic function is an estimation of the 0/1 function. More details on this algorithm can be found in Stanford University Tutorial on Supervised Learning. A derivative of Logistic Regression is the General Linear Model (GLM). ??? 2.3.1 Interpretability Category Intuitive The coefficients of the hyperplane found by the logistic regression cannot be interpreted directly as indicator for variable importance. The reason is that the variables ranges might be inherently different, meaning that some coefficients need to be larger to compensate for larger values. For example, if the values of one variable is in the range of 1000 and the other is in the range of 0.1, it is expected that the coefficients related to the first variable to be larger. This, however, does not show that the first variable is more important than the second. If the value of variables are standardized (see Section 12), however, the coefficients can be used as indicators of importance. The smaller the absolute value of a coefficient is, the less important that variable is. To imagine this, think of a variable that is not important at all (see Fig. 2.6), i.e., from the perspective of that variable, the instances from both groups are the same. We expect the discriminatory hyperplane to have a small or zero coefficient value for that variable. Figure 2.6: ????. 2.3.2 Pros and cons Category Intuitive Pros It is easy to implement Can be effectively solved by second order optimization methods It supports sparse representation of data Cons Might not provide the most generalizable line 2.3.3 Implementation Category Code Implementation from scratch: see Stanford University Tutorial on Supervised Learning. In Python: the Scipy library and the Sikit-Learn library both have implementation of this. 2.4 Bayes classifier Category Intuitive For a classification task, Bayes classifier calculates how likely it is that a given instance belongs to each class. Intuitively, the probability that a given instance, \\(\\vec x\\), belongs to a class \\(c\\) depends on two main components: Prior: How likely it is that any given instance belongs to the class \\(c\\) in general. Likelihood: If we know an instance from class \\(c\\), how likely it is that instance looks like \\(\\vec x\\). For any given instance, we calculate these two and multiply their values. The outcome is a measure (not exact) of the probability if that instance belongs to class \\(c\\). The larger this value, the more likely it is that the instance belongs to the class \\(c\\). This approach is called the Bayes optimal classifier. The first component is easy to calculate given the history. We can simply count the number of instances in the class \\(c\\) and divide that by the total number of training instances, which represents how likely it is that an instance come from that class. The second component is very difficult, if possible, to calculate if we assume that the attributes are not independent. The reason is each instance is a mix of multiple attributes, some might be the same as what has been observed in the training set, some might not be. The comparison between these instances to calculate the likelihood is difficult as all attributes need to be considered at the same time. If we assume independence between variables, however, that probability is calculated easily. This is called the Naive Bayes classifier because the independence assumption is somewhat “naive”. For a given instance \\(\\vec x\\), for each attribute, we calculate how likely it is that an instance from class \\(c\\) has the same value as of \\(\\vec x\\) for that attribute, i.e., attributes are independent. We then multiply those probabilities which would be an estimate of the original likelihood with mixed variables. See wikipedia page and this for examples. For discrete variables it is rather easy to calculate the probabilities based on the given instances. For continuous variables (e.g., age, weight), however, this probability needs to follow a distribution. Given the distribution, one can calculate the probabilities. The wikipedia page provides very good description on this algorithm. 2.4.1 Pros and cons Category Intuitive Pros: It is easy and very fast to predict class of test data set. When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data. It performs well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption and might not be correct). It works well with sparse data-sets. Organically handle multiple classes. They are generative and can be easily interpreted as discriminative Missed values can be easily dealt with If the value for a variable is missed completely, they would still work with simple tricks (not the case for many classifiers) Cons: If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent. For continuous variables, it is assumed that the variables follow a given distribution (usually normal) to be able to calculate the probability, which may not be accurate. 2.4.2 Implementation Category Code For implementation from scratch see here. In Python, scikit-learn can be used. 2.5 Support vector machines Consider a binary classification problem and assume we want to optimize a hyperplane (linear model) to separate the classes. There might be many hyperplanes which separate the classes, all of which would result in the same mis-classification error. This encourages design of constraints which narrow down acceptable hyperplanes. One reasonable constraint is to pick the hyperplane which has maximum distance from the instances from both classes. This idea forms the bases for the support vector machine (SVM). Figure 2.7: Many hyperplanes may perform similarly in separating classes. Figure 2.8: The blue line has the same distance from the instances in each class, which is the idea behind SVM. It is expected that SVM works better than Logistic Regression on the unseen instances as it minimizes the empirical risk. This, however, is not always true as the assumption behind SVM might not be always true and it is bound by the quality of the training data (see Section 1.2). See wikipedia for more details on how this is formulated by the hinge loss and how it can be solved by gradient descent. See also this which describes SVM nicely. 2.5.1 Interpretability Similar to Logistic Regression, absolute value of coefficients of linear SVM provide variable importance only if the variables values have been standidized (see section 12) L1 regularization in SVM can be used for “feature selection” (see Section 15.2) 2.5.2 Improvements While SVM aims can be formulated as a quadratic program, it can also be formulated by a linear program. See (Zhou, Zhang, and Jiao 2002) for details. SVM can be also used for supervised dimensionality reduction, see (Tao, Chu, and Wang 2008) and section ????. Twin SVM, a more recent variant of SVM, is very powerful (much better and faster than SVM) (Khemchandani, Chandra, and others 2007). See an implementation here One-class SVM is also used for anomaly/outlier/novelty detection. See ????. SVM can be extended to provide non-linear hyperplanes to seperate the classes. This can be done by adding Kernel (see kernel trick and representer theorem) to the algorithm. See wikipedia for how. Figure 2.9: The blue line has the same distance from the instances in each class, which is the idea behind SVM. 2.5.3 Implementation See ??? for implementation from scratch. It is also available with sklearn in Python. 2.6 Decision tree http://www.r2d3.us/visual-intro-to-machine-learning-part-1/ 2.7 K-nearest neighbor Category intuitive The k-nearest neighbor (KNN) classifier assumes that the instances “close” to one another have the same class label. Hence, to assign a class label to a new instance, KNN finds \\(k\\) instances from the training data set to which the new instance is “closest” and use those labels to vote for the class label of the new instance (see Fig. 2.10). Figure 2.10: Example of how KNN works. The “closest” instances to a new instance (green) are used to vote for its class label. The “closeness” is defined by a distance measure, such as Euclidean distance. 2.7.1 Improvements Category intuitive In KNN, some attributes may lead to a biased distance. For example, if one of the attributes is in the order of 1,000 and another is in the order of 0.1, the latter would have a small impact on the calculation of the distance. This is usually resolved by standardizing the variables, which ensures all attributes are in the same range. This method, however, may break some structural integrity of the instances. See section ????. Another issue is that some attributes might be misleading and their impact is better to be reduced. For example, Fig. 2.11 indicates that the horizontal dimension is responsible for the green instance to be of the type “red”. This, however, may not be correct as if the horizontal dimension shrinks then the green instance becomes closer to blue instances, which makes it a blue class from KNN perspective. Figure 2.11: Example of how KNN works. The “closest” instances to a new instance (green) are used to vote for its class label. This indeed leads to an in-accuracy in KNN; the algorithm does not take into account the importance of attributes. This can be addressed by optimizing a metric which shrinks/contracts the space along different attributes to achieve the best transformation in which KNN performs best. See Section 5 for details. 2.7.2 Pros and cons Category Intuitive Pros: It is easy to implement. It can model non-linearity. The assumption behind KNN is not parametric and only depends on the given data. Cons: Requires storage of the training data. This can be reduced as proposed by [???] The closeness needs to be defined. Euclidean distance is an obvious choice, however, it is not always optimal [????]. Requires searching for the closest instances in the training set, which is slow. This can be resolved by using smart search methods [????] Some of the dimensions may lead to biasing the distance KNN does not provide any information about the importance of attributes. This, however, is resolved by optimizing a metric to transform the space to represent the training data optimally (see section 5) 2.7.3 Implementation Category Code It is available with sklearn in oython 2.8 Gaussian process classifier TODO https://distill.pub/2019/visual-exploration-gaussian-processes/ 2.9 General additive model 2.10 Turning binary classifiers to multi-class One vs one One vs all 2.11 Performance measures and evaluation Category Intuitive https://medium.com/@wilamelima/metrics-to-measure-machine-learning-model-performance-e8c963665476 Consider we are solving a binary (2-class) classification problem using a classification algorithm. Let’s assume there are 100 items of class 1 and 200 of the class 0 in our training set. The classifier uses this data set to learn the patter of the data and provide a general rule to classify the instances. After training, it can classify 75 items of the class 0 and 190 of the class 1 correctly. Now, the question is, how well the classifier is doing its job? One simple way is to calculate the error percentage: \\(100\\frac{75+190}{100+200}=88.3\\) percent. But is this number a good indicative of how well the classifier is performing? 2.11.1 Signal detection Category Intuitive Signal detection claims that the error percentage is not an accurate measure of performance as it ignores the frequency of the items. For example, in the example above, the number of items in class 0 is twice as much as the the number of items in the class 1. This is usually the case in real-world data sets, i.e., the number of items in classes is imbalance (the number of unhealthy subjects is much smaller than the number of healthy ones). This poses lots of complications to the evaluation of classification methods. For example, assume that the number of items in one class is 100 times larger than the number of items in the other. Mis-classification of items from the smaller class leads to more “catastrophic” decisions as the more rare events are usually the most valuable ones which need to be detected correctly. Traditionally, it is assumed that the items in class 1 are the ones we want to recognize (e.g., unhealthy subjects) and items in class 0 are “normal” items. In reality, the frequency of items in class 1 is usually much smaller than the items in class 0. Note that, algebraically, the class labels do not make any difference to the problem and its solution. Here, however, for the sake of definitions, we consider class 1 as positive recognition of items. Signal detection measures four metrics to quantify performance: False positive, true positive, false negative, and true negative. * False positive (aka false alarm) is the number of responses which the classifier recognized as class 1 while they actually belong to class 0 (i.e., falsely recognized as positive). This is very important to be high when the number of instances in the classes is not balance. For example, if a classifier which is used for diagnosing an illness has a high false positive, it is likely to diagnose a healthy person as ill. Another example would be using a classifier to either invest or not to invest money on a business. High false positive would lead to loss of money. * True positive (aka hit) is the number of responses which the classifier recognized as 1 and they actually belong to class 1. We desire this to be as high as possible. This, however, might not be as important as it sounds. For example, in the investment example, one would prefer to deal with a low false positive and not to loose money than always invest correctly. In fact, a less frequent correct investment (low true positive) is ok, but not loosing money (low false positive) is very important. * False negative (aka miss) is the number of responses which the classifier recognized as 0 and they actually belong to class 1. This is particularly important in the illness diagnosis example. In fact, it is preferred to diagnose someone with a terminal illness (high false positive) and prescribe some more tests than missing if the person has a terminal illness and lead to their death. * True negative (aka ) is the number of responses which the classifier recognized as 0 and they actually belong to class 0. These four measures provide a better tool to evaluate the performance of the classifier. The importance of these factors, however, is problem dependent, as described by examples. Specificty and ??? 2.11.2 Receiver operating characteristic (ROC) and Area under the curve (AUC) Category Intuitive 2.11.3 Confusion matrix Category Intuitive 2.11.4 Benchmarking Category Intuitive, Code https://github.com/EpistasisLab/penn-ml-benchmarks https://www.openml.org/home 2.11.5 Stratified sampling Category Intuitive 2.11.6 Cross validation and random permutation Category Intuitive, Deep 2.11.7 Imbalance data sets Category Intuitive, Deep References "],
["regression.html", "Chapter 3 Regression 3.1 Linear regression 3.2 Multivariate Adaptive Regression Spline 3.3 Generalized additive model 3.4 Decision tree for regression 3.5 Performance measures", " Chapter 3 Regression This introduces regression. 3.1 Linear regression https://www.deeplearning.ai/ai-notes/optimization/ 3.2 Multivariate Adaptive Regression Spline https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline 3.3 Generalized additive model 3.4 Decision tree for regression http://www.r2d3.us/visual-intro-to-machine-learning-part-1/ 3.5 Performance measures R^2 and Adjusted R^2 Mean squared error Error percentile: The percentage of instances which have smaller than x% of error (absolute of difference divided by the actual value). When the actual is zero we have a problem "],
["dimensionality-reduction.html", "Chapter 4 Dimensionality reduction 4.1 More details 4.2 Principal component analysis 4.3 T-SNE 4.4 Independent component analysis 4.5 Partial least square 4.6 Linear discriminant analysis 4.7 SVM dimensionality reduction", " Chapter 4 Dimensionality reduction Assume that you have a classification problem, an input dataset \\(X\\), with \\(m\\) instances (rows) and \\(n\\) features (columns), and response \\(Y\\). If \\(n\\) is very large then it might lead to performance issues with some algorithms for classification. Hence, it is desirable to find a way to represent the same dataset, with all \\(m\\) rows, with smaller number of features. This can be done in two ways: ignoring the responses (supervised dimensionality reduction) or taking into account \\(Y\\) (supervised). Unsupervised dimensionality reduction finds a representation of \\(X\\) with smaller number of features. This, however, may damage some structures in the data that are important for the classification task. Supervised dimensionality reduction finds a representation with smaller \\(n\\), taking into account the classes \\(Y\\). 4.1 More details This can be formulated in general as: let f(., ): R^p -&gt; R^n, and g(., ): R^n -&gt; R^p, p&lt; n, find and in a way that ||X-f(g(X, ), )|| is minimized. For PCA, g(X, ) is XM, where M is a n by p matrix (assuming X is m by n) and g(X, ) is psodu-inverse of M (which is p by n). 4.2 Principal component analysis 4.3 T-SNE 4.4 Independent component analysis Is supervised 4.5 Partial least square 4.6 Linear discriminant analysis 4.7 SVM dimensionality reduction https://ieeexplore.ieee.org/abstract/document/4378279 http://www.jmlr.org/papers/volume3/bi03a/bi03a.pdf "],
["sec-metricslearning.html", "Chapter 5 Metrics learning 5.1 Large margin nearest neighbor 5.2 Metric learning for kernel regression", " Chapter 5 Metrics learning What is it http://contrib.scikit-learn.org/metric-learn/ 5.1 Large margin nearest neighbor Distance Metric Learning for Large Margin Nearest Neighbor Classification https://pypi.org/project/PyLMNN/ 5.2 Metric learning for kernel regression http://proceedings.mlr.press/v2/weinberger07a/weinberger07a.pdf http://contrib.scikit-learn.org/metric-learn/ With sparse metric: https://arxiv.org/pdf/1712.09001.pdf "],
["neural-networks.html", "Chapter 6 Neural networks 6.1 Multi-layer perceptron 6.2 Mixed density networks 6.3 Convolutional neural networks 6.4 Autoencoders 6.5 Generative adversial neural network", " Chapter 6 Neural networks Great question. Classification and regression are done by optimizing a “model” (a parametrized mathematical equation) which is expected to describe the data. A linear model is the simplest one while a deep network represents a highly flexible model for which the “Weights” are optimized by an optimization method (e.g., gradient descent) on the given data. If the number of data points is equal to or larger than the number of variables then the linear model has a unique global optimum, which is the only local optimum (so, basically one optimum). Hence, simple gradient descent can solve it to optimality (i.e., find the best linear model). For a highly flexible model like deep neural networks, the less data you have, the more local optima would appear in that highly flexible equation, which means there is a larger chance that the found local optimum by the optimization algorithm is far from the global optima. Hence, the more data you have the better until the global optimum is the only local optimum. Note, however, that if your network does not have many weights then you would not need many training instances too (as a rule of thumb). http://playground.tensorflow.org/ 6.1 Multi-layer perceptron 6.2 Mixed density networks 6.3 Convolutional neural networks 6.4 Autoencoders PCA’s objective is to find an invertible transformation from the original space of the data to a lower-dimensional space. In its original form, PCA seeks a linear transformation for this, however, it can be kernalized to enable the search for non-linear transformations. Autoencoder generalizes this idea, seeking a transformation (not necessarily linear) from original transformation to a lower-dimensional space (encoder) and another transformation that inverses the encoder (called decoder). Autoencoders are a type of deep neural networks that map the data to itself through a process of (non-linear) dimensionality reduction, followed by dimensionality expansion. Given X as an m by n matrix (m samples and n dimensions), M_i an n_i by n_{i+1} dimensional matrix, f_i a function R^{n_{i+1}} R^{n_{i+1}}, an autoencoder maps the samples by f_p(…f_3(f_2(f_1(XM_1)M_2)M_3)…M_p) etc., where p being the size of the last matrix, and it is equal to n. Usually, some middle layers have smaller number of dimensions than n (i.e., there exists an z where \\(n_z&lt;&lt;n\\)). The set of layers before z encode the information into n_i dimensions, while the layers after that decode the information back. The aim is to find the best values in M_is so that the, for each sample, the encoded and then decoded is the same. That means each sample is mapped to itself. Once optimized, the layer z embeds information in the n dimensions. Autoencoders can be used for dimensionality reduction (detach the decoder, all n-dimensional samples are encoded into z dimensions) and anomaly detection (normal samples are mapped back to themselves with less error comparing to abnormal instances). Figure 6.1: Two candidate lines, green gives 4/30 and the orange gives 4/30 loss value according to the 0/1 loss function. 6.5 Generative adversial neural network "],
["bayesian-inference.html", "Chapter 7 Bayesian inference", " Chapter 7 Bayesian inference Frequentists: p-value depends on the number of trials Bayesian: After each trial, the probability distribution is updated (prior is modified to get posterior) https://www.quantstart.com/articles/Bayesian-Linear-Regression-Models-with-PyMC3 https://twiecki.io/blog/2015/11/10/mcmc-sampling/ "],
["ensemble-techniques.html", "Chapter 8 Ensemble techniques 8.1 Random forest 8.2 Gradient boosting", " Chapter 8 Ensemble techniques AutoML 8.1 Random forest 8.2 Gradient boosting "],
["reinforcement-learning.html", "Chapter 9 Reinforcement learning 9.1 Q-learning 9.2 Curiosity", " Chapter 9 Reinforcement learning 9.1 Q-learning 9.2 Curiosity In reinforcement learning, you have an agent that is exploring an environment to perform a task for which it receives an award if it does it right (or partially right). Now assume that the rewards are not that frequent (e.g., consider fishing, you only receive the reward once you capture a fish, while many steps are needed to actually capture the fish). In that case, the agent experiences difficulty to find even one simple way to receive a reward. As such, it will be difficult for the agent to learn the game effectively as it cannot extent its learning as there is no reward for most actions it take. Here comes curiosity. The idea is, the agent should avoid strategies it has tried before and lean towards strategies that have not been explored much. This provides an “intrinsic” reward for the agent which help to find new ways which, hopefully, would lead to more rewards. Figure 9.1: The objective is to find a line/hyperplane that separates the instances optimally. The search continues until a suitable hyperplane, evaluated by the loss function, is found. The final line found in this example classifies all instances correctly except one. A good example of where curiosity can be helpful is in the game Montezuma’s Revenge (see Fig. 9.1). The agent is required to find the “key” in the middle-left of the screen. To do so, it requires to find the path indicated in the figure, which is a quite complicated path. At the beginning, the agent has no idea about this environment, hence, it needs to explore options rather randomly. If the agent falls down or hits the skeleton, it will die. The agent does not receive any reward for most of the steps in this game, until it actually captures the key. Finding that successful path until the agent receives a reward randomly is close to impossible. In that case, because most paths lead to death rather than a reward, the agent would think that it would be better not to move at all to reduce the possibility of loss rather than finding any rewards. An intrinsic reward, here curiosity, would still keep the agent motivated to explore. 9.2.1 Implementation idea Curiosity provides intrinsic motivation for the agent to explore alternative solutions which have been investigated less frequently. One frequently used implementation is based on the following assumption: If the agent can guess what is the consequence of its move at the current step then the agent has tried this path before, hence, intrinsic reward would be lower. Adding this intrinsic reward to the total reward, the agent would receive a reward for the movements that are “novel”, which encourages the agent to be “curious”. See https://arxiv.org/pdf/1808.04355.pdf (https://arxiv.org/pdf/1808.04355.pdf). 9.2.2 What does it solve? "],
["bagging-and-boosting.html", "Chapter 10 Bagging and boosting 10.1 Extreme boosted tree", " Chapter 10 Bagging and boosting 10.1 Extreme boosted tree "],
["anomaly-detection.html", "Chapter 11 Anomaly detection 11.1 Autoencoder 11.2 One class SVM", " Chapter 11 Anomaly detection Highly imbalanced classification 11.1 Autoencoder 11.2 One class SVM "],
["sec-preprocessing.html", "Chapter 12 Preprocessing 12.1 Normalization and standardization", " Chapter 12 Preprocessing 12.1 Normalization and standardization "],
["sec-signalProcessing.html", "Chapter 13 Signal processing", " Chapter 13 Signal processing "],
["sec-causal-models.html", "Chapter 14 Causal models 14.1 Graphical models 14.2 Bayesian networks 14.3 Markov random field", " Chapter 14 Causal models 14.1 Graphical models Markov random field and conditional random field establish an undirected graphical model of variables relationship, interpreted as conditional association. Bayesian network (aka belief network) establishes a directed graphical model (usually asyclic) of variables relationship, interpreted as causality model. 14.2 Bayesian networks https://www.bayesfusion.com/bayesian-networks/?gclid=Cj0KCQjwmdzzBRC7ARIsANdqRRlBjvMJJ14NNnWx53iEbkVp_T-llMQIgL5VvvaA8AGCezAnHgvYiVcaAl-HEALw_wcB 14.2.1 Improvements https://arxiv.org/pdf/1803.01422.pdf 14.2.2 Implementations and libraries: https://github.com/xunzheng/notears https://github.com/jmoss20/notears 14.3 Markov random field 14.3.1 Implementation Sklearn: GraphicalLasso https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLasso.html#sklearn.covariance.GraphicalLasso https://github.com/skggm/skggm 14.3.2 Applications In clustering: https://scikit-learn.org/stable/auto_examples/applications/plot_stock_market.html#sphx-glr-auto-examples-applications-plot-stock-market-py https://cs.stanford.edu/people/jure/pubs/ticc-kdd17.pdf "],
["important-considerations-in-machine-learning-methods.html", "Chapter 15 Important considerations in machine learning methods 15.1 The bias-variance debate 15.2 Regularization 15.3 Raw data vs characterized data (features) 15.4 Variable importance and insights considerations", " Chapter 15 Important considerations in machine learning methods 15.1 The bias-variance debate Assume there is a set of given instances, \\(X\\) (m rows and n columns), and we are asked to optimize a model \\(M(X, \\theta)\\) to estimate \\(Y\\) (m rows) given some examples, i.e., supervised learning. Also, it is assumed that the underlying pattern of the data is not known (note that, this is a very important assumption as if this is not true then any model except the one which formulates the pattern is irrelevant). Another assumption is that the collected data may have some noise in it (another important assumption). If \\(M\\) is a flexible formula (which usually means it involves lots of parameters to complex formulations) then it can fit to the given data-set very well. This, however, means that the model would formulate also the noise in the data, which would impose lots of fluctuations and non-linearities that are not really related to the pattern in the data but the noise in the observations. This leads to a model that has a low bias. However, as it is very flexible and formulates lots of fluctuations coming from random events (noise), it also offer a high variance. This high variance and low bias would lead to worsening the generalization ability of the model as it is not really formulating the pattern in the data but also the noise and unnecessary fluctuations. Let’s go a bit more detail. Let us call each instance \\(x\\) and its corresponding desired output as \\(y\\). In reality, \\(y\\) is the outcome of a system working with inputs \\(x\\), and some noise, \\(y=f(x)+\\epsilon\\), where \\(f(.)\\) is not known and \\(\\epsilon\\) is noise which is also unknown. For example, if \\(x\\) represents characteristics of people (smoking habit, weight, genetics, etc.) and \\(y\\) is whether or not they would have a heart-attack after their 50s, then \\(f(x)\\) is how body would respond to those characteristics and leads to a heart-attack or not, which is not really known. With modelling, we estimate this function \\(f(x)\\), given some examples and some assumptions on the shape of \\(f\\). A model \\(M\\) is responsible to estimate observed \\(Y\\) as \\(Y=f(X)+\\epsilon=M(X, \\hat{\\theta})+e\\), where \\(e\\) has \\(m\\) rows and indicates the error from \\(Y\\). If \\(e\\) is small then \\(M(X, \\hat{\\theta})\\) is an accurate estimation of \\(Y\\), hence the model estimates not only \\(f\\) but large amount of the noise \\(\\epsilon\\) in it. This takes place if the model \\(M\\) is a complex, flexible, equation which is able to fit any complex behavior of \\(Y\\) as function of \\(X\\), including all fluctuations resulting from the noise. Hence, the outputs of the model will be similar to the values of \\(Y\\) (non-biased), which includes noisy fluctuations, which leads to a high variance (fluctuations usually lead to large variance). If the model is not complex though, the gap between the output of the model and \\(Y\\) might be large (bias), however, it would fit less to the noisy fluctuations in the \\(Y\\) which leads to a smaller variance. That is why it is said a complex model has a large variance and small bias, and vice versa. Formal calculation of this trade off can be found in the bias-variance tradeoff in wikipedia. 15.2 Regularization Category Deep Let’s assume that we are fitting a model \\(M(X, \\theta)\\) to the instances \\(X\\), given labels \\(Y\\), using an optimization algorithm and an evaluation metric, \\(E(M(X, \\theta), Y)\\), which provides a scaler describing how dissimilar is \\(M(X, \\theta)\\) to the corresponding \\(Y\\). \\[\\begin{equation} min_{\\theta} ~E(M(X, \\theta), Y) \\tag{15.1} \\end{equation}\\] Depending on the definition of \\(E\\) and \\(M\\), this problem might not have a unique solution. Hence, the optimization algorithm should be informed which solution is more acceptable so that it converges to what it supposed to. It is also possible that the optimizer perceives an “illusion” of better solutions, see (Bonyadi and Reutens 2019) for details. Here comes regularization. It provides another constraint, formulated into the objective function for convenience (see section 20.1.1 to see how is this possible), to make the solution unique and regulate the illusions, as follows: \\[\\begin{equation} min_{\\theta} ~E(M(X, \\theta), Y) + \\alpha R(\\theta) \\tag{15.2} \\end{equation}\\] where \\(R(\\theta)\\) is the regularization term and \\(\\alpha\\) is the regularization weight. 15.2.1 Famous types Category Deep As the idea is a “simpler” model has a better chance to generalize better (see Section 15.1), the regularization term is defined in a way that it simplifies the model. Two of most frequently used regularization terms are called \\(L_1\\) (aka LASSO) and \\(L_2\\) (special case of Tikhonov), defined by \\(||\\theta||_1=\\sum_i |\\theta_i|\\) and \\(||\\theta||_2=\\sum_i \\theta_i^2\\), respectively. 15.2.2 More details Category Deep The regularization introduced in Eq. (15.2) can be defined by a constrained optimization problem as follows: \\[\\begin{equation} min_{\\theta} ~E(M(X, \\theta), Y)+\\alpha \\epsilon s.t. R(\\theta) = \\epsilon \\end{equation}\\] The Lagrangian of this problem is equivalent to the original definition of regularization in Eq. (15.2). If \\(R(\\theta)\\) is the \\(L_1\\), this constraint limits the values of the parameters \\(\\theta\\) within a hyper cube, parameterized by \\(\\epsilon\\). If \\(L_2\\) is used, however, the parameter values are limited to a hyper-sphere, parameterized by \\(\\epsilon\\). See wekipedia for more information on this. Another useful regularization term is the \\(L_0\\) which counts the number of non-zero 15.3 Raw data vs characterized data (features) The data in its original form, raw form, represents what is measured. The measurements, however, are not always useful to learn from. For example, in a stock market signal, the actual close daily prices are not as important as the trend of those values. Hence, depending on the problem, some characteristics of the data might be more informative than the raw data itself. These characteristics are usually called features. Features basically represent important (relevant to a given problem which uses that data for solution) attributes of the problem. They are either designed by an expert (i.e., feature engineering) or optimized by an algorithm for a given model (i.e., feature formation, popular in deep learning). For example, assume that we want to characterize fruits. Some obvious features of fruits are their shapes (e.g., being round, or egg-shaped, or long), their tastes, and their smell. Features characterize attributes that distinguish different objects For many problems, the features are already known. For example, we already know that a feed in a newspaper is considered interesting for a particular reader if the reader spends more time on it (the spent time is a feature). However, recognizing someone’s face from an image is not easy to be defined by apparent features. This encourages methods like Deep Learning to not only learn the task but also find the best presentation for the problem, just from the raw data. 15.4 Variable importance and insights considerations Bootstrap aggregation Importance level References "],
["introduction-1.html", "Chapter 16 Introduction 16.1 Derivative-free vs with derivative optimization methods", " Chapter 16 Introduction Iterative - What is optimziation - Examples 16.1 Derivative-free vs with derivative optimization methods Tips: https://optimization.mccormick.northwestern.edu/index.php/Optimization_with_absolute_values https://download.aimms.com/aimms/download/manuals/AIMMS3OM_LinearProgrammingTricks.pdf "],
["optimization-problems.html", "Chapter 17 Optimization problems 17.1 Single and Multi objective 17.2 Constrains in problems 17.3 Dynamic optimization problems", " Chapter 17 Optimization problems 17.1 Single and Multi objective 17.2 Constrains in problems 17.3 Dynamic optimization problems "],
["use-of-derivative-in-optimization.html", "Chapter 18 Use of derivative in optimization", " Chapter 18 Use of derivative in optimization https://www.datasciencecentral.com/profiles/blogs/an-overview-of-gradient-descent-optimization-algorithms "],
["derivative-free-algorithms.html", "Chapter 19 Derivative-free algorithms 19.1 Finite difference 19.2 Population-based optimization", " Chapter 19 Derivative-free algorithms 19.1 Finite difference 19.2 Population-based optimization 19.2.1 Genetic algorithm 19.2.2 Evolutionary strategy 19.2.3 Covariance matrix adaptation 19.2.4 Particle swarm optimization "],
["what-are-constraints.html", "Chapter 20 What are Constraints 20.1 How to deal with constraints", " Chapter 20 What are Constraints 20.1 How to deal with constraints 20.1.1 Lagrangian "],
["famous-forms-of-optimization-problems.html", "Chapter 21 Famous forms of optimization problems 21.1 Linear program 21.2 Quadratic objective with linear constraints 21.3 Quadratic objective and constraints", " Chapter 21 Famous forms of optimization problems 21.1 Linear program 21.2 Quadratic objective with linear constraints 21.3 Quadratic objective and constraints "],
["introduction-2.html", "Chapter 22 Introduction", " Chapter 22 Introduction "],
["basics-statistics.html", "Chapter 23 Basics statistics 23.1 Correlation 23.2 Moments 23.3 Covariance matrix 23.4 Matrix decomposition 23.5 Distributions", " Chapter 23 Basics statistics http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf 23.1 Correlation 23.2 Moments 23.3 Covariance matrix https://datascienceplus.com/understanding-the-covariance-matrix/ 23.4 Matrix decomposition In Mathematics, matrix decomposition aims to decompose a given matrix into two or more matrices for which the dot product is equal to th original matrix. It is usually desirable that the matrices decomposed have some specific characteristics. For example, some have https://en.wikipedia.org/wiki/Matrix_decomposition 23.4.1 Eigen decomposition https://datascienceplus.com/understanding-the-covariance-matrix/ 23.4.2 Singular value decomposition 23.5 Distributions How to measure dissimilarity between distributions? https://towardsdatascience.com/earth-movers-distance-68fff0363ef2 "],
["statistical-analysis.html", "Chapter 24 Statistical analysis 24.1 Statistical tests 24.2 Causality 24.3 Anova", " Chapter 24 Statistical analysis 24.1 Statistical tests 24.2 Causality 24.3 Anova "],
["terms-and-notations.html", "Chapter 25 Terms and notations", " Chapter 25 Terms and notations Attributes of instances: Instances are characterized by their attributes. Dimensions, variables, dependent variables, "]
]
