[
["index.html", "Machine Learning, optimization, and statistics: A collection of intuitions Preface", " Machine Learning, optimization, and statistics: A collection of intuitions Reza Bonyadi, Ph.D. 2019-07-30 Preface I wrote my first machine learning (ML) project in the autumn of 2000, which was a characters hand-written recognition in the Pascal language, and I quite enjoyed the topic ever since. The field is growing astronomically fast these days and everyday I am learning something new. One reason behind such fast growth is that the top largest companies in the world, i.e., Microsoft, Google, Amazon, and Apple, all invest their largest R&amp;D budgets on this. This collection (or as I call it, hyper-book) is meant to document and centralize the best links, descriptions, and material in each topic I found for my learning. The intention is not to re-write what has been written hundereds of times by very skilled authors, but to summarize methods, rank best pages/books which describe them, and refer to programming codes I found the best for that topic, if relevant. For each topic I provide my oppinions on what it is and when/where/why/and how to use it. To ensure the audience of this collection covers a larger range, I keep the descriptions simple and to the point, with minimum mathematical equations, if possible. "],
["terms-and-notations.html", "Chapter 1 Terms and notations", " Chapter 1 Terms and notations Attributes of instances: Instances are characterized by their attributes. Dimensions, variables, dependent variables, "],
["introduction.html", "Chapter 2 Introduction", " Chapter 2 Introduction Machine learning refers to a set of algorithms which make a machine (with a computer as its brain) to apparently learn. Learning is not Not fully observed, hence, there need to be assumptions as the solutions is not unique "],
["classification.html", "Chapter 3 Classification 3.1 The optimal binary classifier 3.2 Logistic regression 3.3 Support vector machines 3.4 Decision tree 3.5 K-nearest neighbor 3.6 Gaussian process classifier 3.7 Turning binary classifiers to multi-class 3.8 Performance measures", " Chapter 3 Classification Classification refers to learning to categorize instances by examples. For example, learning if a person going to have a heart attach after their 50’s given person’s characteristics such as smoking habits, history of heart attack in the family, number of hours of exercise per day, hight, weight, among others. The task is to find a generic rule which estimates, for any given new person with given characteristics, if the person is going to have a heart attack after their 50s. Another example of classification is, given some attributes of a set of tumors (e.g., shape, color, genetic information, history of the patient, among others) and their categories (benign, type 1, type 2, or type 3), find a generic rule that estimates the tumor type for any tumor. In a more general framework, given a set of instances, \\(X\\), with \\(m\\) rows and \\(n\\) columns (each row is an instance and each column is an attribute) and their associated classes, \\(Y\\), with \\(m\\) rows (containing a categorical value in each row), find a mathematical rule (line and hyper-plane are special cases of this) which can generate the class label, \\(y\\), for any given instance, \\(x\\) (\\(1\\) row and \\(n\\) columns). A synthetic classification problem has been shown in Fig. 3.1. In this problem, we have two attributes per instance and two groups of instances (blue and red), and the discriminatory rule has been assumed to be represented by a line. The aim of classification (binary in this case) with these assumptions is to find the “best” discriminatory line (hyper-plane in higher number of dimensions) which discriminates between the two classes “optimally” (see figure). Ideally, this optimality needs to make sure that the rule not only works for the given data (training data) based on a performance measure but also for any unseen data point (test data), referred to as the generalization ability of the rule. Figure 3.1: Two attributes (horizontal and vertical axes), two classes (red and blue), and three lines which can separate the classes successfully. As usually the given instances do not represent all possible instances for a given problem, therefore, any rule, in any shape and form, that separates the given instances and optimizes the performance measure is acceptable (purple, green, and orange lines in the figure). Hence, defining “the best discriminatory rule”, a rule that not only separates the given instances but also all other unseen instances, is not possible. This leads to different assumptions upon which different classifiers are formulated. For example, support vector machines assume that the discriminatory rule is represented by a line which has the maximum distance from the instances in each class. The idea is that such line is more empirically robust against potential uncertainties in unseen instances. See this article (Bonyadi and Reutens 2019) for a formal definition of discriminative classification. Key points: Classification aims to find a mathematical rule that provides the category to which an instance belong using a set of given examples. As the training data-set is a subset of all possible data points, some assumptions need to be made to ensure that the “optimal” rule performs well not only on the training data-set but also on unseen instances (generalization ability). These assumptions lead to different classification algorithms. These assumptions include the shapes of the mathematical rule (a line, “if-then” rules, etc.) and the performance measure by which the rule is evaluated. 3.1 The optimal binary classifier Let’s assume that the discriminatory rule in a classification problem is represented by a line (hyper-plane in more than 2 dimensions), and we want to find a line that “optimally” discriminates between two classes. This optimality can be measured by a function that assigns a “wrongness” score to each line (called the loss function), which needs to be minimized. One simple function would be the number of instances that are not in the correct side of the line. This loss function returns a \\(0\\) if an instance is in the correct side of the given line and a \\(1\\) otherwise. Adding the output of this function for over all given instances provides a measure of how wrong the given line and is called the “0/1 loss function”. The aim is to find a line (hyper-plane) which minimizes this function by an optimization algorithm. The outcome of this minimization is a line which has a minimum 0/1 loss value, i.e., the number of instances that are in the wrong side of the line is minimized. Figure 3.2: Two candidate lines, green gives 4/30 and the orange gives 4/30 loss value according to the 0/1 loss function. In Fig. 3.2, for example, the 0/1 loss value is the same for the green line and the orange line. 3.1.1 Improvements The 0/1 loss function in its original form does not take into account the importance of the instances from different classes (all classes are assumed to be equally important). In the previous example (Fig. 3.2), the number of blue instances is smaller than the number of red instances, which means miss-classification of blue instances leads to a larger natural loss comparing to the red instances (blue instances are rare, hence more important to be classified correctly). For example, the orange line has a very high accuracy on red instances while its accuracy on the blue is not as high as the green line. To address this issue, it has been proposed in (Bonyadi and Reutens 2019) to minimize the average of the 0/1 loss values over the classes rather than than the summation of the 0/1 function across all instances (Bonyadi and Reutens 2019). In Fig. 3.2, for example, this average for the green line is \\(\\frac{(2/10+2/20)}{2}\\) (2 miss-classified instances from each class) and for the orange line is \\(\\frac{3/10+1/20}{2}\\) (3 miss-classified from blue and 1 from red). Hence, the green line would be preferred. Finding a line, or a hyper-plane in larger number of dimensions, that minimizes the average of the 0/1 loss function in both above-mentioned forms is not easy (see (Doerr et al. 2015) and (Bonyadi and Reutens 2019)). One issue is that this line is not unique. For example, both orange and green lines in Fig. 1 have the same accuracy in terms of discriminating between the classes. This is solved by selecting the line (hyper plane) which has the maximum distance from the instances of both classes (see this (Bonyadi and Reutens 2019)), which is called maximum margin hyper-plane. The basic idea is, if there are multiple lines (hyper-planes) which separate instances from each class with the same accuracy, the one which has the maximum distance from each class is preferred. 3.1.2 Pros and cons Pros: The 0/1 loss function in both above-mentioned forms is unbiased and not sensitive to outliers as any outlier would only contribute 1 unit to the loss function if it is miss-classified (see Fig. 3.3). Figure 3.3: 0/1 loss function is not sensitive to outliers while other loss functions are (other loss functions are described in next sections). The figure was taken from (Doerr et al. 2015) Cons: The main issue with this idea (optimal 0/1 binary classification, with or without maximum margin idea) is that optimizing the 0/1 loss function in the formed mentioned before is not practical, i.e., it is NP-Complete. Hence, all algorithms which implement this idea are rather slow. The only practically fast implementation has been described in (Bonyadi and Reutens 2019). This has encouraged introduction to lots of new loss functions which approximate the 0/1 loss function while they are differentiable. 3.1.3 How to implement There are multiple ways to find a hyper-plane which minimizes the original 0/1 loss function (sum of 0/1 losses), which have been described in details in (Doerr et al. 2015). The Python, Java, and Matlab code for (Bonyadi and Reutens 2019) is available (“Optimal-Margin Evolutionary Classifier–source Code” 2018). 3.2 Logistic regression Logistic regression is a classification technique that introduces a loss function that estimates the 0/1 loss function with a differentiable alternative, hence, can be optimized effectively by gradient descent. The 0/1 loss function is estimated by function that http://ufldl.stanford.edu/tutorial/supervised/LogisticRegression/ 3.2.1 Variable importance in LR 3.3 Support vector machines What is support vector machine 3.3.1 Kernel tricks 3.3.2 Implementations 3.4 Decision tree 3.5 K-nearest neighbor The k-nearest neighbor (KNN) classifier assumes that the instances “close” to one another have the same class label. Hence, to assign a class label to a new instances, KNN finds \\(k\\) instances from the training data set to which the new instance is “closest” and use those labels to vote for the class label of the new instance (see Fig. 3.4). Figure 3.4: Example of how KNN works. The “closest” instances to a new instance (green) are used to vote for its class label. The “closeness” is defined by a distance measure, such as Euclidean distance. 3.5.1 Improvements In KNN, some attributes may lead to a biased distance. For example, if one of the attributes is in the order of 1,000 and another is in the order of 0.1, the latter would have a small impact on the calculation of the distance. This is usually resolved by standardizing the space, which ensures all attributes are in the same range. This method, however, may break some structural integrity of the instances. Another issue is that some attributes might be misleading and their impact is better to be reduced. For example, Fig. 3.5 indicates that the horizontal dimension is responsible for the green instance to be of the type “red”. This, however, may not be correct as if the horizontal dimension shrinks then the green instance becomes closer to blue instances, which makes it a blue class. Figure 3.5: Example of how KNN works. The “closest” instances to a new instance (green) are used to vote for its class label. This indeed leads to in-accuracy in KNN; the algorithm does not take into account the importance of attributes. This can be addressed by optimizing a metric which shrinks/contracts the space along different attributes to achieve the best transformation in which KNN performs best. See Section 6 for details. 3.5.2 Pros and cons Pros: It is easy to implement. It can model non-linearity. The assumption behind KNN is not parametric and only depends on the given data. Cons: Requires storage of the training data. This can be reduced as proposed by [???] The closeness need to be defined. Euclidean distance is an obvious choice, however, it is not always optimal [????]. Requires searching for the closest instances in the training set, which is slow. This can be resolved by using smart search methods [????] Some of the dimensions may lead to biasing the distance KNN does not provide any information about the importance of attributes. This, however, is resolved by optimizing a metric to transform the space to represent the training data optimally (see section 6) 3.5.3 How to implement 3.6 Gaussian process classifier TODO 3.7 Turning binary classifiers to multi-class 3.8 Performance measures References "],
["regression.html", "Chapter 4 Regression 4.1 Linear regression 4.2 Decision tree for regression 4.3 Performance measures", " Chapter 4 Regression This introduces regression. 4.1 Linear regression 4.2 Decision tree for regression 4.3 Performance measures "],
["dimensionality-reduction.html", "Chapter 5 Dimensionality reduction 5.1 Principle component analysis 5.2 Singular value decomposition 5.3 T-SNE", " Chapter 5 Dimensionality reduction What is it Supervised and unsupervised 5.1 Principle component analysis 5.2 Singular value decomposition 5.3 T-SNE "],
["sec-metricslearning.html", "Chapter 6 Metrics learning 6.1 Large margin nearest neighbor", " Chapter 6 Metrics learning What is it 6.1 Large margin nearest neighbor "],
["neural-networks.html", "Chapter 7 Neural networks 7.1 Multi-layer perceptron 7.2 Mixed density networks 7.3 Convolutional neural networks 7.4 Autoencoders 7.5 Generative adversial neural network", " Chapter 7 Neural networks Great question. Classification and regression are done by optimizing a “model” (a parametrized mathematical equation) which is expected to describe the data. A linear model is the simplest one while a deep network represents a highly flexible model for which the “Weights” are optimized by an optimization method (e.g., gradient descent) on the given data. If the number of data points is equal to or larger than the number of variables then the linear model has a unique global optimum, which is the only local optimum (so, basically one optimum). Hence, simple gradient descent can solve it to optimality (i.e., find the best linear model). For a highly flexible model like deep neural networks, the less data you have, the more local optima would appear in that highly flexible equation, which means there is a larger chance that the found local optimum by the optimization algorithm is far from the global optima. Hence, the more data you have the better until the global optimum is the only local optimum. Note, however, that if your network does not have many weights then you would not need many training instances too (as a rule of thumb). 7.1 Multi-layer perceptron 7.2 Mixed density networks 7.3 Convolutional neural networks 7.4 Autoencoders 7.5 Generative adversial neural network "],
["bayesian-inference.html", "Chapter 8 Bayesian inference", " Chapter 8 Bayesian inference Frequentists: p-value depends on the number of trials Bayesian: After each trial, the probability distribution is updated (prior is modified to get posterior) https://www.quantstart.com/articles/Bayesian-Linear-Regression-Models-with-PyMC3 https://twiecki.io/blog/2015/11/10/mcmc-sampling/ "],
["ensemble-techniques.html", "Chapter 9 Ensemble techniques 9.1 Random forest 9.2 Gradient boosting", " Chapter 9 Ensemble techniques 9.1 Random forest 9.2 Gradient boosting "],
["reinforcement-learning.html", "Chapter 10 Reinforcement learning 10.1 Q-learning", " Chapter 10 Reinforcement learning 10.1 Q-learning "],
["bagging-and-boosting.html", "Chapter 11 Bagging and boosting 11.1 Extreme boosted tree 11.2 Anomaly detection", " Chapter 11 Bagging and boosting 11.1 Extreme boosted tree 11.2 Anomaly detection "],
["introduction-1.html", "Chapter 12 Introduction", " Chapter 12 Introduction What is optimziation Examples Derivative-free vs with derivative "],
["optimization-problems.html", "Chapter 13 Optimization problems 13.1 Single and Multi objective 13.2 Constrains in problems 13.3 Dynamic optimization problems", " Chapter 13 Optimization problems 13.1 Single and Multi objective 13.2 Constrains in problems 13.3 Dynamic optimization problems "],
["use-of-derivative-in-optimization.html", "Chapter 14 Use of derivative in optimization", " Chapter 14 Use of derivative in optimization "],
["derivative-free-algorithms.html", "Chapter 15 Derivative-free algorithms 15.1 Finite difference 15.2 Population-based optimization", " Chapter 15 Derivative-free algorithms 15.1 Finite difference 15.2 Population-based optimization 15.2.1 Genetic algorithm 15.2.2 Evolutionary strategy 15.2.3 Covariance matrix adaptation 15.2.4 Particle swarm optimization "]
]
