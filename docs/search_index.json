[
["index.html", "Optimization and Machine Learning: A Collection Preface", " Optimization and Machine Learning: A Collection Reza Bonyadi, Ph.D. 2019-07-27 Preface I wrote my first machine learning (ML) project in the autumn of 2000, which was a characters hand-written recognition in the Pascal language, and I quite enjoyed the topic ever since. The field is growing astronomically fast these days and everyday I am learning something new. One reason behind such fast growth is that the top largest companies in the world, i.e., Microsoft, Google, Amazon, and Apple, all invest their largest R&amp;D budgets on this. This collection (or as I call it, hyper-book) is meant to document and centralize the best links, descriptions, and material in each topic I found for my learning. The intention is not to re-write what has been written hundereds of times by very skilled authors, but to summarize methods, rank best pages/books which describe them, and refer to programming codes I found the best for that topic, if relevant. For each topic I provide my oppinions on what it is and when/where/why/and how to use it. To ensure the audience of this collection covers a larger range, I keep the descriptions simple and to the point, with minimum mathematical equations, if possible. "],
["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Machine learning refers to a set of algorithms which make a machine (with a computer as its brain) to apparently learn. Learning is not Not fully observed, hence, there need to be assumptions as the solutions is not unique "],
["classification.html", "Chapter 2 Classification 2.1 Logistic regression 2.2 Support vector machines 2.3 Decision tree 2.4 K-nearest neighbor", " Chapter 2 Classification Classification refers to learning by examples to categorize instances. For example, you are given a data-set of people who got a heart attach or no heart attack after their 50. Each instance has been characterized by attributes such as smoking habits, history of heart attack in the family, number of hours of exercise per day, hight, weight, among others. The task is to estimate, for a given characteristics of a new person, if the person is going to have a heart attack after their 50. Another example of classification is that given some attributes of a tumor (e.g., shape, color, genetic information, history of the patient, among others), estimate if the tumor type is benign, type 1, type 2, or type 3. Letâ€™s assume we have two attributes per instance and two groups of instances (blue and red). The aim of classification (binary in this case) is to find a line (separatory line) which separates the two classes (see figure). Fig 1: Two attributes (horizontal and vertical axes), two classes (red and blue), and As usually the given instances do not represent all possible instances for a given problem, therefore, any line, in any shape and form, that separates the given instances is acceptable (purple, green, and orange in the figure). This leads to different assumptions upon which different classifiers are formulated. For example, support vector machines assume that the line which has the maximum distance from each class is preferred and more empirically robust against potential uncertainties in unseen instances. In a more general framework, given a set of instances (\\(X\\), which has \\(m\\) rows and \\(n\\) columns, each row is an instance and each column is an attribute) and their associated classes (\\(Y\\), which has \\(m\\) rows, containing a categorical value in each row), we seek a mathematical rule (line and hyper-plane are special case of this) which can generate the class label, \\(y\\), for any given instance, \\(x\\) (\\(1\\) row and \\(n\\) columns). 2.1 Logistic regression 2.1.1 Variable importance in LR 2.2 Support vector machines What is support vector machine 2.2.1 Kernel tricks 2.2.2 Implementations 2.3 Decision tree 2.4 K-nearest neighbor What is it Why does it work How to implement "],
["regression.html", "Chapter 3 Regression 3.1 Linear regression 3.2 Decision tree for regression", " Chapter 3 Regression This introduces regression. 3.1 Linear regression 3.2 Decision tree for regression "],
["dimensionality-reduction.html", "Chapter 4 Dimensionality reduction 4.1 Principle component analysis 4.2 Singular value decomposition", " Chapter 4 Dimensionality reduction What is it Supervised and unsupervised 4.1 Principle component analysis 4.2 Singular value decomposition "],
["metrics-learning.html", "Chapter 5 Metrics learning 5.1 Large margin nearest neighbor", " Chapter 5 Metrics learning What is it 5.1 Large margin nearest neighbor "],
["neural-networks.html", "Chapter 6 Neural networks 6.1 Multi-layer perceptron 6.2 Mixed density networks 6.3 Convolutional neural networks 6.4 Autoencoders", " Chapter 6 Neural networks 6.1 Multi-layer perceptron 6.2 Mixed density networks 6.3 Convolutional neural networks 6.4 Autoencoders "],
["bayesian-inference.html", "Chapter 7 Bayesian inference 7.1 Random forest 7.2 Ensemble techniques", " Chapter 7 Bayesian inference Frequentists: p-value depends on the number of trials Bayesian: After each trial, the probability distribution is updated (prior is modified to get posterior) https://www.quantstart.com/articles/Bayesian-Linear-Regression-Models-with-PyMC3 https://twiecki.io/blog/2015/11/10/mcmc-sampling/ 7.1 Random forest 7.2 Ensemble techniques "],
["reinforcement-learning.html", "Chapter 8 Reinforcement learning 8.1 Q-learning", " Chapter 8 Reinforcement learning 8.1 Q-learning "],
["bagging-and-boosting.html", "Chapter 9 Bagging and boosting 9.1 Extreme boosted tree", " Chapter 9 Bagging and boosting 9.1 Extreme boosted tree "],
["introduction-1.html", "Chapter 10 Introduction", " Chapter 10 Introduction What is optimziation Examples Derivative-free vs with derivative "],
["optimization-problems.html", "Chapter 11 Optimization problems 11.1 Single and Multi objective 11.2 Constrains in problems 11.3 Dynamic optimization problems", " Chapter 11 Optimization problems 11.1 Single and Multi objective 11.2 Constrains in problems 11.3 Dynamic optimization problems "],
["use-of-derivative-in-optimization.html", "Chapter 12 Use of derivative in optimization", " Chapter 12 Use of derivative in optimization "],
["derivative-free-algorithms.html", "Chapter 13 Derivative-free algorithms 13.1 Finite difference 13.2 Population-based optimization", " Chapter 13 Derivative-free algorithms 13.1 Finite difference 13.2 Population-based optimization 13.2.1 Genetic algorithm 13.2.2 Evolutionary strategy 13.2.3 Covariance matrix adaptation 13.2.4 Particle swarm optimization "]
]
