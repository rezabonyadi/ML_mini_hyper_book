<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 Important considerations in machine learning methods | Machine learning, statistics, and optimization: A collection of intuitions</title>
  <meta name="description" content="." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 Important considerations in machine learning methods | Machine learning, statistics, and optimization: A collection of intuitions" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="." />
  <meta name="github-repo" content="rezabontadi/machine-learning-hyper-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Important considerations in machine learning methods | Machine learning, statistics, and optimization: A collection of intuitions" />
  
  <meta name="twitter:description" content="." />
  

<meta name="author" content="Reza Bonyadi, Ph.D." />


<meta name="date" content="2020-03-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec-causal-models.html">
<link rel="next" href="introduction-1.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Machine learning</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#sec:supervisedvsunsupervised"><i class="fa fa-check"></i><b>1.1</b> Supervised and unsupervised learning</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#sec:supervisedmodels"><i class="fa fa-check"></i><b>1.2</b> Supervised models</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>2</b> Classification</a><ul>
<li class="chapter" data-level="2.1" data-path="classification.html"><a href="classification.html#generative-vs.discriminative-classifiers"><i class="fa fa-check"></i><b>2.1</b> Generative vs.Â discriminative classifiers</a></li>
<li class="chapter" data-level="2.2" data-path="classification.html"><a href="classification.html#loss-function-for-classification"><i class="fa fa-check"></i><b>2.2</b> 0/1 loss function for classification</a><ul>
<li class="chapter" data-level="2.2.1" data-path="classification.html"><a href="classification.html#improvements"><i class="fa fa-check"></i><b>2.2.1</b> Improvements</a></li>
<li class="chapter" data-level="2.2.2" data-path="classification.html"><a href="classification.html#pros-and-cons"><i class="fa fa-check"></i><b>2.2.2</b> Pros and cons</a></li>
<li class="chapter" data-level="2.2.3" data-path="classification.html"><a href="classification.html#implementation"><i class="fa fa-check"></i><b>2.2.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>2.3</b> Logistic regression</a><ul>
<li class="chapter" data-level="2.3.1" data-path="classification.html"><a href="classification.html#interpretability"><i class="fa fa-check"></i><b>2.3.1</b> Interpretability</a></li>
<li class="chapter" data-level="2.3.2" data-path="classification.html"><a href="classification.html#pros-and-cons-1"><i class="fa fa-check"></i><b>2.3.2</b> Pros and cons</a></li>
<li class="chapter" data-level="2.3.3" data-path="classification.html"><a href="classification.html#implementation-1"><i class="fa fa-check"></i><b>2.3.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="classification.html"><a href="classification.html#bayes-classifier"><i class="fa fa-check"></i><b>2.4</b> Bayes classifier</a><ul>
<li class="chapter" data-level="2.4.1" data-path="classification.html"><a href="classification.html#pros-and-cons-2"><i class="fa fa-check"></i><b>2.4.1</b> Pros and cons</a></li>
<li class="chapter" data-level="2.4.2" data-path="classification.html"><a href="classification.html#implementation-2"><i class="fa fa-check"></i><b>2.4.2</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="classification.html"><a href="classification.html#support-vector-machines"><i class="fa fa-check"></i><b>2.5</b> Support vector machines</a><ul>
<li class="chapter" data-level="2.5.1" data-path="classification.html"><a href="classification.html#interpretability-1"><i class="fa fa-check"></i><b>2.5.1</b> Interpretability</a></li>
<li class="chapter" data-level="2.5.2" data-path="classification.html"><a href="classification.html#improvements-1"><i class="fa fa-check"></i><b>2.5.2</b> Improvements</a></li>
<li class="chapter" data-level="2.5.3" data-path="classification.html"><a href="classification.html#implementation-3"><i class="fa fa-check"></i><b>2.5.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="classification.html"><a href="classification.html#sec:descisiontree"><i class="fa fa-check"></i><b>2.6</b> Decision tree</a></li>
<li class="chapter" data-level="2.7" data-path="classification.html"><a href="classification.html#sec:KNN"><i class="fa fa-check"></i><b>2.7</b> K-nearest neighbor</a><ul>
<li class="chapter" data-level="2.7.1" data-path="classification.html"><a href="classification.html#improvements-2"><i class="fa fa-check"></i><b>2.7.1</b> Improvements</a></li>
<li class="chapter" data-level="2.7.2" data-path="classification.html"><a href="classification.html#pros-and-cons-3"><i class="fa fa-check"></i><b>2.7.2</b> Pros and cons</a></li>
<li class="chapter" data-level="2.7.3" data-path="classification.html"><a href="classification.html#more-details"><i class="fa fa-check"></i><b>2.7.3</b> More details</a></li>
<li class="chapter" data-level="2.7.4" data-path="classification.html"><a href="classification.html#implementation-4"><i class="fa fa-check"></i><b>2.7.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="classification.html"><a href="classification.html#gaussianprocessclassifier"><i class="fa fa-check"></i><b>2.8</b> Gaussian process classifier</a></li>
<li class="chapter" data-level="2.9" data-path="classification.html"><a href="classification.html#general-additive-model"><i class="fa fa-check"></i><b>2.9</b> General additive model</a></li>
<li class="chapter" data-level="2.10" data-path="classification.html"><a href="classification.html#turning-binary-classifiers-to-multi-class"><i class="fa fa-check"></i><b>2.10</b> Turning binary classifiers to multi-class</a></li>
<li class="chapter" data-level="2.11" data-path="classification.html"><a href="classification.html#performance-measures-and-evaluation"><i class="fa fa-check"></i><b>2.11</b> Performance measures and evaluation</a><ul>
<li class="chapter" data-level="2.11.1" data-path="classification.html"><a href="classification.html#signal-detection"><i class="fa fa-check"></i><b>2.11.1</b> Signal detection</a></li>
<li class="chapter" data-level="2.11.2" data-path="classification.html"><a href="classification.html#receiver-operating-characteristic-roc-and-area-under-the-curve-auc"><i class="fa fa-check"></i><b>2.11.2</b> Receiver operating characteristic (ROC) and Area under the curve (AUC)</a></li>
<li class="chapter" data-level="2.11.3" data-path="classification.html"><a href="classification.html#confusion-matrix"><i class="fa fa-check"></i><b>2.11.3</b> Confusion matrix</a></li>
<li class="chapter" data-level="2.11.4" data-path="classification.html"><a href="classification.html#benchmarking"><i class="fa fa-check"></i><b>2.11.4</b> Benchmarking</a></li>
<li class="chapter" data-level="2.11.5" data-path="classification.html"><a href="classification.html#stratified-sampling"><i class="fa fa-check"></i><b>2.11.5</b> Stratified sampling</a></li>
<li class="chapter" data-level="2.11.6" data-path="classification.html"><a href="classification.html#cross-validation-and-random-permutation"><i class="fa fa-check"></i><b>2.11.6</b> Cross validation and random permutation</a></li>
<li class="chapter" data-level="2.11.7" data-path="classification.html"><a href="classification.html#imbalance-data-sets"><i class="fa fa-check"></i><b>2.11.7</b> Imbalance data sets</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>3</b> Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="regression.html"><a href="regression.html#linear-regression"><i class="fa fa-check"></i><b>3.1</b> Linear regression</a></li>
<li class="chapter" data-level="3.2" data-path="regression.html"><a href="regression.html#multivariate-adaptive-regression-spline"><i class="fa fa-check"></i><b>3.2</b> Multivariate Adaptive Regression Spline</a></li>
<li class="chapter" data-level="3.3" data-path="regression.html"><a href="regression.html#generalized-additive-model"><i class="fa fa-check"></i><b>3.3</b> Generalized additive model</a></li>
<li class="chapter" data-level="3.4" data-path="regression.html"><a href="regression.html#decision-tree-for-regression"><i class="fa fa-check"></i><b>3.4</b> Decision tree for regression</a></li>
<li class="chapter" data-level="3.5" data-path="regression.html"><a href="regression.html#performance-measures"><i class="fa fa-check"></i><b>3.5</b> Performance measures</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>4</b> Dimensionality reduction</a><ul>
<li class="chapter" data-level="4.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#more-details-1"><i class="fa fa-check"></i><b>4.1</b> More details</a></li>
<li class="chapter" data-level="4.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#principal-component-analysis"><i class="fa fa-check"></i><b>4.2</b> Principal component analysis</a></li>
<li class="chapter" data-level="4.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#t-sne"><i class="fa fa-check"></i><b>4.3</b> T-SNE</a></li>
<li class="chapter" data-level="4.4" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#independent-component-analysis"><i class="fa fa-check"></i><b>4.4</b> Independent component analysis</a></li>
<li class="chapter" data-level="4.5" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#partial-least-square"><i class="fa fa-check"></i><b>4.5</b> Partial least square</a></li>
<li class="chapter" data-level="4.6" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>4.6</b> Linear discriminant analysis</a></li>
<li class="chapter" data-level="4.7" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#svm-dimensionality-reduction"><i class="fa fa-check"></i><b>4.7</b> SVM dimensionality reduction</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html"><i class="fa fa-check"></i><b>5</b> Metrics learning</a><ul>
<li class="chapter" data-level="5.1" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html#large-margin-nearest-neighbor"><i class="fa fa-check"></i><b>5.1</b> Large margin nearest neighbor</a></li>
<li class="chapter" data-level="5.2" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html#metric-learning-for-kernel-regression"><i class="fa fa-check"></i><b>5.2</b> Metric learning for kernel regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>6</b> Neural networks</a><ul>
<li class="chapter" data-level="6.1" data-path="neural-networks.html"><a href="neural-networks.html#multi-layer-perceptron"><i class="fa fa-check"></i><b>6.1</b> Multi-layer perceptron</a></li>
<li class="chapter" data-level="6.2" data-path="neural-networks.html"><a href="neural-networks.html#mixed-density-networks"><i class="fa fa-check"></i><b>6.2</b> Mixed density networks</a></li>
<li class="chapter" data-level="6.3" data-path="neural-networks.html"><a href="neural-networks.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>6.3</b> Convolutional neural networks</a></li>
<li class="chapter" data-level="6.4" data-path="neural-networks.html"><a href="neural-networks.html#autoencoders"><i class="fa fa-check"></i><b>6.4</b> Autoencoders</a></li>
<li class="chapter" data-level="6.5" data-path="neural-networks.html"><a href="neural-networks.html#generative-adversial-neural-network"><i class="fa fa-check"></i><b>6.5</b> Generative adversial neural network</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>7</b> Bayesian inference</a></li>
<li class="chapter" data-level="8" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html"><i class="fa fa-check"></i><b>8</b> Ensemble techniques</a><ul>
<li class="chapter" data-level="8.1" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html#random-forest"><i class="fa fa-check"></i><b>8.1</b> Random forest</a></li>
<li class="chapter" data-level="8.2" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html#gradient-boosting"><i class="fa fa-check"></i><b>8.2</b> Gradient boosting</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html"><i class="fa fa-check"></i><b>9</b> Reinforcement learning</a><ul>
<li class="chapter" data-level="9.1" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#q-learning"><i class="fa fa-check"></i><b>9.1</b> Q-learning</a></li>
<li class="chapter" data-level="9.2" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#curiosity"><i class="fa fa-check"></i><b>9.2</b> Curiosity</a><ul>
<li class="chapter" data-level="9.2.1" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#implementation-idea"><i class="fa fa-check"></i><b>9.2.1</b> Implementation idea</a></li>
<li class="chapter" data-level="9.2.2" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#what-does-it-solve"><i class="fa fa-check"></i><b>9.2.2</b> What does it solve?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html"><i class="fa fa-check"></i><b>10</b> Bagging and boosting</a><ul>
<li class="chapter" data-level="10.1" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#extreme-boosted-tree"><i class="fa fa-check"></i><b>10.1</b> Extreme boosted tree</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="anomaly-detection.html"><a href="anomaly-detection.html"><i class="fa fa-check"></i><b>11</b> Anomaly detection</a><ul>
<li class="chapter" data-level="11.1" data-path="anomaly-detection.html"><a href="anomaly-detection.html#autoencoder"><i class="fa fa-check"></i><b>11.1</b> Autoencoder</a></li>
<li class="chapter" data-level="11.2" data-path="anomaly-detection.html"><a href="anomaly-detection.html#one-class-svm"><i class="fa fa-check"></i><b>11.2</b> One class SVM</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="sec-preprocessing.html"><a href="sec-preprocessing.html"><i class="fa fa-check"></i><b>12</b> Preprocessing</a><ul>
<li class="chapter" data-level="12.1" data-path="sec-preprocessing.html"><a href="sec-preprocessing.html#normalization-and-standardization"><i class="fa fa-check"></i><b>12.1</b> Normalization and standardization</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="sec-signalProcessing.html"><a href="sec-signalProcessing.html"><i class="fa fa-check"></i><b>13</b> Signal processing</a></li>
<li class="chapter" data-level="14" data-path="sec-causal-models.html"><a href="sec-causal-models.html"><i class="fa fa-check"></i><b>14</b> Causal models</a><ul>
<li class="chapter" data-level="14.1" data-path="sec-causal-models.html"><a href="sec-causal-models.html#graphical-models"><i class="fa fa-check"></i><b>14.1</b> Graphical models</a></li>
<li class="chapter" data-level="14.2" data-path="sec-causal-models.html"><a href="sec-causal-models.html#bayesian-networks"><i class="fa fa-check"></i><b>14.2</b> Bayesian networks</a><ul>
<li class="chapter" data-level="14.2.1" data-path="sec-causal-models.html"><a href="sec-causal-models.html#improvements-3"><i class="fa fa-check"></i><b>14.2.1</b> Improvements</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="sec-causal-models.html"><a href="sec-causal-models.html#markov-random-field"><i class="fa fa-check"></i><b>14.3</b> Markov random field</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="important-considerations-in-machine-learning-methods.html"><a href="important-considerations-in-machine-learning-methods.html"><i class="fa fa-check"></i><b>15</b> Important considerations in machine learning methods</a><ul>
<li class="chapter" data-level="15.1" data-path="important-considerations-in-machine-learning-methods.html"><a href="important-considerations-in-machine-learning-methods.html#sec:biasVariance"><i class="fa fa-check"></i><b>15.1</b> The bias-variance debate</a></li>
<li class="chapter" data-level="15.2" data-path="important-considerations-in-machine-learning-methods.html"><a href="important-considerations-in-machine-learning-methods.html#sec:regularization"><i class="fa fa-check"></i><b>15.2</b> Regularization</a><ul>
<li class="chapter" data-level="15.2.1" data-path="important-considerations-in-machine-learning-methods.html"><a href="important-considerations-in-machine-learning-methods.html#famous-types"><i class="fa fa-check"></i><b>15.2.1</b> Famous types</a></li>
<li class="chapter" data-level="15.2.2" data-path="important-considerations-in-machine-learning-methods.html"><a href="important-considerations-in-machine-learning-methods.html#more-details-2"><i class="fa fa-check"></i><b>15.2.2</b> More details</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="important-considerations-in-machine-learning-methods.html"><a href="important-considerations-in-machine-learning-methods.html#raw-data-vs-characterized-data-features"><i class="fa fa-check"></i><b>15.3</b> Raw data vs characterized data (features)</a></li>
<li class="chapter" data-level="15.4" data-path="important-considerations-in-machine-learning-methods.html"><a href="important-considerations-in-machine-learning-methods.html#variable-importance-and-insights-considerations"><i class="fa fa-check"></i><b>15.4</b> Variable importance and insights considerations</a></li>
</ul></li>
<li class="part"><span><b>II Optimziation</b></span></li>
<li class="chapter" data-level="16" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>16</b> Introduction</a><ul>
<li class="chapter" data-level="16.1" data-path="introduction-1.html"><a href="introduction-1.html#derivative-free-vs-with-derivative-optimization-methods"><i class="fa fa-check"></i><b>16.1</b> Derivative-free vs with derivative optimization methods</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="optimization-problems.html"><a href="optimization-problems.html"><i class="fa fa-check"></i><b>17</b> Optimization problems</a><ul>
<li class="chapter" data-level="17.1" data-path="optimization-problems.html"><a href="optimization-problems.html#single-and-multi-objective"><i class="fa fa-check"></i><b>17.1</b> Single and Multi objective</a></li>
<li class="chapter" data-level="17.2" data-path="optimization-problems.html"><a href="optimization-problems.html#constrains-in-problems"><i class="fa fa-check"></i><b>17.2</b> Constrains in problems</a></li>
<li class="chapter" data-level="17.3" data-path="optimization-problems.html"><a href="optimization-problems.html#dynamic-optimization-problems"><i class="fa fa-check"></i><b>17.3</b> Dynamic optimization problems</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="use-of-derivative-in-optimization.html"><a href="use-of-derivative-in-optimization.html"><i class="fa fa-check"></i><b>18</b> Use of derivative in optimization</a></li>
<li class="chapter" data-level="19" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html"><i class="fa fa-check"></i><b>19</b> Derivative-free algorithms</a><ul>
<li class="chapter" data-level="19.1" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#finite-difference"><i class="fa fa-check"></i><b>19.1</b> Finite difference</a></li>
<li class="chapter" data-level="19.2" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#population-based-optimization"><i class="fa fa-check"></i><b>19.2</b> Population-based optimization</a><ul>
<li class="chapter" data-level="19.2.1" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#genetic-algorithm"><i class="fa fa-check"></i><b>19.2.1</b> Genetic algorithm</a></li>
<li class="chapter" data-level="19.2.2" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#evolutionary-strategy"><i class="fa fa-check"></i><b>19.2.2</b> Evolutionary strategy</a></li>
<li class="chapter" data-level="19.2.3" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#covariance-matrix-adaptation"><i class="fa fa-check"></i><b>19.2.3</b> Covariance matrix adaptation</a></li>
<li class="chapter" data-level="19.2.4" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#particle-swarm-optimization"><i class="fa fa-check"></i><b>19.2.4</b> Particle swarm optimization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="what-are-constraints.html"><a href="what-are-constraints.html"><i class="fa fa-check"></i><b>20</b> What are Constraints</a><ul>
<li class="chapter" data-level="20.1" data-path="what-are-constraints.html"><a href="what-are-constraints.html#how-to-deal-with-constraints"><i class="fa fa-check"></i><b>20.1</b> How to deal with constraints</a><ul>
<li class="chapter" data-level="20.1.1" data-path="what-are-constraints.html"><a href="what-are-constraints.html#sec:lagrangian"><i class="fa fa-check"></i><b>20.1.1</b> Lagrangian</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="famous-forms-of-optimization-problems.html"><a href="famous-forms-of-optimization-problems.html"><i class="fa fa-check"></i><b>21</b> Famous forms of optimization problems</a><ul>
<li class="chapter" data-level="21.1" data-path="famous-forms-of-optimization-problems.html"><a href="famous-forms-of-optimization-problems.html#linear-program"><i class="fa fa-check"></i><b>21.1</b> Linear program</a></li>
<li class="chapter" data-level="21.2" data-path="famous-forms-of-optimization-problems.html"><a href="famous-forms-of-optimization-problems.html#quadratic-objective-with-linear-constraints"><i class="fa fa-check"></i><b>21.2</b> Quadratic objective with linear constraints</a></li>
<li class="chapter" data-level="21.3" data-path="famous-forms-of-optimization-problems.html"><a href="famous-forms-of-optimization-problems.html#quadratic-objective-and-constraints"><i class="fa fa-check"></i><b>21.3</b> Quadratic objective and constraints</a></li>
</ul></li>
<li class="part"><span><b>III Statistics</b></span></li>
<li class="chapter" data-level="22" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>22</b> Introduction</a></li>
<li class="chapter" data-level="23" data-path="basics-statistics.html"><a href="basics-statistics.html"><i class="fa fa-check"></i><b>23</b> Basics statistics</a><ul>
<li class="chapter" data-level="23.1" data-path="basics-statistics.html"><a href="basics-statistics.html#correlation"><i class="fa fa-check"></i><b>23.1</b> Correlation</a></li>
<li class="chapter" data-level="23.2" data-path="basics-statistics.html"><a href="basics-statistics.html#moments"><i class="fa fa-check"></i><b>23.2</b> Moments</a></li>
<li class="chapter" data-level="23.3" data-path="basics-statistics.html"><a href="basics-statistics.html#covariance-matrix"><i class="fa fa-check"></i><b>23.3</b> Covariance matrix</a></li>
<li class="chapter" data-level="23.4" data-path="basics-statistics.html"><a href="basics-statistics.html#matrix-decomposition"><i class="fa fa-check"></i><b>23.4</b> Matrix decomposition</a><ul>
<li class="chapter" data-level="23.4.1" data-path="basics-statistics.html"><a href="basics-statistics.html#eigen-decomposition"><i class="fa fa-check"></i><b>23.4.1</b> Eigen decomposition</a></li>
<li class="chapter" data-level="23.4.2" data-path="basics-statistics.html"><a href="basics-statistics.html#singular-value-decomposition"><i class="fa fa-check"></i><b>23.4.2</b> Singular value decomposition</a></li>
</ul></li>
<li class="chapter" data-level="23.5" data-path="basics-statistics.html"><a href="basics-statistics.html#distributions"><i class="fa fa-check"></i><b>23.5</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="statistical-analysis.html"><a href="statistical-analysis.html"><i class="fa fa-check"></i><b>24</b> Statistical analysis</a><ul>
<li class="chapter" data-level="24.1" data-path="statistical-analysis.html"><a href="statistical-analysis.html#statistical-tests"><i class="fa fa-check"></i><b>24.1</b> Statistical tests</a></li>
<li class="chapter" data-level="24.2" data-path="statistical-analysis.html"><a href="statistical-analysis.html#causality"><i class="fa fa-check"></i><b>24.2</b> Causality</a></li>
<li class="chapter" data-level="24.3" data-path="statistical-analysis.html"><a href="statistical-analysis.html#anova"><i class="fa fa-check"></i><b>24.3</b> Anova</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="terms-and-notations.html"><a href="terms-and-notations.html"><i class="fa fa-check"></i><b>25</b> Terms and notations</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine learning, statistics, and optimization: A collection of intuitions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="important-considerations-in-machine-learning-methods" class="section level1">
<h1><span class="header-section-number">Chapter 15</span> Important considerations in machine learning methods</h1>
<div id="sec:biasVariance" class="section level2">
<h2><span class="header-section-number">15.1</span> The bias-variance debate</h2>
<p>Assume there is a set of given instances, <span class="math inline">\(X\)</span> (m rows and n columns), and we are asked to optimize a model <span class="math inline">\(M(X, \theta)\)</span> to estimate <span class="math inline">\(Y\)</span> (m rows) given some examples, i.e., supervised learning. Also, it is assumed that the underlying pattern of the data is not known (note that, this is a very important assumption as if this is not true then any model except the one which formulates the pattern is irrelevant). Another assumption is that the collected data may have some noise in it (another important assumption). If <span class="math inline">\(M\)</span> is a flexible formula (which usually means it involves lots of parameters to complex formulations) then it can fit to the given data-set very well. This, however, means that the model would formulate also the noise in the data, which would impose lots of fluctuations and non-linearities that are not really related to the pattern in the data but the noise in the observations. This leads to a model that has a low bias. However, as it is very flexible and formulates lots of fluctuations coming from random events (noise), it also offer a high variance. This high variance and low bias would lead to worsening the generalization ability of the model as it is not really formulating the pattern in the data but also the noise and unnecessary fluctuations.</p>
<p>Letâs go a bit more detail. Let us call each instance <span class="math inline">\(x\)</span> and its corresponding desired output as <span class="math inline">\(y\)</span>. In reality, <span class="math inline">\(y\)</span> is the outcome of a system working with inputs <span class="math inline">\(x\)</span>, and some noise, <span class="math inline">\(y=f(x)+\epsilon\)</span>, where <span class="math inline">\(f(.)\)</span> is not known and <span class="math inline">\(\epsilon\)</span> is noise which is also unknown. For example, if <span class="math inline">\(x\)</span> represents characteristics of people (smoking habit, weight, genetics, etc.) and <span class="math inline">\(y\)</span> is whether or not they would have a heart-attack after their 50s, then <span class="math inline">\(f(x)\)</span> is how body would respond to those characteristics and leads to a heart-attack or not, which is not really known. With modelling, we estimate this function <span class="math inline">\(f(x)\)</span>, given some examples and some assumptions on the shape of <span class="math inline">\(f\)</span>.</p>
<p>A model <span class="math inline">\(M\)</span> is responsible to estimate observed <span class="math inline">\(Y\)</span> as <span class="math inline">\(Y=f(X)+\epsilon=M(X, \hat{\theta})+e\)</span>, where <span class="math inline">\(e\)</span> has <span class="math inline">\(m\)</span> rows and indicates the error from <span class="math inline">\(Y\)</span>. If <span class="math inline">\(e\)</span> is small then <span class="math inline">\(M(X, \hat{\theta})\)</span> is an accurate estimation of <span class="math inline">\(Y\)</span>, hence the model estimates not only <span class="math inline">\(f\)</span> but large amount of the noise <span class="math inline">\(\epsilon\)</span> in it. This takes place if the model <span class="math inline">\(M\)</span> is a complex, flexible, equation which is able to fit any complex behavior of <span class="math inline">\(Y\)</span> as function of <span class="math inline">\(X\)</span>, including all fluctuations resulting from the noise. Hence, the outputs of the model will be similar to the values of <span class="math inline">\(Y\)</span> (non-biased), which includes noisy fluctuations, which leads to a high variance (fluctuations usually lead to large variance). If the model is not complex though, the gap between the output of the model and <span class="math inline">\(Y\)</span> might be large (bias), however, it would fit less to the noisy fluctuations in the <span class="math inline">\(Y\)</span> which leads to a smaller variance. That is why it is said a complex model has a large variance and small bias, and vice versa. Formal calculation of this trade off can be found in the <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Derivation">bias-variance tradeoff in wikipedia</a>.</p>
</div>
<div id="sec:regularization" class="section level2">
<h2><span class="header-section-number">15.2</span> Regularization</h2>
<p><strong><em>Category Deep</em></strong></p>
<p>Letâs assume that we are fitting a model <span class="math inline">\(M(X, \theta)\)</span> to the instances <span class="math inline">\(X\)</span>, given labels <span class="math inline">\(Y\)</span>, using an optimization algorithm and an evaluation metric, <span class="math inline">\(E(M(X, \theta), Y)\)</span>, which provides a scaler describing how dissimilar is <span class="math inline">\(M(X, \theta)\)</span> to the corresponding <span class="math inline">\(Y\)</span>.</p>
<span class="math display" id="eq:optimizationOrginal">\[\begin{equation}
min_{\theta}  ~E(M(X, \theta), Y) \tag{15.1}
\end{equation}\]</span>
<p>Depending on the definition of <span class="math inline">\(E\)</span> and <span class="math inline">\(M\)</span>, this problem might not have a unique solution. Hence, the optimization algorithm should be informed which solution is more acceptable so that it converges to what it supposed to. It is also possible that the optimizer perceives an âillusionâ of better solutions, see <span class="citation">(Bonyadi and Reutens <a href="#ref-bonyadi2019optimal">2019</a>)</span> for details.</p>
<p>Here comes regularization. It provides another constraint, formulated into the objective function for convenience (see section <a href="what-are-constraints.html#sec:lagrangian">20.1.1</a> to see how is this possible), to make the solution unique and regulate the illusions, as follows:</p>
<span class="math display" id="eq:optimizationregularized">\[\begin{equation}
min_{\theta}  ~E(M(X, \theta), Y) + \alpha R(\theta) \tag{15.2}
\end{equation}\]</span>
<p>where <span class="math inline">\(R(\theta)\)</span> is the regularization term and <span class="math inline">\(\alpha\)</span> is the regularization weight.</p>
<div id="famous-types" class="section level3">
<h3><span class="header-section-number">15.2.1</span> Famous types</h3>
<p><strong><em>Category Deep</em></strong></p>
<p>As the idea is a âsimplerâ model has a better chance to generalize better (see Section <a href="important-considerations-in-machine-learning-methods.html#sec:biasVariance">15.1</a>), the regularization term is defined in a way that it simplifies the model. Two of most frequently used regularization terms are called <span class="math inline">\(L_1\)</span> (aka LASSO) and <span class="math inline">\(L_2\)</span> (special case of Tikhonov), defined by <span class="math inline">\(||\theta||_1=\sum_i |\theta_i|\)</span> and <span class="math inline">\(||\theta||_2=\sum_i \theta_i^2\)</span>, respectively.</p>
</div>
<div id="more-details-2" class="section level3">
<h3><span class="header-section-number">15.2.2</span> More details</h3>
<p><strong><em>Category Deep</em></strong></p>
<p>The regularization introduced in Eq. <a href="important-considerations-in-machine-learning-methods.html#eq:optimizationregularized">(15.2)</a> can be defined by a constrained optimization problem as follows:</p>
<span class="math display">\[\begin{equation}
min_{\theta}  ~E(M(X, \theta), Y)+\alpha \epsilon s.t. R(\theta) = \epsilon
\end{equation}\]</span>
<p>The Lagrangian of this problem is equivalent to the original definition of regularization in Eq. <a href="important-considerations-in-machine-learning-methods.html#eq:optimizationregularized">(15.2)</a>. If <span class="math inline">\(R(\theta)\)</span> is the <span class="math inline">\(L_1\)</span>, this constraint limits the values of the parameters <span class="math inline">\(\theta\)</span> within a hyper cube, parameterized by <span class="math inline">\(\epsilon\)</span>. If <span class="math inline">\(L_2\)</span> is used, however, the parameter values are limited to a hyper-sphere, parameterized by <span class="math inline">\(\epsilon\)</span>.</p>
<p>See <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">wekipedia</a> for more information on this.</p>
<p>Another useful regularization term is the <span class="math inline">\(L_0\)</span> which counts the number of non-zero</p>
</div>
</div>
<div id="raw-data-vs-characterized-data-features" class="section level2">
<h2><span class="header-section-number">15.3</span> Raw data vs characterized data (features)</h2>
<p>The data in its original form, raw form, represents what is measured. The measurements, however, are not always useful to learn from. For example, in a stock market signal, the actual close daily prices are not as important as the trend of those values. Hence, depending on the problem, some characteristics of the data might be more informative than the raw data itself. These characteristics are usually called features. Features basically represent important (relevant to a given problem which uses that data for solution) attributes of the problem. They are either designed by an expert (i.e., feature engineering) or optimized by an algorithm for a given model (i.e., feature formation, popular in deep learning). For example, assume that we want to characterize fruits. Some obvious features of fruits are their shapes (e.g., being round, or egg-shaped, or long), their tastes, and their smell. Features characterize attributes that distinguish different objects</p>
<p>For many problems, the features are already known. For example, we already know that a feed in a newspaper is considered interesting for a particular reader if the reader spends more time on it (the spent time is a feature). However, recognizing someoneâs face from an image is not easy to be defined by apparent features. This encourages methods like Deep Learning to not only learn the task but also find the best presentation for the problem, just from the raw data.</p>
</div>
<div id="variable-importance-and-insights-considerations" class="section level2">
<h2><span class="header-section-number">15.4</span> Variable importance and insights considerations</h2>
<p>Bootstrap aggregation Importance level</p>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references">
<div id="ref-bonyadi2019optimal">
<p>Bonyadi, Mohammad Reza, and David C Reutens. 2019. âOptimal-Margin Evolutionary Classifier.â <em>IEEE Transactions on Evolutionary Computation</em>. IEEE. <a href="https://arxiv.org/abs/1804.09891" class="uri">https://arxiv.org/abs/1804.09891</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec-causal-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="introduction-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
