<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Neural networks | Machine learning, statistics, and optimization: A collection of intuitions</title>
  <meta name="description" content="." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Neural networks | Machine learning, statistics, and optimization: A collection of intuitions" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="." />
  <meta name="github-repo" content="rezabontadi/machine-learning-hyper-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Neural networks | Machine learning, statistics, and optimization: A collection of intuitions" />
  
  <meta name="twitter:description" content="." />
  

<meta name="author" content="Reza Bonyadi, Ph.D." />


<meta name="date" content="2019-08-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec-metricslearning.html">
<link rel="next" href="bayesian-inference.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Machine learning</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#raw-data-vs-characterized-data-features"><i class="fa fa-check"></i><b>1.1</b> Raw data vs characterized data (features)</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#sec:supervisedvsunsupervised"><i class="fa fa-check"></i><b>1.2</b> Supervised and unsupervised learning</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#sec:supervisedmodels"><i class="fa fa-check"></i><b>1.3</b> Supervised models</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#sec:biasVariance"><i class="fa fa-check"></i><b>1.4</b> The bias-variance debate</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>2</b> Classification</a><ul>
<li class="chapter" data-level="2.1" data-path="classification.html"><a href="classification.html#generative-vs.discriminative-classifiers"><i class="fa fa-check"></i><b>2.1</b> Generative vs.Â discriminative classifiers</a></li>
<li class="chapter" data-level="2.2" data-path="classification.html"><a href="classification.html#loss-function-for-classification"><i class="fa fa-check"></i><b>2.2</b> 0/1 loss function for classification</a><ul>
<li class="chapter" data-level="2.2.1" data-path="classification.html"><a href="classification.html#improvements"><i class="fa fa-check"></i><b>2.2.1</b> Improvements</a></li>
<li class="chapter" data-level="2.2.2" data-path="classification.html"><a href="classification.html#sec:01lossMath"><i class="fa fa-check"></i><b>2.2.2</b> More details</a></li>
<li class="chapter" data-level="2.2.3" data-path="classification.html"><a href="classification.html#pros-and-cons"><i class="fa fa-check"></i><b>2.2.3</b> Pros and cons</a></li>
<li class="chapter" data-level="2.2.4" data-path="classification.html"><a href="classification.html#implementation"><i class="fa fa-check"></i><b>2.2.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>2.3</b> Logistic regression</a><ul>
<li class="chapter" data-level="2.3.1" data-path="classification.html"><a href="classification.html#variable-importance"><i class="fa fa-check"></i><b>2.3.1</b> Variable importance</a></li>
<li class="chapter" data-level="2.3.2" data-path="classification.html"><a href="classification.html#pros-and-cons-1"><i class="fa fa-check"></i><b>2.3.2</b> Pros and cons</a></li>
<li class="chapter" data-level="2.3.3" data-path="classification.html"><a href="classification.html#more-details"><i class="fa fa-check"></i><b>2.3.3</b> More details</a></li>
<li class="chapter" data-level="2.3.4" data-path="classification.html"><a href="classification.html#implementation-1"><i class="fa fa-check"></i><b>2.3.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="classification.html"><a href="classification.html#bayes-classifier"><i class="fa fa-check"></i><b>2.4</b> Bayes classifier</a><ul>
<li class="chapter" data-level="2.4.1" data-path="classification.html"><a href="classification.html#more-details-1"><i class="fa fa-check"></i><b>2.4.1</b> More details</a></li>
<li class="chapter" data-level="2.4.2" data-path="classification.html"><a href="classification.html#continuous-variables"><i class="fa fa-check"></i><b>2.4.2</b> Continuous variables</a></li>
<li class="chapter" data-level="2.4.3" data-path="classification.html"><a href="classification.html#pros-and-cons-2"><i class="fa fa-check"></i><b>2.4.3</b> Pros and cons</a></li>
<li class="chapter" data-level="2.4.4" data-path="classification.html"><a href="classification.html#implementation-2"><i class="fa fa-check"></i><b>2.4.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="classification.html"><a href="classification.html#support-vector-machines"><i class="fa fa-check"></i><b>2.5</b> Support vector machines</a><ul>
<li class="chapter" data-level="2.5.1" data-path="classification.html"><a href="classification.html#kernel-trick"><i class="fa fa-check"></i><b>2.5.1</b> Kernel trick</a></li>
<li class="chapter" data-level="2.5.2" data-path="classification.html"><a href="classification.html#some-improvements"><i class="fa fa-check"></i><b>2.5.2</b> Some improvements</a></li>
<li class="chapter" data-level="2.5.3" data-path="classification.html"><a href="classification.html#more-details-2"><i class="fa fa-check"></i><b>2.5.3</b> More details</a></li>
<li class="chapter" data-level="2.5.4" data-path="classification.html"><a href="classification.html#implementation-3"><i class="fa fa-check"></i><b>2.5.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="classification.html"><a href="classification.html#sec:descisiontree"><i class="fa fa-check"></i><b>2.6</b> Decision tree</a></li>
<li class="chapter" data-level="2.7" data-path="classification.html"><a href="classification.html#sec:KNN"><i class="fa fa-check"></i><b>2.7</b> K-nearest neighbor</a><ul>
<li class="chapter" data-level="2.7.1" data-path="classification.html"><a href="classification.html#improvements-1"><i class="fa fa-check"></i><b>2.7.1</b> Improvements</a></li>
<li class="chapter" data-level="2.7.2" data-path="classification.html"><a href="classification.html#pros-and-cons-3"><i class="fa fa-check"></i><b>2.7.2</b> Pros and cons</a></li>
<li class="chapter" data-level="2.7.3" data-path="classification.html"><a href="classification.html#more-details-3"><i class="fa fa-check"></i><b>2.7.3</b> More details</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="classification.html"><a href="classification.html#gaussianprocessclassifier"><i class="fa fa-check"></i><b>2.8</b> Gaussian process classifier</a></li>
<li class="chapter" data-level="2.9" data-path="classification.html"><a href="classification.html#general-additive-model"><i class="fa fa-check"></i><b>2.9</b> General additive model</a></li>
<li class="chapter" data-level="2.10" data-path="classification.html"><a href="classification.html#regularization"><i class="fa fa-check"></i><b>2.10</b> Regularization</a><ul>
<li class="chapter" data-level="2.10.1" data-path="classification.html"><a href="classification.html#famous-types"><i class="fa fa-check"></i><b>2.10.1</b> Famous types</a></li>
<li class="chapter" data-level="2.10.2" data-path="classification.html"><a href="classification.html#more-details-4"><i class="fa fa-check"></i><b>2.10.2</b> More details</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="classification.html"><a href="classification.html#turning-binary-classifiers-to-multi-class"><i class="fa fa-check"></i><b>2.11</b> Turning binary classifiers to multi-class</a></li>
<li class="chapter" data-level="2.12" data-path="classification.html"><a href="classification.html#performance-measures-and-evaluation"><i class="fa fa-check"></i><b>2.12</b> Performance measures and evaluation</a><ul>
<li class="chapter" data-level="2.12.1" data-path="classification.html"><a href="classification.html#signal-detection"><i class="fa fa-check"></i><b>2.12.1</b> Signal detection</a></li>
<li class="chapter" data-level="2.12.2" data-path="classification.html"><a href="classification.html#receiver-operating-characteristic-roc-and-area-under-the-curve-auc"><i class="fa fa-check"></i><b>2.12.2</b> Receiver operating characteristic (ROC) and Area under the curve (AUC)</a></li>
<li class="chapter" data-level="2.12.3" data-path="classification.html"><a href="classification.html#confusion-matrix"><i class="fa fa-check"></i><b>2.12.3</b> Confusion matrix</a></li>
<li class="chapter" data-level="2.12.4" data-path="classification.html"><a href="classification.html#benchmarking"><i class="fa fa-check"></i><b>2.12.4</b> Benchmarking</a></li>
<li class="chapter" data-level="2.12.5" data-path="classification.html"><a href="classification.html#stratified-sampling"><i class="fa fa-check"></i><b>2.12.5</b> Stratified sampling</a></li>
<li class="chapter" data-level="2.12.6" data-path="classification.html"><a href="classification.html#cross-validation-and-random-permutation"><i class="fa fa-check"></i><b>2.12.6</b> Cross validation and random permutation</a></li>
<li class="chapter" data-level="2.12.7" data-path="classification.html"><a href="classification.html#imbalance-data-sets"><i class="fa fa-check"></i><b>2.12.7</b> Imbalance data sets</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>3</b> Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="regression.html"><a href="regression.html#linear-regression"><i class="fa fa-check"></i><b>3.1</b> Linear regression</a></li>
<li class="chapter" data-level="3.2" data-path="regression.html"><a href="regression.html#decision-tree-for-regression"><i class="fa fa-check"></i><b>3.2</b> Decision tree for regression</a></li>
<li class="chapter" data-level="3.3" data-path="regression.html"><a href="regression.html#performance-measures"><i class="fa fa-check"></i><b>3.3</b> Performance measures</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>4</b> Dimensionality reduction</a><ul>
<li class="chapter" data-level="4.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#matrix-decomposition"><i class="fa fa-check"></i><b>4.1</b> Matrix decomposition</a><ul>
<li class="chapter" data-level="4.1.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#eigen-decomposition"><i class="fa fa-check"></i><b>4.1.1</b> Eigen decomposition</a></li>
<li class="chapter" data-level="4.1.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#singular-value-decomposition"><i class="fa fa-check"></i><b>4.1.2</b> Singular value decomposition</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#principle-component-analysis"><i class="fa fa-check"></i><b>4.2</b> Principle component analysis</a></li>
<li class="chapter" data-level="4.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#t-sne"><i class="fa fa-check"></i><b>4.3</b> T-SNE</a></li>
<li class="chapter" data-level="4.4" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#independent-component-analysis"><i class="fa fa-check"></i><b>4.4</b> Independent component analysis</a></li>
<li class="chapter" data-level="4.5" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#partial-least-square"><i class="fa fa-check"></i><b>4.5</b> Partial least square</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html"><i class="fa fa-check"></i><b>5</b> Metrics learning</a><ul>
<li class="chapter" data-level="5.1" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html#large-margin-nearest-neighbor"><i class="fa fa-check"></i><b>5.1</b> Large margin nearest neighbor</a></li>
<li class="chapter" data-level="5.2" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html#metric-learning-for-kernel-regression"><i class="fa fa-check"></i><b>5.2</b> Metric learning for kernel regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>6</b> Neural networks</a><ul>
<li class="chapter" data-level="6.1" data-path="neural-networks.html"><a href="neural-networks.html#multi-layer-perceptron"><i class="fa fa-check"></i><b>6.1</b> Multi-layer perceptron</a></li>
<li class="chapter" data-level="6.2" data-path="neural-networks.html"><a href="neural-networks.html#mixed-density-networks"><i class="fa fa-check"></i><b>6.2</b> Mixed density networks</a></li>
<li class="chapter" data-level="6.3" data-path="neural-networks.html"><a href="neural-networks.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>6.3</b> Convolutional neural networks</a></li>
<li class="chapter" data-level="6.4" data-path="neural-networks.html"><a href="neural-networks.html#autoencoders"><i class="fa fa-check"></i><b>6.4</b> Autoencoders</a></li>
<li class="chapter" data-level="6.5" data-path="neural-networks.html"><a href="neural-networks.html#generative-adversial-neural-network"><i class="fa fa-check"></i><b>6.5</b> Generative adversial neural network</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>7</b> Bayesian inference</a></li>
<li class="chapter" data-level="8" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html"><i class="fa fa-check"></i><b>8</b> Ensemble techniques</a><ul>
<li class="chapter" data-level="8.1" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html#random-forest"><i class="fa fa-check"></i><b>8.1</b> Random forest</a></li>
<li class="chapter" data-level="8.2" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html#gradient-boosting"><i class="fa fa-check"></i><b>8.2</b> Gradient boosting</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html"><i class="fa fa-check"></i><b>9</b> Reinforcement learning</a><ul>
<li class="chapter" data-level="9.1" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#q-learning"><i class="fa fa-check"></i><b>9.1</b> Q-learning</a></li>
<li class="chapter" data-level="9.2" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#curiosity"><i class="fa fa-check"></i><b>9.2</b> Curiosity</a><ul>
<li class="chapter" data-level="9.2.1" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#implementation-idea"><i class="fa fa-check"></i><b>9.2.1</b> Implementation idea</a></li>
<li class="chapter" data-level="9.2.2" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#what-does-it-solve"><i class="fa fa-check"></i><b>9.2.2</b> What does it solve?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html"><i class="fa fa-check"></i><b>10</b> Bagging and boosting</a><ul>
<li class="chapter" data-level="10.1" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#extreme-boosted-tree"><i class="fa fa-check"></i><b>10.1</b> Extreme boosted tree</a></li>
<li class="chapter" data-level="10.2" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#anomaly-detection"><i class="fa fa-check"></i><b>10.2</b> Anomaly detection</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="sec-preprocessing.html"><a href="sec-preprocessing.html"><i class="fa fa-check"></i><b>11</b> Preprocessing</a></li>
<li class="part"><span><b>II Optimziation</b></span></li>
<li class="chapter" data-level="12" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>12</b> Introduction</a><ul>
<li class="chapter" data-level="12.1" data-path="introduction-1.html"><a href="introduction-1.html#derivative-free-vs-with-derivative-optimization-methods"><i class="fa fa-check"></i><b>12.1</b> Derivative-free vs with derivative optimization methods</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="optimization-problems.html"><a href="optimization-problems.html"><i class="fa fa-check"></i><b>13</b> Optimization problems</a><ul>
<li class="chapter" data-level="13.1" data-path="optimization-problems.html"><a href="optimization-problems.html#single-and-multi-objective"><i class="fa fa-check"></i><b>13.1</b> Single and Multi objective</a></li>
<li class="chapter" data-level="13.2" data-path="optimization-problems.html"><a href="optimization-problems.html#constrains-in-problems"><i class="fa fa-check"></i><b>13.2</b> Constrains in problems</a></li>
<li class="chapter" data-level="13.3" data-path="optimization-problems.html"><a href="optimization-problems.html#dynamic-optimization-problems"><i class="fa fa-check"></i><b>13.3</b> Dynamic optimization problems</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="use-of-derivative-in-optimization.html"><a href="use-of-derivative-in-optimization.html"><i class="fa fa-check"></i><b>14</b> Use of derivative in optimization</a></li>
<li class="chapter" data-level="15" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html"><i class="fa fa-check"></i><b>15</b> Derivative-free algorithms</a><ul>
<li class="chapter" data-level="15.1" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#finite-difference"><i class="fa fa-check"></i><b>15.1</b> Finite difference</a></li>
<li class="chapter" data-level="15.2" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#population-based-optimization"><i class="fa fa-check"></i><b>15.2</b> Population-based optimization</a><ul>
<li class="chapter" data-level="15.2.1" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#genetic-algorithm"><i class="fa fa-check"></i><b>15.2.1</b> Genetic algorithm</a></li>
<li class="chapter" data-level="15.2.2" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#evolutionary-strategy"><i class="fa fa-check"></i><b>15.2.2</b> Evolutionary strategy</a></li>
<li class="chapter" data-level="15.2.3" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#covariance-matrix-adaptation"><i class="fa fa-check"></i><b>15.2.3</b> Covariance matrix adaptation</a></li>
<li class="chapter" data-level="15.2.4" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#particle-swarm-optimization"><i class="fa fa-check"></i><b>15.2.4</b> Particle swarm optimization</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Statistics</b></span></li>
<li class="chapter" data-level="16" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>16</b> Introduction</a></li>
<li class="chapter" data-level="17" data-path="basics-statistics.html"><a href="basics-statistics.html"><i class="fa fa-check"></i><b>17</b> Basics statistics</a><ul>
<li class="chapter" data-level="17.1" data-path="basics-statistics.html"><a href="basics-statistics.html#correlation"><i class="fa fa-check"></i><b>17.1</b> Correlation</a></li>
<li class="chapter" data-level="17.2" data-path="basics-statistics.html"><a href="basics-statistics.html#moments"><i class="fa fa-check"></i><b>17.2</b> Moments</a></li>
<li class="chapter" data-level="17.3" data-path="basics-statistics.html"><a href="basics-statistics.html#covariance-matrix"><i class="fa fa-check"></i><b>17.3</b> Covariance matrix</a></li>
<li class="chapter" data-level="17.4" data-path="basics-statistics.html"><a href="basics-statistics.html#distributions"><i class="fa fa-check"></i><b>17.4</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="statistical-analysis.html"><a href="statistical-analysis.html"><i class="fa fa-check"></i><b>18</b> Statistical analysis</a><ul>
<li class="chapter" data-level="18.1" data-path="statistical-analysis.html"><a href="statistical-analysis.html#statistical-tests"><i class="fa fa-check"></i><b>18.1</b> Statistical tests</a></li>
<li class="chapter" data-level="18.2" data-path="statistical-analysis.html"><a href="statistical-analysis.html#causality"><i class="fa fa-check"></i><b>18.2</b> Causality</a></li>
<li class="chapter" data-level="18.3" data-path="statistical-analysis.html"><a href="statistical-analysis.html#anova"><i class="fa fa-check"></i><b>18.3</b> Anova</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="terms-and-notations.html"><a href="terms-and-notations.html"><i class="fa fa-check"></i><b>19</b> Terms and notations</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine learning, statistics, and optimization: A collection of intuitions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="neural-networks" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Neural networks</h1>
<p>Great question. Classification and regression are done by optimizing a âmodelâ (a parametrized mathematical equation) which is expected to describe the data. A linear model is the simplest one while a deep network represents a highly flexible model for which the âWeightsâ are optimized by an optimization method (e.g., gradient descent) on the given data.</p>
<p>If the number of data points is equal to or larger than the number of variables then the linear model has a unique global optimum, which is the only local optimum (so, basically one optimum). Hence, simple gradient descent can solve it to optimality (i.e., find the best linear model). For a highly flexible model like deep neural networks, the less data you have, the more local optima would appear in that highly flexible equation, which means there is a larger chance that the found local optimum by the optimization algorithm is far from the global optima. Hence, the more data you have the better until the global optimum is the only local optimum. Note, however, that if your network does not have many weights then you would not need many training instances too (as a rule of thumb).</p>
<p><a href="http://playground.tensorflow.org/" class="uri">http://playground.tensorflow.org/</a></p>

<div id="multi-layer-perceptron" class="section level2">
<h2><span class="header-section-number">6.1</span> Multi-layer perceptron</h2>

</div>
<div id="mixed-density-networks" class="section level2">
<h2><span class="header-section-number">6.2</span> Mixed density networks</h2>

</div>
<div id="convolutional-neural-networks" class="section level2">
<h2><span class="header-section-number">6.3</span> Convolutional neural networks</h2>

</div>
<div id="autoencoders" class="section level2">
<h2><span class="header-section-number">6.4</span> Autoencoders</h2>
<p>PCAâs objective is to find an invertible transformation from the original space of the data to a lower-dimensional space. In its original form, PCA seeks a linear transformation for this, however, it can be kernalized to enable the search for non-linear transformations. Autoencoder generalizes this idea, seeking a transformation (not necessarily linear) from original transformation to a lower-dimensional space (encoder) and another transformation that inverses the encoder (called decoder).</p>
<p>This can be formulated in general as: let f(., ): R^p -&gt; R^n, and g(., ): R^n -&gt; R^p, p&lt; n, find and in a way that ||X-f(g(X, ), )|| is minimized. For PCA, g(X, ) is XM, where M is a n by p matrix (assuming X is m by n) and g(X, ) is psodu-inverse of M (which is p by n).</p>
<p>Autoencoders are a type of deep neural networks that map the data to itself through a process of (non-linear) dimensionality reduction, followed by dimensionality expansion. Given X as an m by n matrix (m samples and n dimensions), M_i an n_i by n_{i+1} dimensional matrix, f_i a function R^{n_{i+1}} R^{n_{i+1}}, an autoencoder maps the samples by f_p(â¦f_3(f_2(f_1(XM_1)M_2)M_3)â¦M_p) etc., where p being the size of the last matrix, and it is equal to n. Usually, some middle layers have smaller number of dimensions than n (i.e., there exists an z where <span class="math inline">\(n_z&lt;&lt;n\)</span>). The set of layers before z encode the information into n_i dimensions, while the layers after that decode the information back. The aim is to find the best values in M_is so that the, for each sample, the encoded and then decoded is the same. That means each sample is mapped to itself. Once optimized, the layer z embeds information in the n dimensions.</p>
<p>Autoencoders can be used for dimensionality reduction (detach the decoder, all n-dimensional samples are encoded into z dimensions) and anomaly detection (normal samples are mapped back to themselves with less error comparing to abnormal instances).</p>
<div class="figure"><span id="fig:autoencoder1"></span>
<img src="images/neuralnetworks/autoencoder_1.PNG" alt="Two candidate lines, green gives 4/30 and the orange gives 4/30 loss value according to the 0/1 loss function." width="70%" />
<p class="caption">
Figure 6.1: Two candidate lines, green gives 4/30 and the orange gives 4/30 loss value according to the 0/1 loss function.
</p>
</div>

</div>
<div id="generative-adversial-neural-network" class="section level2">
<h2><span class="header-section-number">6.5</span> Generative adversial neural network</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec-metricslearning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayesian-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
