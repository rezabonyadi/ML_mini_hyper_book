<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Classification | Machine Learning, optimization, and statistics: A collection of intuitions</title>
  <meta name="description" content="." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Classification | Machine Learning, optimization, and statistics: A collection of intuitions" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="." />
  <meta name="github-repo" content="rezabontadi/machine-learning-hyper-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Classification | Machine Learning, optimization, and statistics: A collection of intuitions" />
  
  <meta name="twitter:description" content="." />
  

<meta name="author" content="Reza Bonyadi, Ph.D." />


<meta name="date" content="2019-07-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html">
<link rel="next" href="regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="terms-and-notations.html"><a href="terms-and-notations.html"><i class="fa fa-check"></i><b>1</b> Terms and notations</a></li>
<li class="part"><span><b>I Machine learning</b></span></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#raw-data-vs-characterized-data-features"><i class="fa fa-check"></i><b>2.1</b> Raw data vs characterized data (features)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>3</b> Classification</a><ul>
<li class="chapter" data-level="3.1" data-path="classification.html"><a href="classification.html#loss-function-for-classification"><i class="fa fa-check"></i><b>3.1</b> 0/1 loss function for classification</a><ul>
<li class="chapter" data-level="3.1.1" data-path="classification.html"><a href="classification.html#improvements"><i class="fa fa-check"></i><b>3.1.1</b> Improvements</a></li>
<li class="chapter" data-level="3.1.2" data-path="classification.html"><a href="classification.html#sec:01lossMath"><i class="fa fa-check"></i><b>3.1.2</b> More details</a></li>
<li class="chapter" data-level="3.1.3" data-path="classification.html"><a href="classification.html#pros-and-cons"><i class="fa fa-check"></i><b>3.1.3</b> Pros and cons</a></li>
<li class="chapter" data-level="3.1.4" data-path="classification.html"><a href="classification.html#implementation"><i class="fa fa-check"></i><b>3.1.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>3.2</b> Logistic regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="classification.html"><a href="classification.html#variable-importance"><i class="fa fa-check"></i><b>3.2.1</b> Variable importance</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification.html"><a href="classification.html#variable-importance-in-lr"><i class="fa fa-check"></i><b>3.2.2</b> Variable importance in LR</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification.html"><a href="classification.html#pros-and-cons-1"><i class="fa fa-check"></i><b>3.2.3</b> Pros and cons</a></li>
<li class="chapter" data-level="3.2.4" data-path="classification.html"><a href="classification.html#more-details"><i class="fa fa-check"></i><b>3.2.4</b> More details</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="classification.html"><a href="classification.html#support-vector-machines"><i class="fa fa-check"></i><b>3.3</b> Support vector machines</a><ul>
<li class="chapter" data-level="3.3.1" data-path="classification.html"><a href="classification.html#kernel-trick"><i class="fa fa-check"></i><b>3.3.1</b> Kernel trick</a></li>
<li class="chapter" data-level="3.3.2" data-path="classification.html"><a href="classification.html#some-improvements"><i class="fa fa-check"></i><b>3.3.2</b> Some improvements</a></li>
<li class="chapter" data-level="3.3.3" data-path="classification.html"><a href="classification.html#more-details-1"><i class="fa fa-check"></i><b>3.3.3</b> More details</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="classification.html"><a href="classification.html#sec:descisiontree"><i class="fa fa-check"></i><b>3.4</b> Decision tree</a></li>
<li class="chapter" data-level="3.5" data-path="classification.html"><a href="classification.html#sec:KNN"><i class="fa fa-check"></i><b>3.5</b> K-nearest neighbor</a><ul>
<li class="chapter" data-level="3.5.1" data-path="classification.html"><a href="classification.html#improvements-1"><i class="fa fa-check"></i><b>3.5.1</b> Improvements</a></li>
<li class="chapter" data-level="3.5.2" data-path="classification.html"><a href="classification.html#pros-and-cons-2"><i class="fa fa-check"></i><b>3.5.2</b> Pros and cons</a></li>
<li class="chapter" data-level="3.5.3" data-path="classification.html"><a href="classification.html#more-details-2"><i class="fa fa-check"></i><b>3.5.3</b> More details</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="classification.html"><a href="classification.html#gaussianprocessclassifier"><i class="fa fa-check"></i><b>3.6</b> Gaussian process classifier</a></li>
<li class="chapter" data-level="3.7" data-path="classification.html"><a href="classification.html#turning-binary-classifiers-to-multi-class"><i class="fa fa-check"></i><b>3.7</b> Turning binary classifiers to multi-class</a></li>
<li class="chapter" data-level="3.8" data-path="classification.html"><a href="classification.html#performance-measures"><i class="fa fa-check"></i><b>3.8</b> Performance measures</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>4</b> Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="regression.html"><a href="regression.html#linear-regression"><i class="fa fa-check"></i><b>4.1</b> Linear regression</a></li>
<li class="chapter" data-level="4.2" data-path="regression.html"><a href="regression.html#decision-tree-for-regression"><i class="fa fa-check"></i><b>4.2</b> Decision tree for regression</a></li>
<li class="chapter" data-level="4.3" data-path="regression.html"><a href="regression.html#performance-measures-1"><i class="fa fa-check"></i><b>4.3</b> Performance measures</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>5</b> Dimensionality reduction</a><ul>
<li class="chapter" data-level="5.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#principle-component-analysis"><i class="fa fa-check"></i><b>5.1</b> Principle component analysis</a></li>
<li class="chapter" data-level="5.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#singular-value-decomposition"><i class="fa fa-check"></i><b>5.2</b> Singular value decomposition</a></li>
<li class="chapter" data-level="5.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#t-sne"><i class="fa fa-check"></i><b>5.3</b> T-SNE</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html"><i class="fa fa-check"></i><b>6</b> Metrics learning</a><ul>
<li class="chapter" data-level="6.1" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html#large-margin-nearest-neighbor"><i class="fa fa-check"></i><b>6.1</b> Large margin nearest neighbor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>7</b> Neural networks</a><ul>
<li class="chapter" data-level="7.1" data-path="neural-networks.html"><a href="neural-networks.html#multi-layer-perceptron"><i class="fa fa-check"></i><b>7.1</b> Multi-layer perceptron</a></li>
<li class="chapter" data-level="7.2" data-path="neural-networks.html"><a href="neural-networks.html#mixed-density-networks"><i class="fa fa-check"></i><b>7.2</b> Mixed density networks</a></li>
<li class="chapter" data-level="7.3" data-path="neural-networks.html"><a href="neural-networks.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>7.3</b> Convolutional neural networks</a></li>
<li class="chapter" data-level="7.4" data-path="neural-networks.html"><a href="neural-networks.html#autoencoders"><i class="fa fa-check"></i><b>7.4</b> Autoencoders</a></li>
<li class="chapter" data-level="7.5" data-path="neural-networks.html"><a href="neural-networks.html#generative-adversial-neural-network"><i class="fa fa-check"></i><b>7.5</b> Generative adversial neural network</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>8</b> Bayesian inference</a></li>
<li class="chapter" data-level="9" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html"><i class="fa fa-check"></i><b>9</b> Ensemble techniques</a><ul>
<li class="chapter" data-level="9.1" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html#random-forest"><i class="fa fa-check"></i><b>9.1</b> Random forest</a></li>
<li class="chapter" data-level="9.2" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html#gradient-boosting"><i class="fa fa-check"></i><b>9.2</b> Gradient boosting</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html"><i class="fa fa-check"></i><b>10</b> Reinforcement learning</a><ul>
<li class="chapter" data-level="10.1" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#q-learning"><i class="fa fa-check"></i><b>10.1</b> Q-learning</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html"><i class="fa fa-check"></i><b>11</b> Bagging and boosting</a><ul>
<li class="chapter" data-level="11.1" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#extreme-boosted-tree"><i class="fa fa-check"></i><b>11.1</b> Extreme boosted tree</a></li>
<li class="chapter" data-level="11.2" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#anomaly-detection"><i class="fa fa-check"></i><b>11.2</b> Anomaly detection</a></li>
</ul></li>
<li class="part"><span><b>II Optimziation</b></span></li>
<li class="chapter" data-level="12" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>12</b> Introduction</a></li>
<li class="chapter" data-level="13" data-path="optimization-problems.html"><a href="optimization-problems.html"><i class="fa fa-check"></i><b>13</b> Optimization problems</a><ul>
<li class="chapter" data-level="13.1" data-path="optimization-problems.html"><a href="optimization-problems.html#single-and-multi-objective"><i class="fa fa-check"></i><b>13.1</b> Single and Multi objective</a></li>
<li class="chapter" data-level="13.2" data-path="optimization-problems.html"><a href="optimization-problems.html#constrains-in-problems"><i class="fa fa-check"></i><b>13.2</b> Constrains in problems</a></li>
<li class="chapter" data-level="13.3" data-path="optimization-problems.html"><a href="optimization-problems.html#dynamic-optimization-problems"><i class="fa fa-check"></i><b>13.3</b> Dynamic optimization problems</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="use-of-derivative-in-optimization.html"><a href="use-of-derivative-in-optimization.html"><i class="fa fa-check"></i><b>14</b> Use of derivative in optimization</a></li>
<li class="chapter" data-level="15" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html"><i class="fa fa-check"></i><b>15</b> Derivative-free algorithms</a><ul>
<li class="chapter" data-level="15.1" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#finite-difference"><i class="fa fa-check"></i><b>15.1</b> Finite difference</a></li>
<li class="chapter" data-level="15.2" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#population-based-optimization"><i class="fa fa-check"></i><b>15.2</b> Population-based optimization</a><ul>
<li class="chapter" data-level="15.2.1" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#genetic-algorithm"><i class="fa fa-check"></i><b>15.2.1</b> Genetic algorithm</a></li>
<li class="chapter" data-level="15.2.2" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#evolutionary-strategy"><i class="fa fa-check"></i><b>15.2.2</b> Evolutionary strategy</a></li>
<li class="chapter" data-level="15.2.3" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#covariance-matrix-adaptation"><i class="fa fa-check"></i><b>15.2.3</b> Covariance matrix adaptation</a></li>
<li class="chapter" data-level="15.2.4" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#particle-swarm-optimization"><i class="fa fa-check"></i><b>15.2.4</b> Particle swarm optimization</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning, optimization, and statistics: A collection of intuitions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Classification</h1>
<p>Classification refers to learning to categorize instances by labeled examples. For example, learning if a person going to have a heart attack after their 50’s given person’s characteristics such as smoking habits, history of a heart attack in the family, number of hours of exercise per day, height, weight, among others. The task is to find a general <em>rule</em> which estimates, for any given person with given characteristics, if the person is going to have a heart attack after their 50s. Another example of classification is, given some attributes of a set of tumors (e.g., shape, color, genetic information, history of the patient, among others) and their categories (benign, type 1, type 2, or type 3), find a generic rule that estimates the tumor type for any tumor. The rule (also known as the model) is optimized by given examples of the problem to be solved.</p>
<p>In a more general framework, given a set of instances, <span class="math inline">\(X\)</span>, with <span class="math inline">\(m\)</span> rows and <span class="math inline">\(n\)</span> columns (each row is an instance and each column is an attribute) and their associated classes, <span class="math inline">\(Y\)</span>, with <span class="math inline">\(m\)</span> rows (containing a categorical value in each row), find a mathematical <em>rule</em> (line and hyperplane are special cases of this), also known as a <em>model</em>, which can generate the class label, <span class="math inline">\(y\)</span>, for any given instance, <span class="math inline">\(x\)</span> (<span class="math inline">\(1\)</span> row and <span class="math inline">\(n\)</span> columns).</p>
<p>A synthetic classification problem has been shown in Fig. <a href="classification.html#fig:classExample">3.1</a>. In this problem, there are two attributes per instance and two groups of instances (blue and red), and the discriminatory rule has been assumed to be represented by a line. The aim of classification (binary in this case) with these assumptions is to find the “best” discriminatory line (hyperplane in a higher number of dimensions) which discriminates between the two classes “optimally” (see figure). Ideally, this optimality needs to make sure that the rule not only works for the given data (training data) based on a performance measure but also for any unseen data point (test data), referred to as the <em>generalization ability</em> of the rule.</p>
<div class="figure"><span id="fig:classExample"></span>
<img src="images/classification/classification.png" alt="Two attributes (horizontal and vertical axes), two classes (red and blue), and three lines which can separate the classes successfully." width="50%" />
<p class="caption">
Figure 3.1: Two attributes (horizontal and vertical axes), two classes (red and blue), and three lines which can separate the classes successfully.
</p>
</div>
<p> </p>
<p>As usually the given instances do not represent all possible instances for a given problem, therefore, any rule, in any shape and form, that separates the given instances and optimizes the performance measure is acceptable (purple, green, and orange lines in the figure). Hence, defining “the best discriminatory rule”, a rule that not only separates the given instances but also all other unseen instances, is not possible. This leads to different assumptions upon which different classifiers are formulated. For example, support vector machines assume that the discriminatory rule is represented by a line which has the maximum distance from the instances in each class. The idea is that such a line is more empirically robust against potential uncertainties in unseen instances.</p>
<p>See <span class="citation">(Ng and Jordan <a href="#ref-ng2002discriminative">2002</a>)</span> for a formal definition of discriminative classification.</p>
<blockquote>
<p><strong>Key points</strong>:</p>
<ul>
<li>Classification aims to find a mathematical rule, aka model, that provides the category to which an instance belongs using a set of given examples.</li>
<li>As the training data-set is a subset of all possible data points, some assumptions need to be made to ensure that the “optimal” rule performs well not only on the training data-set but also on unseen instances (generalization ability).</li>
<li>These assumptions include the shapes of the mathematical rule (a line, “if-then” rules, etc.) and the <em>performance measure</em> by which the rule is evaluated.</li>
<li>Different classification methods usually differ by their assumptions and performance measures used.</li>
</ul>
</blockquote>

<div id="loss-function-for-classification" class="section level2">
<h2><span class="header-section-number">3.1</span> 0/1 loss function for classification</h2>
<p>Let’s assume that the discriminatory rule in a classification problem is represented by a line (hyperplane in more than 2 dimensions). The objective is to find a hyperplane that “optimally” discriminates between two classes. This optimality is measured by a function that assigns a “wrongness” score to each hyperplane (called the <em>loss function</em>), which needs to be minimized. Ideally, we expect to find a hyperplane for which all instances from each class fall in one side of it and the instances from the other class fall in the other side of the hyperplane. Hence, one simple function would be to count the number of instances that are not in the “correct” side of the hyperplane, which is called the <em>0/1 loss function</em>. The aim is to find a hyperplane which minimizes this function by an optimization algorithm. The outcome of this minimization is a hyperplane which has a minimum 0/1 loss value, i.e., the number of instances that are in the wrong side of the line is minimized.</p>
<div class="figure"><span id="fig:loss01"></span>
<img src="images/classification/0_1_loss_1.PNG" alt="Two candidate lines, green gives 4/30 and the orange gives 4/30 loss value according to the 0/1 loss function." width="50%" />
<p class="caption">
Figure 3.2: Two candidate lines, green gives 4/30 and the orange gives 4/30 loss value according to the 0/1 loss function.
</p>
</div>
<p> </p>
<p>One should note that there might be many hyperplanes which have the same 0/1 loss value, which means the solutions to the 0/1 loss function is not unique. In Fig. <a href="classification.html#fig:loss01">3.2</a>, for example, the 0/1 loss value is the same for the green line and the orange line.</p>
<div id="improvements" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Improvements</h3>
<p>The 0/1 loss function in its original form does not take into account the <em>importance</em> of the instances from different classes (all classes are assumed to be equally important). In the previous example (Fig. <a href="classification.html#fig:loss01">3.2</a>), the number of blue instances is smaller than the number of red instances, which means miss-classification of blue instances leads to a larger natural loss comparing to the red instances (blue instances are rare, hence more important to be classified correctly). For example, the orange line has a very high accuracy on red instances while its accuracy on the blue is not as high as the green line. To address this issue, it has been proposed in <span class="citation">(Bonyadi and Reutens <a href="#ref-bonyadi2019optimal">2019</a>)</span> to minimize the average percentage of mis-classified instances accross all classes rather than counting the number of mis-classified instances. In Fig. <a href="classification.html#fig:loss01">3.2</a>, for example, this average for the green line is <span class="math inline">\(\frac{(2/10+2/20)}{2}\)</span> (2 miss-classified instances from each class) and for the orange line is <span class="math inline">\(\frac{3/10+1/20}{2}\)</span> (3 miss-classified from blue and 1 from red). Hence, the green line would be preferred.</p>
<p>Finding a line, or a hyperplane in larger number of dimensions, that minimizes the average of the 0/1 loss function in both above-mentioned forms is not easy (see <span class="citation">(Doerr et al. <a href="#ref-doerr2015direct">2015</a>)</span> and <span class="citation">(Bonyadi and Reutens <a href="#ref-bonyadi2019optimal">2019</a>)</span>). One issue is that this line is not unique. For example, both orange and green lines in Fig. 1 have the same accuracy in terms of discriminating between the classes. This is solved by selecting the line (hyperplane) which has the maximum distance from the instances of both classes (see this <span class="citation">(Bonyadi and Reutens <a href="#ref-bonyadi2019optimal">2019</a>)</span>), called maximum margin hyperplane. The basic idea is, if there are multiple lines (hyperplanes) which separate instances from each class with the same accuracy, the one which has the maximum distance from each class is preferred. Otehr improvements in <span class="citation">(Bonyadi and Reutens <a href="#ref-bonyadi2019optimal">2019</a>)</span> included addition of <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> regularization, that enables the usage of coefficients for estimation of variable importance.</p>
</div>
<div id="sec:01lossMath" class="section level3">
<h3><span class="header-section-number">3.1.2</span> More details</h3>
<p>Any hyperplane can be represented by its normal vector (norm), <span class="math inline">\(\vec w\)</span>, and an intercept, <span class="math inline">\(b\)</span>, <span class="math inline">\(&lt;\vec w, b&gt;\)</span>. An instance <span class="math inline">\(\vec x_i\)</span> is classified by a given hyperplane as class 0 if <span class="math inline">\(\vec w \vec x_i^T+b&lt;0\)</span> (<span class="math inline">\(T\)</span> is the transpose) and class 1 otherwise. One way to define this loss function is then as follows: <span class="math display">\[
\frac{1}{N}\sum_i y_i(1-f(\vec x_i))+(1-y_i)f(\vec x_i)
\]</span> where <span class="math inline">\(N\)</span> is the number of instances. For the 0/1 loss function, <span class="math inline">\(f(.): \mathbb{R}^n \to \mathbb{R}\)</span> is defined by: <span class="math display">\[
f(\vec x)=
\begin{cases}
0 &amp; \vec w \vec x_i^T+b&lt;0\\
1 &amp; otherwise\\
\end{cases}
\]</span></p>
<p>The loss function was revised in <span class="citation">(Bonyadi and Reutens <a href="#ref-bonyadi2019optimal">2019</a>)</span> as follows: <span class="math display">\[
\frac{1}{K} \sum_{i=1}^K (\frac{1}{N_i}\sum_{j=1}^{N_i} y_j(1-f(\vec x_j))+(1-y_j)f(\vec x_j))
\]</span> This formulation is less biased in the case of imbalance number of instances in different classes. The value of k was set to 2 in that paper.</p>
<p>There are two equivalent geometrical views to imagine the separation by a hyperplane. One is that the hyperplane, defined by <span class="math inline">\(&lt;\vec w, b&gt;\)</span>, separates the two classes, which means it is placed somewhere between the instances from two classes in a way that (ideally) all instances from one class are in one side and all instances from the other class are on the other side of that hyperplane. The other view is that <span class="math inline">\(\vec w\)</span> is a linear transformation which transforms each instance <span class="math inline">\(\vec x_i\)</span> to a one dimensional line. The scalar <span class="math inline">\(b\)</span> then is the threshold to determine the class label of the transformed instances.</p>
<div class="figure"><span id="fig:geometrical"></span>
<img src="images/classification/01_gemetrical_two_views.PNG" alt="Two geometrical views: The given hyperplane equation defines a discriminatory hyperplane to separate the classes (black line in the left panel) OR the norm of the hyperplane transforms the instances to a one dimensional space (green line, and the right panel) and the intercept separates the classes." width="49%" /><img src="images/classification/01_geometrical_1.PNG" alt="Two geometrical views: The given hyperplane equation defines a discriminatory hyperplane to separate the classes (black line in the left panel) OR the norm of the hyperplane transforms the instances to a one dimensional space (green line, and the right panel) and the intercept separates the classes." width="49%" />
<p class="caption">
Figure 3.3: Two geometrical views: The given hyperplane equation defines a discriminatory hyperplane to separate the classes (black line in the left panel) OR the norm of the hyperplane transforms the instances to a one dimensional space (green line, and the right panel) and the intercept separates the classes.
</p>
</div>
<p>There are multiple ways to find a hyper-plane which minimizes the original 0/1 loss function (sum of 0/1 losses), which have been described in details in <span class="citation">(Doerr et al. <a href="#ref-doerr2015direct">2015</a>)</span>. Find also more detailes in <span class="citation">(Bonyadi and Reutens <a href="#ref-bonyadi2019optimal">2019</a>)</span>.</p>
</div>
<div id="pros-and-cons" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Pros and cons</h3>
<p><em>Pros</em>: The 0/1 loss function in both above-mentioned forms is unbiased and not sensitive to outliers as any outlier would only contribute 1 unit to the loss function if it is miss-classified (see Fig. <a href="classification.html#fig:lossesExamples">3.4</a>).</p>
<div class="figure"><span id="fig:lossesExamples"></span>
<img src="images/classification/0_1_loss_2.PNG" alt="0/1 loss function is not sensitive to outliers while other loss functions are (other loss functions are described in next sections). The figure was taken from [@doerr2015direct]" width="50%" />
<p class="caption">
Figure 3.4: 0/1 loss function is not sensitive to outliers while other loss functions are (other loss functions are described in next sections). The figure was taken from <span class="citation">(Doerr et al. <a href="#ref-doerr2015direct">2015</a>)</span>
</p>
</div>
<p> </p>
<p><em>Cons</em>: The main issue with this idea (optimal 0/1 binary classification, with or without maximum margin idea) is that optimizing the 0/1 loss function in the formed mentioned before is not practical, i.e., it is NP-Complete. Hence, all algorithms which implement this idea are rather slow. The only practically fast implementation has been described in <span class="citation">(Bonyadi and Reutens <a href="#ref-bonyadi2019optimal">2019</a>)</span>, which uses evolutionary strategy for optimization. This has encouraged introduction to lots of new loss functions which approximate the 0/1 loss function while they are differentiable.</p>
</div>
<div id="implementation" class="section level3">
<h3><span class="header-section-number">3.1.4</span> Implementation</h3>
<p>Methods described in <span class="citation">(Doerr et al. <a href="#ref-doerr2015direct">2015</a>)</span> are in an algorithmic, step by step, format which makes implementation easier. The Python, Java, and Matlab code for <span class="citation">(Bonyadi and Reutens <a href="#ref-bonyadi2019optimal">2019</a>)</span> is available <span class="citation">(“Optimal-Margin Evolutionary Classifier–source Code” <a href="#ref-bonyadi2019optimalcode">2018</a>)</span>.</p>

</div>
</div>
<div id="logistic-regression" class="section level2">
<h2><span class="header-section-number">3.2</span> Logistic regression</h2>
<p>Logistic regression seeks a hyperplane which best discriminates two classes. The hyperplane is evaluated by a loss function which is a smooth (differentiable) estimation of the function used 0/1 loss function (see Section <a href="classification.html#sec:01lossMath">3.1.2</a>), hence, can be optimized effectively by a gradient descent. For a given hyperplane, the estimation provides a value between 0 and 1 (rather than 0 or 1 as it was the case in the 0/1 loss function) for each instance that represents to what extent the instance has been classified correctly by that hyperplane. While there might be many different choices for such function, logistic regression uses the logarithm sigmoid function that looks like Fig. ????.</p>
<p>The idea is, It is clear that this function provides values between 0 and 1 for a given instance, after transforming by the hyperplane equation.</p>
<p><a href="http://ufldl.stanford.edu/tutorial/supervised/LogisticRegression/" class="uri">http://ufldl.stanford.edu/tutorial/supervised/LogisticRegression/</a></p>
<div id="variable-importance" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Variable importance</h3>
</div>
<div id="variable-importance-in-lr" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Variable importance in LR</h3>
</div>
<div id="pros-and-cons-1" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Pros and cons</h3>
</div>
<div id="more-details" class="section level3">
<h3><span class="header-section-number">3.2.4</span> More details</h3>
<ul>
<li><strong>Mathematics</strong>:</li>
<li><strong>Implementation</strong>: <a href="http://ufldl.stanford.edu/tutorial/supervised/LogisticRegression/" class="uri">http://ufldl.stanford.edu/tutorial/supervised/LogisticRegression/</a></li>
<li><strong>API</strong>: Scipy and Sikit-Learn</li>
</ul>

</div>
</div>
<div id="support-vector-machines" class="section level2">
<h2><span class="header-section-number">3.3</span> Support vector machines</h2>
<ul>
<li>What is support vector machine</li>
<li></li>
</ul>
<div id="kernel-trick" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Kernel trick</h3>
</div>
<div id="some-improvements" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Some improvements</h3>
</div>
<div id="more-details-1" class="section level3">
<h3><span class="header-section-number">3.3.3</span> More details</h3>
<ul>
<li><strong>Mathematics</strong>:</li>
<li><strong>Implementation</strong>:</li>
<li><strong>API</strong>:</li>
</ul>

</div>
</div>
<div id="sec:descisiontree" class="section level2">
<h2><span class="header-section-number">3.4</span> Decision tree</h2>

</div>
<div id="sec:KNN" class="section level2">
<h2><span class="header-section-number">3.5</span> K-nearest neighbor</h2>
<p>The k-nearest neighbor (KNN) classifier assumes that the instances “close” to one another have the same class label. Hence, to assign a class label to a new instance, KNN finds <span class="math inline">\(k\)</span> instances from the training data set to which the new instance is “closest” and use those labels to vote for the class label of the new instance (see Fig. <a href="classification.html#fig:KnnDemons">3.5</a>).</p>
<div class="figure"><span id="fig:KnnDemons"></span>
<img src="images/classification/01_knn_1.PNG" alt="Example of how KNN works. The &quot;closest&quot; instances to a new instance (green) are used to vote for its class label." width="50%" />
<p class="caption">
Figure 3.5: Example of how KNN works. The “closest” instances to a new instance (green) are used to vote for its class label.
</p>
</div>
<p>The “closeness” is defined by a distance measure, such as Euclidean distance.</p>
<div id="improvements-1" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Improvements</h3>
<p>In KNN, some attributes may lead to a biased distance. For example, if one of the attributes is in the order of 1,000 and another is in the order of 0.1, the latter would have a small impact on the calculation of the distance. This is usually resolved by standardizing the space, which ensures all attributes are in the same range. This method, however, may break some structural integrity of the instances.</p>
<p>Another issue is that some attributes might be misleading and their impact is better to be reduced. For example, Fig. <a href="classification.html#fig:KnnDimesions">3.6</a> indicates that the horizontal dimension is responsible for the green instance to be of the type “red”. This, however, may not be correct as if the horizontal dimension shrinks then the green instance becomes closer to blue instances, which makes it a blue class.</p>
<div class="figure"><span id="fig:KnnDimesions"></span>
<img src="images/classification/knn_scale.png" alt="Example of how KNN works. The &quot;closest&quot; instances to a new instance (green) are used to vote for its class label." width="50%" />
<p class="caption">
Figure 3.6: Example of how KNN works. The “closest” instances to a new instance (green) are used to vote for its class label.
</p>
</div>
<p>This indeed leads to an in-accuracy in KNN; the algorithm does not take into account the importance of attributes. This can be addressed by optimizing a metric which shrinks/contracts the space along different attributes to achieve the best transformation in which KNN performs best. See Section <a href="sec-metricslearning.html#sec:metricslearning">6</a> for details.</p>
</div>
<div id="pros-and-cons-2" class="section level3">
<h3><span class="header-section-number">3.5.2</span> Pros and cons</h3>
<p><em>Pros</em>:</p>
<ul>
<li>It is easy to implement.</li>
<li>It can model non-linearity.</li>
<li>The assumption behind KNN is not parametric and only depends on the given data.</li>
</ul>
<p><em>Cons</em>:</p>
<ul>
<li>Requires storage of the training data. This can be reduced as proposed by [???]</li>
<li>The closeness needs to be defined. Euclidean distance is an obvious choice, however, it is not always optimal [????].</li>
<li>Requires searching for the closest instances in the training set, which is slow. This can be resolved by using smart search methods [????]</li>
<li>Some of the dimensions may lead to biasing the distance</li>
<li>KNN does not provide any information about the importance of attributes. This, however, is resolved by optimizing a metric to transform the space to represent the training data optimally (see section <a href="sec-metricslearning.html#sec:metricslearning">6</a>)</li>
</ul>
</div>
<div id="more-details-2" class="section level3">
<h3><span class="header-section-number">3.5.3</span> More details</h3>

</div>
</div>
<div id="gaussianprocessclassifier" class="section level2">
<h2><span class="header-section-number">3.6</span> Gaussian process classifier</h2>
<p>TODO</p>

</div>
<div id="turning-binary-classifiers-to-multi-class" class="section level2">
<h2><span class="header-section-number">3.7</span> Turning binary classifiers to multi-class</h2>

</div>
<div id="performance-measures" class="section level2">
<h2><span class="header-section-number">3.8</span> Performance measures</h2>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-bonyadi2019optimal">
<p>Bonyadi, Mohammad Reza, and David C Reutens. 2019. “Optimal-Margin Evolutionary Classifier.” <em>IEEE Transactions on Evolutionary Computation</em>. IEEE. <a href="https://arxiv.org/abs/1804.09891" class="uri">https://arxiv.org/abs/1804.09891</a>.</p>
</div>
<div id="ref-doerr2015direct">
<p>Doerr, Andreas, Nathan D Ratliff, Jeannette Bohg, Marc Toussaint, and Stefan Schaal. 2015. “Direct Loss Minimization Inverse Optimal Control.” In <em>Robotics: Science and Systems</em>. <a href="http://proceedings.mlr.press/v28/nguyen13a.pdf" class="uri">http://proceedings.mlr.press/v28/nguyen13a.pdf</a>.</p>
</div>
<div id="ref-ng2002discriminative">
<p>Ng, Andrew Y, and Michael I Jordan. 2002. “On Discriminative Vs. Generative Classifiers: A Comparison of Logistic Regression and Naive Bayes.” In <em>Advances in Neural Information Processing Systems</em>, 841–48.</p>
</div>
<div id="ref-bonyadi2019optimalcode">
<p>“Optimal-Margin Evolutionary Classifier–source Code.” 2018. Accessed June 30. <a href="https://github.com/rezabonyadi/LinearOEC" class="uri">https://github.com/rezabonyadi/LinearOEC</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
