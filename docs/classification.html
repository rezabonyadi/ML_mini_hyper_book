<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Classification | Machine learning, statistics, and optimization: A collection of intuitions</title>
  <meta name="description" content="." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Classification | Machine learning, statistics, and optimization: A collection of intuitions" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="." />
  <meta name="github-repo" content="rezabontadi/machine-learning-hyper-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Classification | Machine learning, statistics, and optimization: A collection of intuitions" />
  
  <meta name="twitter:description" content="." />
  

<meta name="author" content="Reza Bonyadi, Ph.D." />


<meta name="date" content="2020-03-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html">
<link rel="next" href="regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Machine learning</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#sec:supervisedvsunsupervised"><i class="fa fa-check"></i><b>1.1</b> Supervised and unsupervised learning</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#sec:supervisedmodels"><i class="fa fa-check"></i><b>1.2</b> Supervised models</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>2</b> Classification</a><ul>
<li class="chapter" data-level="2.1" data-path="classification.html"><a href="classification.html#generative-vs.discriminative-classifiers"><i class="fa fa-check"></i><b>2.1</b> Generative vs. discriminative classifiers</a></li>
<li class="chapter" data-level="2.2" data-path="classification.html"><a href="classification.html#loss-function-for-classification"><i class="fa fa-check"></i><b>2.2</b> 0/1 loss function for classification</a><ul>
<li class="chapter" data-level="2.2.1" data-path="classification.html"><a href="classification.html#improvements"><i class="fa fa-check"></i><b>2.2.1</b> Improvements</a></li>
<li class="chapter" data-level="2.2.2" data-path="classification.html"><a href="classification.html#pros-and-cons"><i class="fa fa-check"></i><b>2.2.2</b> Pros and cons</a></li>
<li class="chapter" data-level="2.2.3" data-path="classification.html"><a href="classification.html#implementation"><i class="fa fa-check"></i><b>2.2.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>2.3</b> Logistic regression</a><ul>
<li class="chapter" data-level="2.3.1" data-path="classification.html"><a href="classification.html#interpretability"><i class="fa fa-check"></i><b>2.3.1</b> Interpretability</a></li>
<li class="chapter" data-level="2.3.2" data-path="classification.html"><a href="classification.html#pros-and-cons-1"><i class="fa fa-check"></i><b>2.3.2</b> Pros and cons</a></li>
<li class="chapter" data-level="2.3.3" data-path="classification.html"><a href="classification.html#implementation-1"><i class="fa fa-check"></i><b>2.3.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="classification.html"><a href="classification.html#bayes-classifier"><i class="fa fa-check"></i><b>2.4</b> Bayes classifier</a><ul>
<li class="chapter" data-level="2.4.1" data-path="classification.html"><a href="classification.html#pros-and-cons-2"><i class="fa fa-check"></i><b>2.4.1</b> Pros and cons</a></li>
<li class="chapter" data-level="2.4.2" data-path="classification.html"><a href="classification.html#implementation-2"><i class="fa fa-check"></i><b>2.4.2</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="classification.html"><a href="classification.html#support-vector-machines"><i class="fa fa-check"></i><b>2.5</b> Support vector machines</a><ul>
<li class="chapter" data-level="2.5.1" data-path="classification.html"><a href="classification.html#interpretability-1"><i class="fa fa-check"></i><b>2.5.1</b> Interpretability</a></li>
<li class="chapter" data-level="2.5.2" data-path="classification.html"><a href="classification.html#improvements-1"><i class="fa fa-check"></i><b>2.5.2</b> Improvements</a></li>
<li class="chapter" data-level="2.5.3" data-path="classification.html"><a href="classification.html#implementation-3"><i class="fa fa-check"></i><b>2.5.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="classification.html"><a href="classification.html#sec:descisiontree"><i class="fa fa-check"></i><b>2.6</b> Decision tree</a></li>
<li class="chapter" data-level="2.7" data-path="classification.html"><a href="classification.html#sec:KNN"><i class="fa fa-check"></i><b>2.7</b> K-nearest neighbor</a><ul>
<li class="chapter" data-level="2.7.1" data-path="classification.html"><a href="classification.html#improvements-2"><i class="fa fa-check"></i><b>2.7.1</b> Improvements</a></li>
<li class="chapter" data-level="2.7.2" data-path="classification.html"><a href="classification.html#pros-and-cons-3"><i class="fa fa-check"></i><b>2.7.2</b> Pros and cons</a></li>
<li class="chapter" data-level="2.7.3" data-path="classification.html"><a href="classification.html#more-details"><i class="fa fa-check"></i><b>2.7.3</b> More details</a></li>
<li class="chapter" data-level="2.7.4" data-path="classification.html"><a href="classification.html#implementation-4"><i class="fa fa-check"></i><b>2.7.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="classification.html"><a href="classification.html#gaussianprocessclassifier"><i class="fa fa-check"></i><b>2.8</b> Gaussian process classifier</a></li>
<li class="chapter" data-level="2.9" data-path="classification.html"><a href="classification.html#general-additive-model"><i class="fa fa-check"></i><b>2.9</b> General additive model</a></li>
<li class="chapter" data-level="2.10" data-path="classification.html"><a href="classification.html#turning-binary-classifiers-to-multi-class"><i class="fa fa-check"></i><b>2.10</b> Turning binary classifiers to multi-class</a></li>
<li class="chapter" data-level="2.11" data-path="classification.html"><a href="classification.html#performance-measures-and-evaluation"><i class="fa fa-check"></i><b>2.11</b> Performance measures and evaluation</a><ul>
<li class="chapter" data-level="2.11.1" data-path="classification.html"><a href="classification.html#signal-detection"><i class="fa fa-check"></i><b>2.11.1</b> Signal detection</a></li>
<li class="chapter" data-level="2.11.2" data-path="classification.html"><a href="classification.html#receiver-operating-characteristic-roc-and-area-under-the-curve-auc"><i class="fa fa-check"></i><b>2.11.2</b> Receiver operating characteristic (ROC) and Area under the curve (AUC)</a></li>
<li class="chapter" data-level="2.11.3" data-path="classification.html"><a href="classification.html#confusion-matrix"><i class="fa fa-check"></i><b>2.11.3</b> Confusion matrix</a></li>
<li class="chapter" data-level="2.11.4" data-path="classification.html"><a href="classification.html#benchmarking"><i class="fa fa-check"></i><b>2.11.4</b> Benchmarking</a></li>
<li class="chapter" data-level="2.11.5" data-path="classification.html"><a href="classification.html#stratified-sampling"><i class="fa fa-check"></i><b>2.11.5</b> Stratified sampling</a></li>
<li class="chapter" data-level="2.11.6" data-path="classification.html"><a href="classification.html#cross-validation-and-random-permutation"><i class="fa fa-check"></i><b>2.11.6</b> Cross validation and random permutation</a></li>
<li class="chapter" data-level="2.11.7" data-path="classification.html"><a href="classification.html#imbalance-data-sets"><i class="fa fa-check"></i><b>2.11.7</b> Imbalance data sets</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>3</b> Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="regression.html"><a href="regression.html#linear-regression"><i class="fa fa-check"></i><b>3.1</b> Linear regression</a></li>
<li class="chapter" data-level="3.2" data-path="regression.html"><a href="regression.html#multivariate-adaptive-regression-spline"><i class="fa fa-check"></i><b>3.2</b> Multivariate Adaptive Regression Spline</a></li>
<li class="chapter" data-level="3.3" data-path="regression.html"><a href="regression.html#generalized-additive-model"><i class="fa fa-check"></i><b>3.3</b> Generalized additive model</a></li>
<li class="chapter" data-level="3.4" data-path="regression.html"><a href="regression.html#decision-tree-for-regression"><i class="fa fa-check"></i><b>3.4</b> Decision tree for regression</a></li>
<li class="chapter" data-level="3.5" data-path="regression.html"><a href="regression.html#performance-measures"><i class="fa fa-check"></i><b>3.5</b> Performance measures</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>4</b> Dimensionality reduction</a><ul>
<li class="chapter" data-level="4.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#more-details-1"><i class="fa fa-check"></i><b>4.1</b> More details</a></li>
<li class="chapter" data-level="4.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#principal-component-analysis"><i class="fa fa-check"></i><b>4.2</b> Principal component analysis</a></li>
<li class="chapter" data-level="4.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#t-sne"><i class="fa fa-check"></i><b>4.3</b> T-SNE</a></li>
<li class="chapter" data-level="4.4" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#independent-component-analysis"><i class="fa fa-check"></i><b>4.4</b> Independent component analysis</a></li>
<li class="chapter" data-level="4.5" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#partial-least-square"><i class="fa fa-check"></i><b>4.5</b> Partial least square</a></li>
<li class="chapter" data-level="4.6" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>4.6</b> Linear discriminant analysis</a></li>
<li class="chapter" data-level="4.7" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#svm-dimensionality-reduction"><i class="fa fa-check"></i><b>4.7</b> SVM dimensionality reduction</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html"><i class="fa fa-check"></i><b>5</b> Metrics learning</a><ul>
<li class="chapter" data-level="5.1" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html#large-margin-nearest-neighbor"><i class="fa fa-check"></i><b>5.1</b> Large margin nearest neighbor</a></li>
<li class="chapter" data-level="5.2" data-path="sec-metricslearning.html"><a href="sec-metricslearning.html#metric-learning-for-kernel-regression"><i class="fa fa-check"></i><b>5.2</b> Metric learning for kernel regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>6</b> Neural networks</a><ul>
<li class="chapter" data-level="6.1" data-path="neural-networks.html"><a href="neural-networks.html#multi-layer-perceptron"><i class="fa fa-check"></i><b>6.1</b> Multi-layer perceptron</a></li>
<li class="chapter" data-level="6.2" data-path="neural-networks.html"><a href="neural-networks.html#mixed-density-networks"><i class="fa fa-check"></i><b>6.2</b> Mixed density networks</a></li>
<li class="chapter" data-level="6.3" data-path="neural-networks.html"><a href="neural-networks.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>6.3</b> Convolutional neural networks</a></li>
<li class="chapter" data-level="6.4" data-path="neural-networks.html"><a href="neural-networks.html#autoencoders"><i class="fa fa-check"></i><b>6.4</b> Autoencoders</a></li>
<li class="chapter" data-level="6.5" data-path="neural-networks.html"><a href="neural-networks.html#generative-adversial-neural-network"><i class="fa fa-check"></i><b>6.5</b> Generative adversial neural network</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>7</b> Bayesian inference</a></li>
<li class="chapter" data-level="8" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html"><i class="fa fa-check"></i><b>8</b> Ensemble techniques</a><ul>
<li class="chapter" data-level="8.1" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html#random-forest"><i class="fa fa-check"></i><b>8.1</b> Random forest</a></li>
<li class="chapter" data-level="8.2" data-path="ensemble-techniques.html"><a href="ensemble-techniques.html#gradient-boosting"><i class="fa fa-check"></i><b>8.2</b> Gradient boosting</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html"><i class="fa fa-check"></i><b>9</b> Reinforcement learning</a><ul>
<li class="chapter" data-level="9.1" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#q-learning"><i class="fa fa-check"></i><b>9.1</b> Q-learning</a></li>
<li class="chapter" data-level="9.2" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#curiosity"><i class="fa fa-check"></i><b>9.2</b> Curiosity</a><ul>
<li class="chapter" data-level="9.2.1" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#implementation-idea"><i class="fa fa-check"></i><b>9.2.1</b> Implementation idea</a></li>
<li class="chapter" data-level="9.2.2" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#what-does-it-solve"><i class="fa fa-check"></i><b>9.2.2</b> What does it solve?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html"><i class="fa fa-check"></i><b>10</b> Bagging and boosting</a><ul>
<li class="chapter" data-level="10.1" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#extreme-boosted-tree"><i class="fa fa-check"></i><b>10.1</b> Extreme boosted tree</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="anomaly-detection.html"><a href="anomaly-detection.html"><i class="fa fa-check"></i><b>11</b> Anomaly detection</a><ul>
<li class="chapter" data-level="11.1" data-path="anomaly-detection.html"><a href="anomaly-detection.html#autoencoder"><i class="fa fa-check"></i><b>11.1</b> Autoencoder</a></li>
<li class="chapter" data-level="11.2" data-path="anomaly-detection.html"><a href="anomaly-detection.html#one-class-svm"><i class="fa fa-check"></i><b>11.2</b> One class SVM</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="sec-preprocessing.html"><a href="sec-preprocessing.html"><i class="fa fa-check"></i><b>12</b> Preprocessing</a><ul>
<li class="chapter" data-level="12.1" data-path="sec-preprocessing.html"><a href="sec-preprocessing.html#normalization-and-standardization"><i class="fa fa-check"></i><b>12.1</b> Normalization and standardization</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="sec-signalProcessing.html"><a href="sec-signalProcessing.html"><i class="fa fa-check"></i><b>13</b> Signal processing</a></li>
<li class="chapter" data-level="14" data-path="sec-causal-models.html"><a href="sec-causal-models.html"><i class="fa fa-check"></i><b>14</b> Causal models</a><ul>
<li class="chapter" data-level="14.1" data-path="sec-causal-models.html"><a href="sec-causal-models.html#graphical-models"><i class="fa fa-check"></i><b>14.1</b> Graphical models</a></li>
<li class="chapter" data-level="14.2" data-path="sec-causal-models.html"><a href="sec-causal-models.html#bayesian-networks"><i class="fa fa-check"></i><b>14.2</b> Bayesian networks</a><ul>
<li class="chapter" data-level="14.2.1" data-path="sec-causal-models.html"><a href="sec-causal-models.html#improvements-3"><i class="fa fa-check"></i><b>14.2.1</b> Improvements</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="sec-causal-models.html"><a href="sec-causal-models.html#markov-random-field"><i class="fa fa-check"></i><b>14.3</b> Markov random field</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="important-considerations-in-machine-learning-methods.html"><a href="important-considerations-in-machine-learning-methods.html"><i class="fa fa-check"></i><b>15</b> Important considerations in machine learning methods</a><ul>
<li class="chapter" data-level="15.1" data-path="important-considerations-in-machine-learning-methods.html"><a href="important-considerations-in-machine-learning-methods.html#sec:biasVariance"><i class="fa fa-check"></i><b>15.1</b> The bias-variance debate</a></li>
<li class="chapter" data-level="15.2" data-path="important-considerations-in-machine-learning-methods.html"><a href="important-considerations-in-machine-learning-methods.html#sec:regularization"><i class="fa fa-check"></i><b>15.2</b> Regularization</a><ul>
<li class="chapter" data-level="15.2.1" data-path="important-considerations-in-machine-learning-methods.html"><a href="important-considerations-in-machine-learning-methods.html#famous-types"><i class="fa fa-check"></i><b>15.2.1</b> Famous types</a></li>
<li class="chapter" data-level="15.2.2" data-path="important-considerations-in-machine-learning-methods.html"><a href="important-considerations-in-machine-learning-methods.html#more-details-2"><i class="fa fa-check"></i><b>15.2.2</b> More details</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="important-considerations-in-machine-learning-methods.html"><a href="important-considerations-in-machine-learning-methods.html#raw-data-vs-characterized-data-features"><i class="fa fa-check"></i><b>15.3</b> Raw data vs characterized data (features)</a></li>
<li class="chapter" data-level="15.4" data-path="important-considerations-in-machine-learning-methods.html"><a href="important-considerations-in-machine-learning-methods.html#variable-importance-and-insights-considerations"><i class="fa fa-check"></i><b>15.4</b> Variable importance and insights considerations</a></li>
</ul></li>
<li class="part"><span><b>II Optimziation</b></span></li>
<li class="chapter" data-level="16" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>16</b> Introduction</a><ul>
<li class="chapter" data-level="16.1" data-path="introduction-1.html"><a href="introduction-1.html#derivative-free-vs-with-derivative-optimization-methods"><i class="fa fa-check"></i><b>16.1</b> Derivative-free vs with derivative optimization methods</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="optimization-problems.html"><a href="optimization-problems.html"><i class="fa fa-check"></i><b>17</b> Optimization problems</a><ul>
<li class="chapter" data-level="17.1" data-path="optimization-problems.html"><a href="optimization-problems.html#single-and-multi-objective"><i class="fa fa-check"></i><b>17.1</b> Single and Multi objective</a></li>
<li class="chapter" data-level="17.2" data-path="optimization-problems.html"><a href="optimization-problems.html#constrains-in-problems"><i class="fa fa-check"></i><b>17.2</b> Constrains in problems</a></li>
<li class="chapter" data-level="17.3" data-path="optimization-problems.html"><a href="optimization-problems.html#dynamic-optimization-problems"><i class="fa fa-check"></i><b>17.3</b> Dynamic optimization problems</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="use-of-derivative-in-optimization.html"><a href="use-of-derivative-in-optimization.html"><i class="fa fa-check"></i><b>18</b> Use of derivative in optimization</a></li>
<li class="chapter" data-level="19" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html"><i class="fa fa-check"></i><b>19</b> Derivative-free algorithms</a><ul>
<li class="chapter" data-level="19.1" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#finite-difference"><i class="fa fa-check"></i><b>19.1</b> Finite difference</a></li>
<li class="chapter" data-level="19.2" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#population-based-optimization"><i class="fa fa-check"></i><b>19.2</b> Population-based optimization</a><ul>
<li class="chapter" data-level="19.2.1" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#genetic-algorithm"><i class="fa fa-check"></i><b>19.2.1</b> Genetic algorithm</a></li>
<li class="chapter" data-level="19.2.2" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#evolutionary-strategy"><i class="fa fa-check"></i><b>19.2.2</b> Evolutionary strategy</a></li>
<li class="chapter" data-level="19.2.3" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#covariance-matrix-adaptation"><i class="fa fa-check"></i><b>19.2.3</b> Covariance matrix adaptation</a></li>
<li class="chapter" data-level="19.2.4" data-path="derivative-free-algorithms.html"><a href="derivative-free-algorithms.html#particle-swarm-optimization"><i class="fa fa-check"></i><b>19.2.4</b> Particle swarm optimization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="what-are-constraints.html"><a href="what-are-constraints.html"><i class="fa fa-check"></i><b>20</b> What are Constraints</a><ul>
<li class="chapter" data-level="20.1" data-path="what-are-constraints.html"><a href="what-are-constraints.html#how-to-deal-with-constraints"><i class="fa fa-check"></i><b>20.1</b> How to deal with constraints</a><ul>
<li class="chapter" data-level="20.1.1" data-path="what-are-constraints.html"><a href="what-are-constraints.html#sec:lagrangian"><i class="fa fa-check"></i><b>20.1.1</b> Lagrangian</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="famous-forms-of-optimization-problems.html"><a href="famous-forms-of-optimization-problems.html"><i class="fa fa-check"></i><b>21</b> Famous forms of optimization problems</a><ul>
<li class="chapter" data-level="21.1" data-path="famous-forms-of-optimization-problems.html"><a href="famous-forms-of-optimization-problems.html#linear-program"><i class="fa fa-check"></i><b>21.1</b> Linear program</a></li>
<li class="chapter" data-level="21.2" data-path="famous-forms-of-optimization-problems.html"><a href="famous-forms-of-optimization-problems.html#quadratic-objective-with-linear-constraints"><i class="fa fa-check"></i><b>21.2</b> Quadratic objective with linear constraints</a></li>
<li class="chapter" data-level="21.3" data-path="famous-forms-of-optimization-problems.html"><a href="famous-forms-of-optimization-problems.html#quadratic-objective-and-constraints"><i class="fa fa-check"></i><b>21.3</b> Quadratic objective and constraints</a></li>
</ul></li>
<li class="part"><span><b>III Statistics</b></span></li>
<li class="chapter" data-level="22" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>22</b> Introduction</a></li>
<li class="chapter" data-level="23" data-path="basics-statistics.html"><a href="basics-statistics.html"><i class="fa fa-check"></i><b>23</b> Basics statistics</a><ul>
<li class="chapter" data-level="23.1" data-path="basics-statistics.html"><a href="basics-statistics.html#correlation"><i class="fa fa-check"></i><b>23.1</b> Correlation</a></li>
<li class="chapter" data-level="23.2" data-path="basics-statistics.html"><a href="basics-statistics.html#moments"><i class="fa fa-check"></i><b>23.2</b> Moments</a></li>
<li class="chapter" data-level="23.3" data-path="basics-statistics.html"><a href="basics-statistics.html#covariance-matrix"><i class="fa fa-check"></i><b>23.3</b> Covariance matrix</a></li>
<li class="chapter" data-level="23.4" data-path="basics-statistics.html"><a href="basics-statistics.html#matrix-decomposition"><i class="fa fa-check"></i><b>23.4</b> Matrix decomposition</a><ul>
<li class="chapter" data-level="23.4.1" data-path="basics-statistics.html"><a href="basics-statistics.html#eigen-decomposition"><i class="fa fa-check"></i><b>23.4.1</b> Eigen decomposition</a></li>
<li class="chapter" data-level="23.4.2" data-path="basics-statistics.html"><a href="basics-statistics.html#singular-value-decomposition"><i class="fa fa-check"></i><b>23.4.2</b> Singular value decomposition</a></li>
</ul></li>
<li class="chapter" data-level="23.5" data-path="basics-statistics.html"><a href="basics-statistics.html#distributions"><i class="fa fa-check"></i><b>23.5</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="statistical-analysis.html"><a href="statistical-analysis.html"><i class="fa fa-check"></i><b>24</b> Statistical analysis</a><ul>
<li class="chapter" data-level="24.1" data-path="statistical-analysis.html"><a href="statistical-analysis.html#statistical-tests"><i class="fa fa-check"></i><b>24.1</b> Statistical tests</a></li>
<li class="chapter" data-level="24.2" data-path="statistical-analysis.html"><a href="statistical-analysis.html#causality"><i class="fa fa-check"></i><b>24.2</b> Causality</a></li>
<li class="chapter" data-level="24.3" data-path="statistical-analysis.html"><a href="statistical-analysis.html#anova"><i class="fa fa-check"></i><b>24.3</b> Anova</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="terms-and-notations.html"><a href="terms-and-notations.html"><i class="fa fa-check"></i><b>25</b> Terms and notations</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine learning, statistics, and optimization: A collection of intuitions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Classification</h1>
<p><strong><em>Category Intuitive</em></strong></p>
<ul>
<li>Classification aims to optimize a mathematical rule, aka model, that provides the category to which an instance belongs using a set of given examples (see Section <a href="introduction.html#sec:supervisedvsunsupervised">1.1</a> and <a href="introduction.html#sec:supervisedmodels">1.2</a>). An example of a classification problem is: given characteristics of a person, such as smoking habits, history of a heart attack in the family, number of hours of exercise per day, height, weight, among others, whether the person going to have a heart attack after their 50’s (yes/no). Another example of a classification problem is to find the type of a tumor (benign, type 1, type 2, or type 3) given its characteristics (e.g., shape, color, genetic information, history of the patient, among others).</li>
<li>As the training data-set is a subset of all possible data points, any model, in any shape and form, that separates the given instances and optimizes the evaluation measure is acceptable (purple, green, and orange lines in the Fig. <a href="classification.html#fig:classExample">2.1</a>).</li>
<li>Because of this incomplete knowledge, some assumptions need to be made to ensure that the “optimized” model performs well not only on the training data-set but also on unseen instances (generalization ability). This means defining “the best model”, a model that not only separates the given instances but also all other unseen instances, is limited by the knowledge encoded in the training set.</li>
<li>These assumptions include the shapes of the mathematical rule (a line, “if-then” rules, etc.) and the <em>evaluation</em> metric by which the rule is evaluated.</li>
<li>Different classification methods usually differ by their assumptions on the model, evaluation metric, and optimization method used.</li>
</ul>
<div class="figure"><span id="fig:classExample"></span>
<img src="images/classification/classification.png" alt="Two attributes (horizontal and vertical axes), two classes (red and blue), and three lines which can separate the classes successfully." width="49%" /><img src="images/classification/classification_.png" alt="Two attributes (horizontal and vertical axes), two classes (red and blue), and three lines which can separate the classes successfully." width="49%" />
<p class="caption">
Figure 2.1: Two attributes (horizontal and vertical axes), two classes (red and blue), and three lines which can separate the classes successfully.
</p>
</div>
<p> </p>
<p>To design a classifier, one would need the training data, the model (e.g., linear), an evaluation metric (e.g., maximum empirical margin in support vector machines), and an optimization algorithm to optimize the model parameters given the evaluation metric and the data. The evaluation metric is responsible to inform the optimizer on how well the current parameters perform while ensure the generalization ability is not sacrificed.</p>
<p>See <span class="citation">(Ng and Jordan <a href="#ref-ng2002discriminative">2002</a>)</span> for a formal definition of discriminative classification.</p>
<div id="generative-vs.discriminative-classifiers" class="section level2">
<h2><span class="header-section-number">2.1</span> Generative vs. discriminative classifiers</h2>
<p><strong><em>Category Intuitive, Deep</em></strong></p>

</div>
<div id="loss-function-for-classification" class="section level2">
<h2><span class="header-section-number">2.2</span> 0/1 loss function for classification</h2>
<p><strong><em>Category Intuitive</em></strong></p>
<ul>
<li>Assuming a binary classification problem (two classes) with a linear model, the objective is to find a hyperplane that “optimally” discriminates between the classes. To do so, we need an evaluation procedure which measures a “wrongness” score for any given hyperplane (called the <em>loss function</em>). Then, an optimization algorithm searches over all possible hyperplanes to find the one which minimizes the loss function (see Fig. <a href="classification.html#fig:findingLine">2.2</a>).</li>
</ul>
<div class="figure"><span id="fig:findingLine"></span>
<img src="images/classification/finding_line_n.gif" alt="The objective is to find a line/hyperplane that separates the instances optimally. The search continues until a suitable hyperplane, evaluated by the loss function, is found. The final line found in this example classifies all instances correctly except one." width="50%" />
<p class="caption">
Figure 2.2: The objective is to find a line/hyperplane that separates the instances optimally. The search continues until a suitable hyperplane, evaluated by the loss function, is found. The final line found in this example classifies all instances correctly except one.
</p>
</div>
<ul>
<li>One simple loss function would be to count the number of instances that are not in the “correct” side of the hyperplane, called the <em>0/1 loss function</em>. The outcome of this minimization is a hyperplane which has a minimum 0/1 loss value, i.e., the number of instances that are in the wrong side of the hyperplane is minimized.</li>
</ul>
<div class="figure"><span id="fig:loss01"></span>
<img src="images/classification/0_1_loss_1.PNG" alt="Two candidate lines, green gives 4/30 and the orange gives 4/30 loss value according to the 0/1 loss function." width="50%" />
<p class="caption">
Figure 2.3: Two candidate lines, green gives 4/30 and the orange gives 4/30 loss value according to the 0/1 loss function.
</p>
</div>
<p>  One should note that there might be many hyperplanes which have the same 0/1 loss value, which means the solutions to the 0/1 loss function is not unique. In Fig. <a href="classification.html#fig:loss01">2.3</a>, for example, the 0/1 loss value is the same for the green line and the orange line.</p>
<div id="improvements" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Improvements</h3>
<p><strong><em>Category Intuitive</em></strong></p>
<ul>
<li>The 0/1 loss function in its original form does not take into account the <em>importance</em> of the instances from different classes (all classes are assumed to be equally important). To address this issue, it has been proposed in <span class="citation">(Bonyadi and Reutens <a href="#ref-bonyadi2019optimal">2019</a>)</span> to minimize the average percentage of mis-classified instances accross all classes rather than counting the number of mis-classified instances.</li>
<li>This process does not provide a unique line. Both orange and green lines in Fig. <a href="classification.html#fig:classExample">2.1</a> have the same accuracy in terms of discriminating between the classes. This is solved by selecting the line (hyperplane) which has the maximum distance from the instances of both classes (see this <span class="citation">(Bonyadi and Reutens <a href="#ref-bonyadi2019optimal">2019</a>)</span>), called maximum margin hyperplane, the one which has the maximum distance from each class is preferred.</li>
<li>Other improvements in <span class="citation">(Bonyadi and Reutens <a href="#ref-bonyadi2019optimal">2019</a>)</span> included addition of <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> regularization, that enables the usage of coefficients for estimation of variable importance.</li>
</ul>
</div>
<div id="pros-and-cons" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Pros and cons</h3>
<p><strong><em>Category Intuitive</em></strong></p>
<p><em>Pros</em>: The 0/1 loss function is unbiased and not sensitive to outliers as any outlier would only contribute 1 unit to the loss function if it is miss-classified (see Fig. <a href="classification.html#fig:lossesExamples">2.4</a>).</p>
<div class="figure"><span id="fig:lossesExamples"></span>
<img src="images/classification/0_1_loss_2.PNG" alt="0/1 loss function is not sensitive to outliers while other loss functions are (other loss functions are described in next sections). The figure was taken from [@doerr2015direct]" width="50%" />
<p class="caption">
Figure 2.4: 0/1 loss function is not sensitive to outliers while other loss functions are (other loss functions are described in next sections). The figure was taken from <span class="citation">(Doerr et al. <a href="#ref-doerr2015direct">2015</a>)</span>
</p>
</div>
<p> </p>
<p><em>Cons</em>: The main issue with this idea (optimal 0/1 binary classification, with or without maximum margin idea) is that optimizing the 0/1 loss function in the formed mentioned before is not practical, i.e., it is NP-Complete. Hence, all algorithms which implement this idea are rather slow. The only practically fast implementation has been described in <span class="citation">(Bonyadi and Reutens <a href="#ref-bonyadi2019optimal">2019</a>)</span>, which uses evolutionary strategy for optimization. This has encouraged introduction to lots of new loss functions which approximate the 0/1 loss function while they are differentiable.</p>
</div>
<div id="implementation" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Implementation</h3>
<p><strong><em>Category Code</em></strong></p>
<p>Methods described in <span class="citation">(Doerr et al. <a href="#ref-doerr2015direct">2015</a>)</span> are in an algorithmic, step by step, format which makes implementation easier. The Python, Java, and Matlab code for <span class="citation">(Bonyadi and Reutens <a href="#ref-bonyadi2019optimal">2019</a>)</span> is available <a href="https://github.com/rezabonyadi/LinearOEC">here</a>.</p>

</div>
</div>
<div id="logistic-regression" class="section level2">
<h2><span class="header-section-number">2.3</span> Logistic regression</h2>
<p><strong><em>Category Intuitive</em></strong></p>
<p>Logistic regression seeks a hyperplane which best discriminates two classes. The hyperplane is evaluated by a loss function which is a smooth (differentiable) estimation of the function used in the 0/1 loss function, hence, can be optimized effectively by a gradient descent. While there might be many different choices for such function, logistic regression uses the logarithm sigmoid function that looks like Fig. <a href="classification.html#fig:logsig">2.5</a>.</p>
<div class="figure"><span id="fig:logsig"></span>
<img src="images/classification/logistic_func.PNG" alt="The logistic function is an estimation of the 0/1 function." width="50%" />
<p class="caption">
Figure 2.5: The logistic function is an estimation of the 0/1 function.
</p>
</div>
<p>More details on this algorithm can be found in <a href="http://ufldl.stanford.edu/tutorial/">Stanford University Tutorial on Supervised Learning</a>.</p>
<p>A derivative of Logistic Regression is the General Linear Model (GLM). ???</p>
<div id="interpretability" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Interpretability</h3>
<p><strong><em>Category Intuitive</em></strong></p>
<ul>
<li>The coefficients of the hyperplane found by the logistic regression <strong>cannot</strong> be interpreted directly as indicator for variable importance. The reason is that the variables ranges might be inherently different, meaning that some coefficients need to be larger to compensate for larger values. For example, if the values of one variable is in the range of 1000 and the other is in the range of 0.1, it is expected that the coefficients related to the first variable to be larger. This, however, does not show that the first variable is more important than the second.</li>
<li>If the value of variables are standardized (see Section <a href="sec-preprocessing.html#sec:preprocessing">12</a>), however, the coefficients can be used as indicators of importance. The smaller the absolute value of a coefficient is, the less important that variable is. To imagine this, think of a variable that is not important at all (see Fig. <a href="classification.html#fig:variableImport">2.6</a>), i.e., from the perspective of that variable, the instances from both groups are the same. We expect the discriminatory hyperplane to have a small or zero coefficient value for that variable.</li>
</ul>
<div class="figure"><span id="fig:variableImport"></span>
<img src="images/classification/v_importance_logistic_func_1.PNG" alt="????." width="49%" /><img src="images/classification/v_importance_logistic_func_2.PNG" alt="????." width="49%" />
<p class="caption">
Figure 2.6: ????.
</p>
</div>
</div>
<div id="pros-and-cons-1" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Pros and cons</h3>
<p><strong><em>Category Intuitive</em></strong></p>
<ul>
<li>Pros</li>
<li>It is easy to implement</li>
<li>Can be effectively solved by second order optimization methods</li>
<li>It supports sparse representation of data</li>
<li>Cons</li>
<li>Might not provide the most generalizable line</li>
</ul>
</div>
<div id="implementation-1" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Implementation</h3>
<p><strong><em>Category Code</em></strong></p>
<ul>
<li><strong>Implementation from scratch</strong>: see <a href="http://ufldl.stanford.edu/tutorial/">Stanford University Tutorial on Supervised Learning</a>.</li>
<li><strong>In Python</strong>: the <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.logistic.html">Scipy</a> library and the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">Sikit-Learn</a> library both have implementation of this.</li>
</ul>

</div>
</div>
<div id="bayes-classifier" class="section level2">
<h2><span class="header-section-number">2.4</span> Bayes classifier</h2>
<p><strong><em>Category Intuitive</em></strong></p>
<p>For a classification task, Bayes classifier calculates how likely it is that a given instance belongs to each class. Intuitively, the probability that a given instance, <span class="math inline">\(\vec x\)</span>, belongs to a class <span class="math inline">\(c\)</span> depends on two main components:</p>
<ul>
<li><em>Prior</em>: How likely it is that any given instance belongs to the class <span class="math inline">\(c\)</span> in general.</li>
<li><em>Likelihood</em>: If we know an instance from class <span class="math inline">\(c\)</span>, how likely it is that instance looks like <span class="math inline">\(\vec x\)</span>.</li>
</ul>
<p>For any given instance, we calculate these two and multiply their values. The outcome is a measure (not exact) of the probability if that instance belongs to class <span class="math inline">\(c\)</span>. The larger this value, the more likely it is that the instance belongs to the class <span class="math inline">\(c\)</span>.</p>
<p>This approach is called the <em>Bayes optimal classifier</em>. The first component is easy to calculate given the history. We can simply count the number of instances in the class <span class="math inline">\(c\)</span> and divide that by the total number of training instances, which represents how likely it is that an instance come from that class. The second component is very difficult, if possible, to calculate if we assume that the attributes are not independent. The reason is each instance is a <em>mix</em> of multiple attributes, some might be the same as what has been observed in the training set, some might not be. The comparison between these instances to calculate the <em>likelihood</em> is difficult as all attributes need to be considered at the same time. If we assume independence between variables, however, that probability is calculated easily. This is called the <em>Naive Bayes</em> classifier because the independence assumption is somewhat “naive”. For a given instance <span class="math inline">\(\vec x\)</span>, for each attribute, we calculate how likely it is that an instance from class <span class="math inline">\(c\)</span> has the same value as of <span class="math inline">\(\vec x\)</span> for that attribute, i.e., attributes are independent. We then multiply those probabilities which would be an estimate of the original likelihood with mixed variables.</p>
<p>See <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">wikipedia page</a> and <a href="https://www.geeksforgeeks.org/naive-bayes-classifiers/">this</a> for examples.</p>
<p>For discrete variables it is rather easy to calculate the probabilities based on the given instances. For continuous variables (e.g., age, weight), however, this probability needs to follow a distribution. Given the distribution, one can calculate the probabilities. The <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">wikipedia page</a> provides very good description on this algorithm.</p>
<div id="pros-and-cons-2" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Pros and cons</h3>
<p><strong><em>Category Intuitive</em></strong></p>
<p><strong>Pros</strong>:</p>
<ul>
<li>It is easy and very fast to predict class of test data set.</li>
<li>When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.</li>
<li>It performs well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption and might not be correct).</li>
<li>It works well with sparse data-sets.</li>
<li>Organically handle multiple classes.</li>
<li>They are generative and can be easily interpreted as discriminative</li>
<li>Missed values can be easily dealt with</li>
<li>If the value for a variable is missed completely, they would still work with simple tricks (not the case for many classifiers)</li>
</ul>
<p><strong>Cons</strong>:</p>
<ul>
<li>If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique.</li>
<li>Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.</li>
<li>For continuous variables, it is assumed that the variables follow a given distribution (usually normal) to be able to calculate the probability, which may not be accurate.</li>
</ul>
</div>
<div id="implementation-2" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Implementation</h3>
<p><strong><em>Category Code</em></strong></p>
<p>For implementation from scratch see <a href="https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/">here</a>. In Python, <a href="https://scikit-learn.org/stable/modules/naive_bayes.html">scikit-learn</a> can be used.</p>

</div>
</div>
<div id="support-vector-machines" class="section level2">
<h2><span class="header-section-number">2.5</span> Support vector machines</h2>
<p><strong><em>Category Intuitive</em></strong></p>
<ul>
<li>Consider a binary classification problem and assume we want to use a hyperplane (linear model) to separate the classes.</li>
<li>There might be many hyperplanes which separate the classes, all of which would result in the same mis-classification error. This encourages design of constraints which narrow down acceptable hyperplanes.</li>
<li>One reasonable constraint is to pick the hyperplane which has maximum distance from the instances from both classes. This idea forms the bases for the support vector machine (SVM).</li>
</ul>
<div class="figure"><span id="fig:manySolutions"></span>
<img src="images/classification/svm_lots_of_choices.gif" alt="Many hyperplanes may perform similarly in separating classes." width="50%" />
<p class="caption">
Figure 2.7: Many hyperplanes may perform similarly in separating classes.
</p>
</div>
<div class="figure"><span id="fig:svm1"></span>
<img src="images/classification/svm_1.png" alt="The blue line has the same distance from the instances in each class, which is the idea behind SVM." width="50%" />
<p class="caption">
Figure 2.8: The blue line has the same distance from the instances in each class, which is the idea behind SVM.
</p>
</div>
<ul>
<li>It is expected that SVM works better than Logistic Regression on the unseen instances. This, however, is not always true as the assumption behind SVM might not be always true and it is bound by the quality of the training data (see Section <a href="introduction.html#sec:supervisedmodels">1.2</a>).</li>
</ul>
<p>See <a href="https://en.wikipedia.org/wiki/Support-vector_machine#Linear_SVM">wikipedia</a> for more details on how this is formulated by the hinge loss and how it can be solved by gradient descent. See also <a href="https://jeremykun.com/2017/06/05/formulating-the-support-vector-machine-optimization-problem/">this</a> which describes SVM nicely.</p>
<div id="interpretability-1" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Interpretability</h3>
<ul>
<li>Similar to Logistic Regression, absolute value of coefficients of linear SVM provide variable importance only if the variables values have been standidized (see section <a href="sec-preprocessing.html#sec:preprocessing">12</a>)</li>
<li>L1 regularization in SVM can be used for “feature selection” (see Section <a href="important-considerations-in-machine-learning-methods.html#sec:regularization">15.2</a>)</li>
<li></li>
</ul>
</div>
<div id="improvements-1" class="section level3">
<h3><span class="header-section-number">2.5.2</span> Improvements</h3>
<ul>
<li>While SVM aims can be formulated as a quadratic program, it can also be formulated by a linear program. See <span class="citation">(Zhou, Zhang, and Jiao <a href="#ref-zhou2002linear">2002</a>)</span> for details.</li>
<li>SVM can be also used for supervised dimensionality reduction, see <span class="citation">(Tao, Chu, and Wang <a href="#ref-tao2008recursive">2008</a>)</span>.</li>
<li>One-class SVM is also used for anomaly/outlier/novelty detection. See ????…</li>
<li>SVM can be extended to provide non-linear hyperplanes to seperate the classes. This can be done by adding Kernel (see <a href="https://en.wikipedia.org/wiki/Kernel_method">kernel trick</a> and <a href="https://en.wikipedia.org/wiki/Representer_theorem">representer theorem</a>) to the algorithm. See <a href="https://en.wikipedia.org/wiki/Support-vector_machine#Linear_SVM">wikipedia</a> for how.</li>
</ul>
<div class="figure"><span id="fig:svmRBF"></span>
<img src="images/classification/svm_RBF.gif" alt="The blue line has the same distance from the instances in each class, which is the idea behind SVM." width="50%" />
<p class="caption">
Figure 2.9: The blue line has the same distance from the instances in each class, which is the idea behind SVM.
</p>
</div>
</div>
<div id="implementation-3" class="section level3">
<h3><span class="header-section-number">2.5.3</span> Implementation</h3>
<p><strong><em>Category Code</em></strong></p>
<ul>
<li>See ??? for implementation from scratch.</li>
<li>It is also available with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">sklearn</a> in Python.</li>
</ul>

</div>
</div>
<div id="sec:descisiontree" class="section level2">
<h2><span class="header-section-number">2.6</span> Decision tree</h2>
<p><a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/" class="uri">http://www.r2d3.us/visual-intro-to-machine-learning-part-1/</a></p>

</div>
<div id="sec:KNN" class="section level2">
<h2><span class="header-section-number">2.7</span> K-nearest neighbor</h2>
<p><strong><em>Category I</em></strong></p>
<p>The k-nearest neighbor (KNN) classifier assumes that the instances “close” to one another have the same class label. Hence, to assign a class label to a new instance, KNN finds <span class="math inline">\(k\)</span> instances from the training data set to which the new instance is “closest” and use those labels to vote for the class label of the new instance (see Fig. <a href="classification.html#fig:KnnDemons">2.10</a>).</p>
<div class="figure"><span id="fig:KnnDemons"></span>
<img src="images/classification/01_knn_1.PNG" alt="Example of how KNN works. The &quot;closest&quot; instances to a new instance (green) are used to vote for its class label." width="50%" />
<p class="caption">
Figure 2.10: Example of how KNN works. The “closest” instances to a new instance (green) are used to vote for its class label.
</p>
</div>
<p>The “closeness” is defined by a distance measure, such as Euclidean distance.</p>
<div id="improvements-2" class="section level3">
<h3><span class="header-section-number">2.7.1</span> Improvements</h3>
<p><strong><em>Category I</em></strong></p>
<p>In KNN, some attributes may lead to a biased distance. For example, if one of the attributes is in the order of 1,000 and another is in the order of 0.1, the latter would have a small impact on the calculation of the distance. This is usually resolved by standardizing the space, which ensures all attributes are in the same range. This method, however, may break some structural integrity of the instances.</p>
<p>Another issue is that some attributes might be misleading and their impact is better to be reduced. For example, Fig. <a href="classification.html#fig:KnnDimesions">2.11</a> indicates that the horizontal dimension is responsible for the green instance to be of the type “red”. This, however, may not be correct as if the horizontal dimension shrinks then the green instance becomes closer to blue instances, which makes it a blue class.</p>
<div class="figure"><span id="fig:KnnDimesions"></span>
<img src="images/classification/knn_scale.png" alt="Example of how KNN works. The &quot;closest&quot; instances to a new instance (green) are used to vote for its class label." width="50%" />
<p class="caption">
Figure 2.11: Example of how KNN works. The “closest” instances to a new instance (green) are used to vote for its class label.
</p>
</div>
<p>This indeed leads to an in-accuracy in KNN; the algorithm does not take into account the importance of attributes. This can be addressed by optimizing a metric which shrinks/contracts the space along different attributes to achieve the best transformation in which KNN performs best. See Section <a href="sec-metricslearning.html#sec:metricslearning">5</a> for details.</p>
</div>
<div id="pros-and-cons-3" class="section level3">
<h3><span class="header-section-number">2.7.2</span> Pros and cons</h3>
<p><strong><em>Category Intuitive</em></strong></p>
<p><em>Pros</em>:</p>
<ul>
<li>It is easy to implement.</li>
<li>It can model non-linearity.</li>
<li>The assumption behind KNN is not parametric and only depends on the given data.</li>
</ul>
<p><em>Cons</em>:</p>
<ul>
<li>Requires storage of the training data. This can be reduced as proposed by [???]</li>
<li>The closeness needs to be defined. Euclidean distance is an obvious choice, however, it is not always optimal [????].</li>
<li>Requires searching for the closest instances in the training set, which is slow. This can be resolved by using smart search methods [????]</li>
<li>Some of the dimensions may lead to biasing the distance</li>
<li>KNN does not provide any information about the importance of attributes. This, however, is resolved by optimizing a metric to transform the space to represent the training data optimally (see section <a href="sec-metricslearning.html#sec:metricslearning">5</a>)</li>
</ul>
</div>
<div id="more-details" class="section level3">
<h3><span class="header-section-number">2.7.3</span> More details</h3>
<p><strong><em>Category Deep</em></strong></p>
</div>
<div id="implementation-4" class="section level3">
<h3><span class="header-section-number">2.7.4</span> Implementation</h3>
<p><strong><em>Category Code</em></strong></p>

</div>
</div>
<div id="gaussianprocessclassifier" class="section level2">
<h2><span class="header-section-number">2.8</span> Gaussian process classifier</h2>
<p>TODO</p>
<p><a href="https://distill.pub/2019/visual-exploration-gaussian-processes/" class="uri">https://distill.pub/2019/visual-exploration-gaussian-processes/</a></p>

</div>
<div id="general-additive-model" class="section level2">
<h2><span class="header-section-number">2.9</span> General additive model</h2>

</div>
<div id="turning-binary-classifiers-to-multi-class" class="section level2">
<h2><span class="header-section-number">2.10</span> Turning binary classifiers to multi-class</h2>
<p>One vs one</p>
<p>One vs all</p>

</div>
<div id="performance-measures-and-evaluation" class="section level2">
<h2><span class="header-section-number">2.11</span> Performance measures and evaluation</h2>
<p><strong><em>Category Intuitive</em></strong></p>
<p><a href="https://medium.com/@wilamelima/metrics-to-measure-machine-learning-model-performance-e8c963665476" class="uri">https://medium.com/@wilamelima/metrics-to-measure-machine-learning-model-performance-e8c963665476</a></p>
<p>Consider we are solving a binary (2-class) classification problem using a classification algorithm. Let’s assume there are 100 items of class 1 and 200 of the class 0 in our training set. The classifier uses this data set to learn the patter of the data and provide a general rule to classify the instances. After training, it can classify 75 items of the class 0 and 190 of the class 1 correctly. Now, the question is, how well the classifier is doing its job? One simple way is to calculate the error percentage: <span class="math inline">\(100\frac{75+190}{100+200}=88.3\)</span> percent. But is this number a good indicative of how well the classifier is performing?</p>
<div id="signal-detection" class="section level3">
<h3><span class="header-section-number">2.11.1</span> Signal detection</h3>
<p><strong><em>Category Intuitive</em></strong></p>
<p>Signal detection claims that the error percentage is not an accurate measure of performance as it ignores the frequency of the items. For example, in the example above, the number of items in class 0 is twice as much as the the number of items in the class 1. This is usually the case in real-world data sets, i.e., the number of items in classes is imbalance (the number of unhealthy subjects is much smaller than the number of healthy ones). This poses lots of complications to the evaluation of classification methods. For example, assume that the number of items in one class is 100 times larger than the number of items in the other. Mis-classification of items from the smaller class leads to more “catastrophic” decisions as the more rare events are usually the most valuable ones which need to be detected correctly.</p>
<p>Traditionally, it is assumed that the items in class 1 are the ones we want to recognize (e.g., unhealthy subjects) and items in class 0 are “normal” items. In reality, the frequency of items in class 1 is usually much smaller than the items in class 0. Note that, algebraically, the class labels do not make any difference to the problem and its solution. Here, however, for the sake of definitions, we consider class 1 as positive recognition of items.</p>
<p>Signal detection measures four metrics to quantify performance: False positive, true positive, false negative, and true negative. * <strong><em>False positive</em></strong> (aka false alarm) is the number of responses which the classifier recognized as class 1 while they actually belong to class 0 (i.e., falsely recognized as positive). This is very important to be high when the number of instances in the classes is not balance. For example, if a classifier which is used for diagnosing an illness has a high false positive, it is likely to diagnose a healthy person as ill. Another example would be using a classifier to either invest or not to invest money on a business. High false positive would lead to loss of money. * <strong><em>True positive</em></strong> (aka hit) is the number of responses which the classifier recognized as 1 and they actually belong to class 1. We desire this to be as high as possible. This, however, might not be as important as it sounds. For example, in the investment example, one would prefer to deal with a low false positive and not to loose money than always invest correctly. In fact, a less frequent correct investment (low true positive) is ok, but not loosing money (low false positive) is very important. * <strong><em>False negative</em></strong> (aka miss) is the number of responses which the classifier recognized as 0 and they actually belong to class 1. This is particularly important in the illness diagnosis example. In fact, it is preferred to diagnose someone with a terminal illness (high false positive) and prescribe some more tests than missing if the person has a terminal illness and lead to their death. * <strong><em>True negative</em></strong> (aka ) is the number of responses which the classifier recognized as 0 and they actually belong to class 0.</p>
<p>These four measures provide a better tool to evaluate the performance of the classifier. The importance of these factors, however, is problem dependent, as described by examples.</p>
<p>Specificty and ???</p>
</div>
<div id="receiver-operating-characteristic-roc-and-area-under-the-curve-auc" class="section level3">
<h3><span class="header-section-number">2.11.2</span> Receiver operating characteristic (ROC) and Area under the curve (AUC)</h3>
<p><strong><em>Category Intuitive</em></strong></p>
</div>
<div id="confusion-matrix" class="section level3">
<h3><span class="header-section-number">2.11.3</span> Confusion matrix</h3>
<p><strong><em>Category Intuitive</em></strong></p>
</div>
<div id="benchmarking" class="section level3">
<h3><span class="header-section-number">2.11.4</span> Benchmarking</h3>
<p><strong><em>Category Intuitive, Code</em></strong></p>
<p><a href="https://github.com/EpistasisLab/penn-ml-benchmarks" class="uri">https://github.com/EpistasisLab/penn-ml-benchmarks</a> <a href="https://www.openml.org/home" class="uri">https://www.openml.org/home</a></p>
</div>
<div id="stratified-sampling" class="section level3">
<h3><span class="header-section-number">2.11.5</span> Stratified sampling</h3>
<p><strong><em>Category Intuitive</em></strong></p>
</div>
<div id="cross-validation-and-random-permutation" class="section level3">
<h3><span class="header-section-number">2.11.6</span> Cross validation and random permutation</h3>
<p><strong><em>Category Intuitive, Deep</em></strong></p>
</div>
<div id="imbalance-data-sets" class="section level3">
<h3><span class="header-section-number">2.11.7</span> Imbalance data sets</h3>
<p><strong><em>Category Intuitive, Deep</em></strong></p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-bonyadi2019optimal">
<p>Bonyadi, Mohammad Reza, and David C Reutens. 2019. “Optimal-Margin Evolutionary Classifier.” <em>IEEE Transactions on Evolutionary Computation</em>. IEEE. <a href="https://arxiv.org/abs/1804.09891" class="uri">https://arxiv.org/abs/1804.09891</a>.</p>
</div>
<div id="ref-doerr2015direct">
<p>Doerr, Andreas, Nathan D Ratliff, Jeannette Bohg, Marc Toussaint, and Stefan Schaal. 2015. “Direct Loss Minimization Inverse Optimal Control.” In <em>Robotics: Science and Systems</em>. <a href="http://proceedings.mlr.press/v28/nguyen13a.pdf" class="uri">http://proceedings.mlr.press/v28/nguyen13a.pdf</a>.</p>
</div>
<div id="ref-ng2002discriminative">
<p>Ng, Andrew Y, and Michael I Jordan. 2002. “On Discriminative Vs. Generative Classifiers: A Comparison of Logistic Regression and Naive Bayes.” In <em>Advances in Neural Information Processing Systems</em>, 841–48.</p>
</div>
<div id="ref-tao2008recursive">
<p>Tao, Qing, Dejun Chu, and Jue Wang. 2008. “Recursive Support Vector Machines for Dimensionality Reduction.” <em>IEEE Transactions on Neural Networks</em> 19 (1). IEEE: 189–93.</p>
</div>
<div id="ref-zhou2002linear">
<p>Zhou, Weida, Li Zhang, and Licheng Jiao. 2002. “Linear Programming Support Vector Machines.” <em>Pattern Recognition</em> 35 (12). Elsevier: 2927–36.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
