# (PART) Machine learning {-} 
# Introduction

Machine learning refers to a set of algorithms which make a machine (with a computer as its brain) to apparently learn. While learning in animals and human is not very well understood, in machines it is simply to find a generic rule, given a set of examples, which  



Not fully observed, hence, there need to be assumptions as the solutions is not unique

## Raw data vs characterized data (features) 

Feature space

## Supervised and unsupervised learning
Supervised learning asks for 

Unsupervised learning, on the other hand, ...

## Supervised models
In a more general framework, given a set of instances, $X$, with $m$ rows and $n$ columns (each row is an instance and each column is an attribute) and their associated responses, $Y$, with $m$ rows, a supervised model $M(.,\theta)$, $\theta$ being model parameters (e.g., linear model), provides a generic rule by which the response, $y$, is correctly estimated for any given instance, $x$ ($1$ row and $n$ columns). The parameters $\theta$ are optimized by an optimizer for the model $M$ to best satisfy an *evaluation* metric (in some algorithms, this is called the *loss function*) which evaluates how similar $M(.,\theta)$ is to actual $y$ for each $x$ and any given $\theta$.

It is very important to design the model carefully as it is responsible to represent the "pattern" in the data in the most generic and accurate way. You may ask why not picking the most flexible mathematical equation in existence (e.g., Taylor series) and optimize its parameters to fit the given data? Flexible models (e.g., deep networks) may be able to fit the data well (simply because they are flexible and can fit anything), however, they might not be able to *generalize* well, i.e., work well for unseen data. Sometimes the models come from specific knowledge about the problem, e.g., physics models. 

## The bias-variance debate {#sec:biasVariance} 
Assume there is a set of given instances, $X$ (m rows and n columns), and we are asked to optimize a model $M(X, \theta)$ to estimate $Y$ (m rows) given some examples, i.e., supervised learning. Let us call each instance $x$ and its corresponding desired output as $y$. In reality, $y$ is the outcome of a system working with inputs $x$, and some noise, $y=f(x)+\epsilon$, where $f(.)$ is not known and $\epsilon$ is noise which is also unknown. For example, if $x$ represents characteristics of people (smoking habit, weight, genetics, etc.) and $y$ is whether or not they would have a heart-attack after their 50s, then $f(x)$ is how body would respond to those characteristics and leads to a heart-attack or not, which is not really known. With modelling, we try to estimate this function $f(x)$, given some examples and some assumptions on the shape of $f$. 

After optimization, what we would get is $Y=M(X, \hat{\theta})+\epsilon$, where $\epsilon$ has $m$ rows and indicates the error from $Y$. If $\epsilon$ is very small then that means $M(X, \hat{\theta})$ is an accurate estimation of $Y$. If the model M is a complex, flexible, equation then it will be able to fit any complex behavior of $Y$ as function of $X$, including all fluctuations. In that case, the outputs of the model will be very very similar to the values of $Y$ (non-biased), however, it will have lots of fluctuations, which leads to a high variance (large fluctuations usually leads to large variance). If the model is not complex though, the gap between the output of the model and $Y$ might be large (bias), however, it will not fit all fluctuations in the $Y$ which leads to smaller variance. That is why it is said a complex model has a large variance and small bias, and vice versa.


