# (PART) Machine learning {-} 
# Introduction

Machine learning refers to a set of algorithms which make a machine (with a computer as its brain) to apparently learn. While learning in animals and human is not very well understood, in machines it is simply to find a generic rule, given a set of examples, which  

Machine learning refers to a set of methodologies that make machines apparently "learning" to perform tasks. Although it is hard to define learning, it is related to progressive improvement in performing a task successfully. Intuitively there are two types of learning: self-learning (unsupervised) and learning from others (supervised).

Not fully observed, hence, there need to be assumptions as the solutions is not unique

## Raw data vs characterized data (features) 
The data in its original form, raw form, represents what is measured. The measurements, however, are not always useful to learn from. For example, in a stock market signal, the actual close daily prices are not as important as the trend of those values. Hence, depending on the problem, some characteristics of the data might be more informative than  the raw data itself. These characteristics are usually called features. Features basically represent important (relevant to a given problem which uses that data for solution) attributes of the problem. They are either designed by an expert (i.e., feature engineering) or optimized by an algorithm for a given model (i.e., feature formation, popular in deep learning). For example, assume that we want to characterize fruits. Some obvious features of fruits are their shapes (e.g., being round, or egg-shaped, or long), their tastes, and their smell. Features characterize attributes that distinguish different objects

For many problems, the features are already known. For example, we already know that a feed in a newspaper is considered interesting for a particular reader if the reader spends more time on it (the spent time is a feature). However, recognizing someone's face form an image is not easy to be defined by apparent features. This encourages methods like Deep Learning to not only learn the task but also find the best presentation for the problem, just from the given data.

## Supervised and unsupervised learning {#sec:supervisedvsunsupervised}
In supervised learning the learner learns to accomplish a task with supervision, i.e., there is a set of instances that the learner learns from, or there is a system that provides feedback to the actions of the learner (reinforcement). Imagine for example you need a system to learn how to drive. For a human, we just hire an instructor and train the person and done. This is precisely the same for a machine, we provide instances or a feedback loop so that the machine can evaluate its performance and learns. Applications of supervised learning include self-driving cars, stock market prediction, object recognition, natural language processing, maintenance prediction, recommendation engines, weather forecast, and more.

In the unsupervised learning, the algorithm "figures out" the patterns in the instances, without any supervision. This is the more sophisticated while the most exciting use of machine learning. Customer segmentation, anomaly detection, product similarities, and many other applications require unsupervised learning.

A human or an animal learns from the environment by [re-structuring the connectivity of neurons in their brain](https://www.cam.ac.uk/research/features/lifelong-learning-and-the-plastic-brain). The phenomenon has not been well understood yet but, in the abstract level, it is just like branching a graph, with billions of nodes and connections, to work better with the learning task at hand. The aim is to provide a generic (abstract) representation of the problem and the solution in the brain that is accessible when needed. This, however, is slightly different for a machine. The problem needs to first be translated into a "mathematical space" (coordinate systems, or simply numbers),the instances (samples of the learning task) need to be presented by their basic components (so-called features) in a "vector space" (numbers each with a particular meaning). 

## Supervised models {#sec:supervisedmodels}
Let's assume a set of $m$ instances, $X$, with $m$ rows and $n$ columns (each row is an instance and each column is a feature) and their associated responses, $Y$, with $m$ rows, is given (called the training data set). A supervised model $M(.,\theta)$, $\theta$ being model parameters (e.g., linear model), provides a generic rule by which the response, $y$, is correctly estimated for any given instance, $x$ ($1$ row and $n$ columns). The parameters $\theta$ are optimized by an optimizer for the model $M$ to best satisfy an *evaluation* metric (in some algorithms, this metric is called the *loss function*) which evaluates how similar $M(.,\theta)$ is to actual $y$ for each $x$ and any given $\theta$.

Usually, the given instances (training set) do not represent all possible instances for a given problem. Therefore, any model, in any shape and form, that transforms $X$ to $Y$ and optimizes the evaluation measure is acceptable. This means defining "the best model", a model that not only separates the given instances but also all other unseen instances, is limited by the knowledge encoded in the training set. This incomplete knowledge about the data leads the designer to make some *assumptions* on the model and the evaluation metric upon which different classifiers are formulated. For example, support vector machines in their original form assume that the discriminatory rule is represented by a line (an assumption on the model, a linear model) which has the maximum distance from the instances in each class (an assumption on the evaluation metric). The idea is that such a line is more empirically robust against potential uncertainties in unseen instances. 

It is important to design the model carefully as it is responsible to represent the "pattern" in the data in the most generic (low variance, see Section \@ref(sec:biasVariance)) and accurate (low bias) way. You may ask why not picking the most flexible mathematical equation in existence (e.g., Taylor series) and optimize its parameters to fit the given data? Flexible models (e.g., deep networks) may be able to fit the data well (simply because they are flexible and can fit anything), however, they might not be able to *generalize* well, i.e., work well for unseen data. Sometimes the models come from specific knowledge about the problem, e.g., physics models. 

## The bias-variance debate {#sec:biasVariance} 
Assume there is a set of given instances, $X$ (m rows and n columns), and we are asked to optimize a model $M(X, \theta)$ to estimate $Y$ (m rows) given some examples, i.e., supervised learning. Also, it is assumed that the underlying pattern of the data is not known (note that, this is a very important assumption as if this is not true then any model except the one which formulates the pattern is irrelevant). Another assumption is that the collected data may have some noise in it (another important assumption). If $M$ is a flexible formula (which usually means it involves lots of parameters to complex formulations) then it can fit to the given data-set very well. This, however, means that the model would formulate also the noise in the data, which would impose lots of fluctuations and non-linearities that are not really related to the pattern in the data but the noise in the observations. This leads to a model that has a low bias. However, as it is very flexible and formulates lots of fluctuations coming from random events (noise), it also offer a high variance. This high variance and low bias would lead to worsening the generalization ability of the model as it is not really formulating the pattern in the data but also the noise and unnecessary fluctuations.

Let's go a bit more detail. Let us call each instance $x$ and its corresponding desired output as $y$. In reality, $y$ is the outcome of a system working with inputs $x$, and some noise, $y=f(x)+\epsilon$, where $f(.)$ is not known and $\epsilon$ is noise which is also unknown. For example, if $x$ represents characteristics of people (smoking habit, weight, genetics, etc.) and $y$ is whether or not they would have a heart-attack after their 50s, then $f(x)$ is how body would respond to those characteristics and leads to a heart-attack or not, which is not really known. With modelling, we estimate this function $f(x)$, given some examples and some assumptions on the shape of $f$. 

A model $M$ is responsible to estimate observed $Y$ as $Y=f(X)+\epsilon=M(X, \hat{\theta})+e$, where $e$ has $m$ rows and indicates the error from $Y$. If $e$ is small then $M(X, \hat{\theta})$ is an accurate estimation of $Y$, hence the model estimates not only $f$ but large amount of the noise $\epsilon$ in it. This takes place if the model $M$ is a complex, flexible, equation which is able to fit any complex behavior of $Y$ as function of $X$, including all fluctuations resulting from the noise. Hence, the outputs of the model will be similar to the values of $Y$ (non-biased), which includes noisy fluctuations, which leads to a high variance (fluctuations usually lead to large variance). If the model is not complex though, the gap between the output of the model and $Y$ might be large (bias), however, it would fit less to the noisy fluctuations in the $Y$ which leads to a smaller variance. That is why it is said a complex model has a large variance and small bias, and vice versa. Formal calculation of this trade off can be found in the [bias-variance tradeoff in wikipedia](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Derivation).


