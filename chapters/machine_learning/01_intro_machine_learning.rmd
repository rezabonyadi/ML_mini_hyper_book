# (PART) Machine learning {-} 
# Introduction

Machine learning refers to a set of algorithms which make a machine (with a computer as its brain) to apparently learn. While learning in animals and human is not very well understood, in machines it is simply to find a generic rule, given a set of examples, which  



Not fully observed, hence, there need to be assumptions as the solutions is not unique

## Raw data vs characterized data (features) 

Feature space

## Supervised and unsupervised algorithms


## Supervised models

In a more general framework, given a set of instances, $X$, with $m$ rows and $n$ columns (each row is an instance and each column is an attribute) and their associated classes, $Y$, with $m$ rows (containing a categorical value in each row), a classifier is tasked to find an *optimal* model $M(.,\theta)$, $\theta$ being model parameters (e.g., linear model), which for any given instance, $x$ ($1$ row and $n$ columns), can generate the class label, $y$, correctly. The parameters $\theta$ are optimized by an optimizer for the model $M$ to best satisfy an *evaluation* metric (in some algorithms, this is called the *loss function*) which evaluates how similar $M(.,\theta)$ is to actual $y$ for each $x$ and any given $\theta$.

Usually, the given instances (training set) do not represent all possible instances for a given problem. Therefore, any model, in any shape and form, that separates the given instances and optimizes the evaluation measure is acceptable (purple, green, and orange lines in the figure). This means defining "the best model", a model that not only separates the given instances but also all other unseen instances, is limited by the knowledge encoded in the training set. This incomplete knowledge about the data leads the designer to make some *assumptions* on the model and the evaluation metric upon which different classifiers are formulated. For example, support vector machines in their original form assume that the discriminatory rule is represented by a line (an assumption on the model, a linear model) which has the maximum distance from the instances in each class (an assumption on the evaluation metric). The idea is that such a line is more empirically robust against potential uncertainties in unseen instances. 

Note that the model is responsible to represent the "pattern" in the data in the most generic and accurate way, hence, very important to be designed carefully. You may ask why not picking the most flexible mathematical equation in existence (e.g., Taylor series) and optimize its parameters to fit the given data? Flexible models (e.g., deep networks) may be able to fit the data well (simply because they are flexible and can fit anything), however, they might not be able to *generalize* well, i.e., work well for unseen data. Sometimes the models come from specific knowledge about the problem, e.g., physics models. 



## The bias-variance debate {#sec:biasVariance} 

Assume there is a set of given instances, $X$ (m rows and n columns), and we are asked to optimize a model $M(X, \theta)$ to estimate $Y$ (m rows) given some examples, i.e., supervised learning. Let us call each instance $x$ and its corresponding desired output as $y$. In reality, $y$ is the outcome of a system working with inputs $x$, and some noise, $y=f(x)+\epsilon$, where $f(.)$ is not known and $\epsilon$ is noise which is also unknown. For example, if $x$ represents characteristics of people (smoking habit, weight, genetics, etc.) and $y$ is whether or not they would have a heart-attack after their 50s, then $f(x)$ is how body would respond to those characteristics and leads to a heart-attack or not, which is not really known. With modelling, we try to estimate this function $f(x)$, given some examples and some assumptions on the shape of $f$. 

After optimization, what we would get is $Y=M(X, \hat{\theta})+\epsilon$, where $\epsilon$ has $m$ rows and indicates the error from $Y$. If $\epsilon$ is very small then that means $M(X, \hat{\theta})$ is an accurate estimation of $Y$. If the model M is a complex, flexible, equation then it will be able to fit any complex behavior of $Y$ as function of $X$, including all fluctuations. In that case, the outputs of the model will be very very similar to the values of $Y$ (non-biased), however, it will have lots of fluctuations, which leads to a high variance (large fluctuations usually leads to large variance). If the model is not complex though, the gap between the output of the model and $Y$ might be large (bias), however, it will not fit all fluctuations in the $Y$ which leads to smaller variance. That is why it is said a complex model has a large variance and small bias, and vice versa.


