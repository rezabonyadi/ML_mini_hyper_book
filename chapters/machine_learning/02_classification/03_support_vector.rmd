## Support vector machines
***Category Intuitive***

Consider a binary classification problem and assume we want to use a hyperplane (linear model) to separate the classes. There might be many hyperplanes which separate the classes, all of which would result in the same mis-classification error. This encourages design of constraints which narrow down acceptable hyperplanes. One reasonable constraint is to pick the hyperplane which has maximum distance from the instances from both classes. This idea forms the bases for the support vector machine (SVM). 


```{r svm1, echo = FALSE, out.width = '50%', fig.cap = 'The blue line has the same distance from the instances in each class, which is the idea behind SVM.'}
knitr::include_graphics('images/classification/svm_1.PNG')
```

It is expected that SVM works better than Logistic Regression on the unseen instances. This, however, is not always true as the assumption behind SVM might not be always true and it is bound by the quality of the training data (see Section \@ref(sec:supervisedmodels)). 

### More details
***Category Deep***
The linear model

Use for dimensionality reduction

One-class SVM

### Kernel trick
***Category Intuitive, Deep***

```{r svmRBF, echo = FALSE, out.width = '50%', fig.cap = 'The blue line has the same distance from the instances in each class, which is the idea behind SVM.'}
knitr::include_graphics('images/classification/svm_RBF.gif')
```

### Some improvements


Also talk about dimensionality reduction

### Implementation
***Category Code***
