# Classification

A *classification problem* asks to provide the category to which an instance belong, given the instance characteristics. An example of a classification problem is: given characteristics of a person, such as smoking habits, history of a heart attack in the family, number of hours of exercise per day, height, weight, among others, whether the person going to have a heart attack after their 50's. Another example of a classification problem is, to find the type of a tumor (benign, type 1, type 2, or type 3) given its characteristics (e.g., shape, color, genetic information, history of the patient, among others). For a given classification problem, a *classifier* seeks a generic *rule* (aka *model*) which generates the category for any given instance, e.g., a generic rule that gives the tumor category given the characteristics of the tumor. The classifier finds this generic rule based on some assumptions and a set of examples, i.e., "learns" from examples. 

In a more general framework, given a set of instances, $X$, with $m$ rows and $n$ columns (each row is an instance and each column is an attribute) and their associated classes, $Y$, with $m$ rows (containing a categorical value in each row), a classifier is tasked to find an *optimal* model $M(.,\theta)$, $\theta$ being model parameters (e.g., linear model), which for any given instance, $x$ ($1$ row and $n$ columns), can generate the class label, $y$, correctly. The parameters $\theta$ are optimized by an optimizer for the model $M$ to best satisfy a *similarity* measure which measures how similar $M(.,\theta)$ is to actual $y$.

Usually, the given instances (training set) do not represent all possible instances for a given problem. Therefore, any model, in any shape and form, that separates the given instances and optimizes the similarity measure is acceptable (purple, green, and orange lines in the figure). This means defining "the best model", a model that not only separates the given instances but also all other unseen instances, is limited by the knowledge encoded in the training set. This incomplete knowledge about the data leads the designer to make some *assumptions* on the model and the similarity metric upon which different classifiers are formulated. For example, support vector machines in their original form assume that the discriminatory rule is represented by a line (an assumption on the model, a linear model) which has the maximum distance from the instances in each class (an assumption on the similarity measure). The idea is that such a line is more empirically robust against potential uncertainties in unseen instances. 

Note that the model is responsible to represent the "pattern" in the data in the most generic and accurate way, hence, very important to be designed carefully. While very flexible models (e.g., deep networks) can fit the data well (simply because they are flexible and can fit anything), they might not be able to *generalize* well, i.e., work well for unseen data. Sometimes the models come from specific knowledge about the problem, e.g., physics models. 

A synthetic classification problem has been shown in Fig. \@ref(fig:classExample). In this problem, there are two attributes per instance and two groups of instances (blue and red), and the model has been assumed to be represented by a line. The aim of classification (binary in this case) with these assumptions is to find the "best" discriminatory line (hyperplane in a higher number of dimensions) which discriminates between the two classes "optimally" (see figure). Ideally, this optimality needs to make sure that the rule not only works for the given data (training data) based on a performance measure but also for any unseen data point (test data), referred to as the *generalization ability* of the rule.

```{r classExample, echo = FALSE, out.width = '49%',fig.show = "hold", fig.cap = 'Two attributes (horizontal and vertical axes), two classes (red and blue), and three lines which can separate the classes successfully.'}
knitr::include_graphics(c('./images/classification/classification.png', './images/classification/classification_.png'))
```

&nbsp;

In summary, to design a classifier, one would need the training data, the model shape (e.g., linear), a similarity measure (e.g., maximum empirical margin in support vector machines), and an optimization algorithm to optimize the model parameters given the similarity measure and the data. The similarity measure is responsible to inform the optimizer on how well the current parameters perform while ensure the generalization ability is not sacrificed. 

See [@ng2002discriminative] for a formal definition of discriminative classification. 

> **Key points**:
>
>* Classification aims to find a mathematical rule, aka model, that provides the category to which an instance belongs using a set of given examples.
>* As the training data-set is a subset of all possible data points, some assumptions need to be made to ensure that the "optimized" model performs well not only on the training data-set but also on unseen instances (generalization ability). 
>* These assumptions include the shapes of the mathematical rule (a line, "if-then" rules, etc.) and the *similarity* measure by which the rule is evaluated.
>* Different classification methods usually differ by their assumptions on the model, similarity measure, and optimization method used.
