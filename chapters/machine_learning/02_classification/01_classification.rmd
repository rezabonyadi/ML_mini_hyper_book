# Classification

Classification refers to learning to categorize instances by examples. For example,
learning if a person going to have a heart attack after their 50's given person's characteristics such as smoking habits, history of a heart attack in the family, number of hours of exercise per day, height, weight, among others. The task is to find a general rule which estimates, for any given new person with given characteristics, if the person is going to have a heart attack after their 50s. Another example of classification is, given some attributes of a set of tumors (e.g., shape, color, genetic information, history of the patient, among others) and their categories (benign, type 1, type 2, or type 3), find a generic rule that estimates the tumor type for any tumor. 

In a more general framework, given a set of instances, $X$, with $m$ rows and $n$ columns (each row is an instance and each column is an attribute) and their associated classes, $Y$, with $m$ rows (containing a categorical value in each row), find a mathematical *rule* (line and hyper-plane are special cases of this) which can generate the class label, $y$, for any given instance, $x$ ($1$ row and $n$ columns). 

A synthetic classification problem has been shown in Fig. \@ref(fig:classExample). In this problem, we have two attributes per instance and two groups of instances (blue and red), and the discriminatory rule has been assumed to be represented by a line. The aim of classification (binary in this case) with these assumptions is to find the "best" discriminatory line (hyperplane in a higher number of dimensions) which discriminates between the two classes "optimally" (see figure). Ideally, this optimality needs to make sure that the rule not only works for the given data (training data) based on a performance measure but also for any unseen data point (test data), referred to as the *generalization ability* of the rule.

```{r classExample, echo = FALSE, fig.cap = 'Two attributes (horizontal and vertical axes), two classes (red and blue), and three lines which can separate the classes successfully.'}
knitr::include_graphics('./images/classification/classification.png')
```

&nbsp;

As usually the given instances do not represent all possible instances for a given problem, therefore, any rule, in any shape and form, that separates the given instances and optimizes the performance measure is acceptable (purple, green, and orange lines in the figure). Hence, defining "the best discriminatory rule", a rule that not only separates the given instances but also all other unseen instances, is not possible. This leads to different assumptions upon which different classifiers are formulated. For example, support vector machines assume that the discriminatory rule is represented by a line which has the maximum distance from the instances in each class. The idea is that such a line is more empirically robust against potential uncertainties in unseen instances.

See [this article](https://arxiv.org/pdf/1804.09891.pdf) [@bonyadi2019optimal] for a formal definition of discriminative classification. 

> **Key points**:
>
>* Classification aims to find a mathematical rule that provides the category to which an instance belongs using a set of given examples.
>* As the training data-set is a subset of all possible data points, some assumptions need to be made to ensure that the "optimal" rule performs well not only on the training data-set but also on unseen instances (generalization ability). These assumptions lead to different classification algorithms.
>* These assumptions include the shapes of the mathematical rule (a line, "if-then" rules, etc.) and the *performance measure* by which the rule is evaluated.
