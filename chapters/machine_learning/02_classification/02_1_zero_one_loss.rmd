## The optimal binary classifier

Let's assume that the discriminatory rule in a classification problem is represented by a line, and we want to find a line that "optimally" discriminates between two classes. This optimality should be measured by an objective function, a function that assigns a "wrongness" score to each line (called the loss function), which needs to be minimized. One simple function would be the number of instances that are not in the correct side of the line. To implement this loss function, one can use a $0/1$ function that returns a $0$ if an instance is in the correct side of the given line and a $1$ otherwise. The sum of the 0/1 function over all instances is called the "0/1 loss function", which is minimized by an optimization algorithm. The outcome of this minimization is a line which has a minimum 0/1 loss value.

<figure>
<img src="images/classification/0_1_loss_1.PNG" alt="0/1 loss" width="80%" height="80%" align="centre">
<figcaption>Fig 2: Two candidate lines, green gives 4/30 and the orange gives 3/30 loss value according to the 0/1 loss function. </figcaption>
</figure>

&nbsp;

In Fig. 2, for example, the 0/1 loss value is the same for the green line and the orange line. 

The 0/1 loss function does not take into account the *importance* of number of miss-classified instances from different classes (all classes are assumed to be equally important). In the previous example (Fig. 2), the number of blue instances is smaller than the number of red instances, which means miss-classification of blue instances leads to a larger natural loss comparing to the red instances. For example, the orange line has a very high accuracy on red instances while its accuracy on the blue is not as high as the green line. 

To address this issue, it has been proposed in [this](https://arxiv.org/abs/1804.09891) [@bonyadi2019optimal] article to minimize the average of the 0/1 loss values of over the classes rather than than the summation of the 0/1 function across all instances [@bonyadi2019optimal]. To implement this idea, for a given discriminatory line, one can iterate over all instances of each class and calculate the 0/1 loss function for each class separately and then average those loss values across all classes. In Fig. 2, for example, this average for the green line is $\frac{(2/10+2/20)}{2}$ (2 miss-classified instances from each class) and for the orange line is $\frac{3/10+1/20}{2}$ (3 miss-classified from blue and 1 from red). Hence, the green line would be preferred. 

The 0/1 loss function in both above-mentioned forms is unbiased and not sensitive to outliers as any outlier would only contribute 1 unit to the loss function if it is miss-classified. Finding a line that minimizes the average of the 0/1 loss function in both above-mentioned forms is not easy (see [this](http://proceedings.mlr.press/v28/nguyen13a.pdf) [@doerr2015direct] and [this](https://arxiv.org/abs/1804.09891) [@bonyadi2019optimal]). One issue is that this line is not unique. For example, both orange and green lines in Fig. 1 have the same accuracy in terms of discriminating between the classes. This is solved by selecting the line which has the maximum distance from the instances of both classes (see this [paper](https://arxiv.org/abs/1804.09891) [@bonyadi2019optimal]). The basic idea is, if there are multiple lines which separate instances from each class with the same accuracy, the one which has the maximum distance from each class is preferred. The Python, Java, and Matlab code is available for that article (https://github.com/rezabonyadi/LinearOEC). Another issue is that optimizing the 0/1 loss function in the formed mentioned before is not practical, i.e., it is NP-Complete. This has encouraged introduction to lots of new loss functions which approximate the 0/1 loss function while they are differentiable. 


