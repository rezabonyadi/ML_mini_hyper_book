## Curiosity

In reinforcement learning, you have an agent that is exploring an environment to perform a task for which it receives an award if it does it right (or partially right). Now assume that the rewards are not that frequent (e.g., consider fishing, you only receive the reward once you capture a fish, while many steps are needed to actually capture the fish). In that case, the agent experiences difficulty to find even one simple way to receive a reward. As such, it will be difficult for the agent to learn the game effectively as it cannot extent its learning as there is no reward for most actions it take. Here comes curiosity. The idea is, the agent should avoid strategies it has tried before and lean towards strategies that have not been explored much. This provides an "intrinsic" reward for the agent which help to find new ways which, hopefully, would lead to more rewards.

```{r montezuma, echo = FALSE, out.width = '50%', fig.cap = 'The objective is to find a line/hyperplane that separates the instances optimally. The search continues until a suitable hyperplane, evaluated by the loss function, is found. The final line found in this example classifies all instances correctly except one.'}
knitr::include_graphics('images/reinforcement_learning/thegamespeop.gif')
```

A good example of where curiosity can be helpful is in the game Montezuma's Revenge (see Fig. \@ref(fig:montezuma)). The agent is required to find the "key" in the middle-left of the screen. To do so, it requires to find the path indicated in the figure, which is a quite complicated path. At the beginning, the agent has no idea about this environment, hence, it needs to explore options rather randomly. If the agent falls down or hits the skeleton, it will die. The agent does not receive any reward for most of the steps in this game, until it actually captures the key. Finding that successful path until the agent receives a reward randomly is close to impossible. In that case, because most paths lead to death rather than a reward, the agent would think that it would be better not to move at all to reduce the possibility of loss rather than finding any rewards. An intrinsic reward, here curiosity, would still keep the agent motivated to explore.  

### Implementation idea
Curiosity provides intrinsic motivation for the agent to explore more options. The idea is to give a higher weight to the moves that the agent has not explored much so far. This can be implemented by a neural network that keeps track of what the agent has tried so far and provided smaller weighting to the ones that the agent has tried very often. Adding this weight to the reward, the agent would receive a reward for the movements that are “novel”, which encourages the agent to be “curious”. See https://arxiv.org/pdf/1808.04355.pdf (https://arxiv.org/pdf/1808.04355.pdf). Another way of implementation is based on the following assumption: If the agent can guess what is the consequence of its move at the current step then the agent has tried this path before, hence, intrinsic reward would be lower.
